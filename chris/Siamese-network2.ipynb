{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "982acb8f",
   "metadata": {},
   "source": [
    "## Siamese network \n",
    "Steps:\n",
    "1. load word embeding and document embedding\n",
    "2. create pytorch dataset and dataloader\n",
    "3. Try Contrastive loss and triplet loss\n",
    "4. further improve negative sampling (e.g. hard negative or word2vec negative sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67d8234",
   "metadata": {},
   "source": [
    "### raw data\n",
    "* word embedding: glove\n",
    "* doc text: ./data/IMDB.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b007799a",
   "metadata": {},
   "source": [
    "### preprocess\n",
    "1. filter too frequent and less frequent words\n",
    "2. stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6a82ca",
   "metadata": {},
   "source": [
    "### model\n",
    "1. Siamese\n",
    "2. TopK\n",
    "3. DNN\n",
    "4. Lasso\n",
    "5. LassoGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b9967b",
   "metadata": {},
   "source": [
    "### evaluation\n",
    "1. F1\n",
    "2. NDCG\n",
    "    1. MAP\n",
    "    2. MRR\n",
    "    3. ERR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e9beff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np \n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from itertools import cycle\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.metrics import ndcg_score\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2341254",
   "metadata": {},
   "source": [
    "## Preprocess config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dc01df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "config[\"n_document\"] = 10000\n",
    "config[\"min_word_freq_threshold\"] = 20\n",
    "config[\"topk_word_freq_threshold\"] = 500\n",
    "config[\"document_vector_agg\"] = 'TF-IDF'\n",
    "config[\"select_topk_TFIDF\"] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05c1627d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfacf768b2ab411695cbde5b14528853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:400000\n"
     ]
    }
   ],
   "source": [
    "# load word embedding\n",
    "embedding_file = \"../data/glove.6B.100d.txt\"\n",
    "\n",
    "word2embedding = dict()\n",
    "word_dim = int(re.findall(r\".(\\d+)d\",embedding_file)[0])\n",
    "\n",
    "with open(embedding_file,\"r\") as f:\n",
    "    for line in tqdm(f):\n",
    "        line = line.strip().split()\n",
    "        word = line[0]\n",
    "        embedding = list(map(float,line[1:]))\n",
    "        word2embedding[word] = embedding\n",
    "\n",
    "print(\"Number of words:%d\" % len(word2embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1233053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, word2embedding, min_word_freq_threshold=0, topk_word_freq_threshold=0):\n",
    "        # The low frequency words will be assigned as <UNK> token\n",
    "        self.itos = {0: \"<UNK>\"}\n",
    "        self.stoi = {\"<UNK>\": 0}\n",
    "        \n",
    "        self.word2embedding = word2embedding\n",
    "        self.min_word_freq_threshold = min_word_freq_threshold\n",
    "        self.topk_word_freq_threshold = topk_word_freq_threshold\n",
    "        \n",
    "        self.word_freq_in_corpus = defaultdict(int)\n",
    "        self.IDF = {}\n",
    "        self.ps = PorterStemmer()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "#     @staticmethod\n",
    "    def tokenizer_eng(self, text):\n",
    "        text = re.sub(r'[^A-Za-z0-9 ]+', '', text)\n",
    "        text = text.strip().split()\n",
    "        \n",
    "        return [self.ps.stem(w) for w in text]\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        \n",
    "        self.doc_freq = defaultdict(int) # # of document a word appear\n",
    "        self.document_num = len(sentence_list)\n",
    "        self.word_vectors = [[0]*word_dim] # unknown word emb\n",
    "        \n",
    "        for sentence in tqdm(sentence_list, desc=\"Preprocessing documents\"):\n",
    "            # for doc_freq\n",
    "            document_words = set()\n",
    "            \n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                # pass unknown word\n",
    "                if word not in self.word2embedding:\n",
    "                    continue\n",
    "                    \n",
    "                # calculate word freq\n",
    "                self.word_freq_in_corpus[word] += 1\n",
    "                document_words.add(word)\n",
    "                \n",
    "            for word in document_words:\n",
    "                self.doc_freq[word] += 1\n",
    "        \n",
    "        # calculate IDF\n",
    "        print('doc num', self.document_num)\n",
    "        for word, freq in self.doc_freq.items():\n",
    "            self.IDF[word] = math.log(self.document_num / (freq+1))\n",
    "        \n",
    "        # delete less freq words:\n",
    "        delete_words = []\n",
    "        for word, v in self.word_freq_in_corpus.items():\n",
    "            if v < self.min_word_freq_threshold:\n",
    "                delete_words.append(word)     \n",
    "        for word in delete_words:\n",
    "            del self.IDF[word]    \n",
    "            del self.word_freq_in_corpus[word]    \n",
    "        \n",
    "        # delete too freq words\n",
    "        print('eliminate freq words')\n",
    "        IDF = [(word, freq) for word, freq in self.IDF.items()]\n",
    "        IDF.sort(key=lambda x: x[1])\n",
    "\n",
    "        for i in range(self.topk_word_freq_threshold):\n",
    "            print(word)\n",
    "            word = IDF[i][0]\n",
    "            del self.IDF[word]\n",
    "            del self.word_freq_in_corpus[word]\n",
    "        \n",
    "        # construct word_vectors\n",
    "        idx = 1\n",
    "        for word in self.word_freq_in_corpus:\n",
    "            self.word_vectors.append(self.word2embedding[word])\n",
    "            self.stoi[word] = idx\n",
    "            self.itos[idx] = word\n",
    "            idx += 1\n",
    "            \n",
    "    def calculate_document_vector(self, sentence_list, agg, select_topk_TFIDF=None):\n",
    "        document_vectors = []\n",
    "        document_answers = []\n",
    "        document_answers_w = []\n",
    "        \n",
    "        for sentence in tqdm(sentence_list, desc=\"calculate document vectors\"):\n",
    "            document_vector = np.zeros(len(self.word_vectors[0]))\n",
    "            select_words = []\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                # pass unknown word\n",
    "                if word not in self.stoi:\n",
    "                    continue\n",
    "                else:\n",
    "                    select_words.append(word)\n",
    "\n",
    "            # select topk TDIDF\n",
    "            if select_topk_TFIDF is not None:\n",
    "                doc_TFIDF = defaultdict(float)\n",
    "                for word in select_words:    \n",
    "                    doc_TFIDF[word] += self.IDF[word]\n",
    "\n",
    "                doc_TFIDF_l = [(word, TFIDF) for word, TFIDF in doc_TFIDF.items()]\n",
    "                doc_TFIDF_l.sort(key=lambda x:x[1], reverse=True)\n",
    "                \n",
    "                select_topk_words = set(list(map(lambda x:x[0], doc_TFIDF_l[:select_topk_TFIDF])))\n",
    "                select_words = [word for word in select_words if word in select_topk_words]\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            total_weight = 0\n",
    "            # aggregate to doc vectors\n",
    "            for word in select_words:\n",
    "                if agg == 'mean':\n",
    "                    document_vector += self.word2embedding[word]\n",
    "                    total_weight += 1\n",
    "                elif agg == 'TF-IDF':\n",
    "                    document_vector += np.array(self.word2embedding[word]) * self.IDF[word]\n",
    "                    total_weight += self.IDF[word]\n",
    "\n",
    "            if len(select_words) == 0:\n",
    "                print('error', sentence)\n",
    "                continue\n",
    "            else:\n",
    "                document_vector /= len(select_words)\n",
    "                total_weight /= len(select_words)\n",
    "            \n",
    "            document_vectors.append(document_vector)\n",
    "            document_answers.append(select_words)\n",
    "            document_answers_w.append(total_weight)\n",
    "        \n",
    "        # get answers\n",
    "        document_answers_idx = []    \n",
    "        for ans in document_answers:\n",
    "            ans_idx = []\n",
    "            for token in ans:\n",
    "                if token in self.stoi:\n",
    "                    ans_idx.append(self.stoi[token])                    \n",
    "            document_answers_idx.append(ans_idx)\n",
    "            \n",
    "        return document_vectors, document_answers_idx, document_answers_w\n",
    "        \n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c5706a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBowDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 raw_data_file_path,\n",
    "                 word2embedding,\n",
    "                 skip_header = False,\n",
    "                 n_document = None, # read first n document\n",
    "                 min_word_freq_threshold = 20, # eliminate less freq words\n",
    "                 topk_word_freq_threshold = 5, # eliminate smallest k IDF words\n",
    "                 select_topk_TFIDF = None, # select topk tf-idf as ground-truth\n",
    "                 document_vector_agg = 'mean',\n",
    "                 ):\n",
    "\n",
    "        assert document_vector_agg in ['mean', 'TF-IDF']\n",
    "        \n",
    "        # raw documents\n",
    "        self.documents = []\n",
    "        \n",
    "        with open(raw_data_file_path,'r',encoding='utf-8') as f:\n",
    "            if skip_header:\n",
    "                f.readline()\n",
    "            for line in tqdm(f, desc=\"Loading documents\"):\n",
    "                # read firt n document\n",
    "                if n_document is not None and len(self.documents) >= n_document:\n",
    "                    break    \n",
    "                self.documents.append(line.strip(\"\\n\"))\n",
    "\n",
    "        # build vocabulary\n",
    "        self.vocab = Vocabulary(word2embedding, min_word_freq_threshold, topk_word_freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.documents)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "        # calculate document vectors\n",
    "        self.document_vectors, self.document_answers, self.document_answers_w = self.vocab.calculate_document_vector(self.documents, \\\n",
    "                                                                                           document_vector_agg, select_topk_TFIDF)\n",
    "        # train-test split\n",
    "        # training\n",
    "        self.train_split_ratio = 0.8\n",
    "        self.train_length = int(len(self.document_answers) * self.train_split_ratio)\n",
    "        self.train_vectors = self.document_vectors[:self.train_length]\n",
    "        self.train_words = self.document_answers[:self.train_length]\n",
    "        self.document_ids = list(range(self.train_length))\n",
    "        self.generator = cycle(self.context_target_generator())\n",
    "        self.dataset_size = sum([len(s) for s in self.train_words])\n",
    "        \n",
    "        # testing\n",
    "        self.test_vectors = self.document_vectors[self.train_length:]\n",
    "        self.test_words = self.document_answers[self.train_length:]\n",
    "\n",
    "    def context_target_generator(self):\n",
    "        np.random.shuffle(self.document_ids) # inplace shuffle\n",
    "\n",
    "        # randomly select a document and create its training example\n",
    "        for document_id in self.document_ids: \n",
    "            word_list = set(self.train_words[document_id])\n",
    "            negative_sample_space = list(set(range(self.vocab_size)) - word_list)\n",
    "            negative_samples = np.random.choice(negative_sample_space,size=len(word_list),replace = False)\n",
    "            for word_id, negative_wordID in zip(word_list, negative_samples):\n",
    "                yield [document_id, word_id, negative_wordID]\n",
    "                \n",
    "    def __getitem__(self, idx):\n",
    "        doc_id, word_id, negative_wordID = next(self.generator)\n",
    "        doc_id = torch.FloatTensor(self.document_vectors[doc_id])\n",
    "        word_id = torch.FloatTensor(self.vocab.word_vectors[word_id])\n",
    "        negative_word = torch.FloatTensor(self.vocab.word_vectors[negative_wordID])\n",
    "\n",
    "        return doc_id, word_id, negative_word\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1a7186c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e6c71214114c78b0cf1b6c0762364b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading documents: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a27a46c7c7124b58815f3cc1a773dd3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing documents:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc num 10000\n",
      "eliminate freq words\n",
      "boel\n",
      "the\n",
      "and\n",
      "a\n",
      "of\n",
      "to\n",
      "is\n",
      "it\n",
      "thi\n",
      "in\n",
      "that\n",
      "i\n",
      "for\n",
      "with\n",
      "but\n",
      "as\n",
      "on\n",
      "wa\n",
      "be\n",
      "film\n",
      "one\n",
      "not\n",
      "are\n",
      "have\n",
      "all\n",
      "an\n",
      "you\n",
      "at\n",
      "by\n",
      "from\n",
      "who\n",
      "like\n",
      "hi\n",
      "ha\n",
      "so\n",
      "he\n",
      "time\n",
      "about\n",
      "out\n",
      "there\n",
      "if\n",
      "veri\n",
      "see\n",
      "good\n",
      "what\n",
      "more\n",
      "they\n",
      "when\n",
      "just\n",
      "some\n",
      "or\n",
      "make\n",
      "watch\n",
      "great\n",
      "get\n",
      "well\n",
      "my\n",
      "other\n",
      "up\n",
      "can\n",
      "love\n",
      "also\n",
      "which\n",
      "would\n",
      "their\n",
      "will\n",
      "even\n",
      "most\n",
      "her\n",
      "me\n",
      "had\n",
      "much\n",
      "than\n",
      "first\n",
      "do\n",
      "way\n",
      "into\n",
      "play\n",
      "end\n",
      "were\n",
      "no\n",
      "best\n",
      "scene\n",
      "think\n",
      "been\n",
      "how\n",
      "go\n",
      "look\n",
      "show\n",
      "made\n",
      "she\n",
      "after\n",
      "we\n",
      "year\n",
      "mani\n",
      "work\n",
      "know\n",
      "too\n",
      "seen\n",
      "act\n",
      "him\n",
      "them\n",
      "come\n",
      "thing\n",
      "perform\n",
      "two\n",
      "life\n",
      "still\n",
      "never\n",
      "take\n",
      "dont\n",
      "could\n",
      "give\n",
      "say\n",
      "then\n",
      "actor\n",
      "ani\n",
      "doe\n",
      "your\n",
      "where\n",
      "seem\n",
      "find\n",
      "enjoy\n",
      "want\n",
      "ever\n",
      "while\n",
      "man\n",
      "did\n",
      "over\n",
      "cast\n",
      "feel\n",
      "here\n",
      "such\n",
      "back\n",
      "these\n",
      "part\n",
      "those\n",
      "lot\n",
      "live\n",
      "tri\n",
      "role\n",
      "plot\n",
      "wonder\n",
      "interest\n",
      "use\n",
      "though\n",
      "better\n",
      "through\n",
      "now\n",
      "real\n",
      "off\n",
      "new\n",
      "befor\n",
      "world\n",
      "should\n",
      "set\n",
      "both\n",
      "quit\n",
      "again\n",
      "alway\n",
      "day\n",
      "director\n",
      "star\n",
      "young\n",
      "actual\n",
      "few\n",
      "own\n",
      "old\n",
      "same\n",
      "doesnt\n",
      "music\n",
      "direct\n",
      "may\n",
      "excel\n",
      "right\n",
      "fact\n",
      "bit\n",
      "start\n",
      "im\n",
      "turn\n",
      "whi\n",
      "between\n",
      "us\n",
      "saw\n",
      "without\n",
      "thought\n",
      "person\n",
      "long\n",
      "bad\n",
      "point\n",
      "down\n",
      "fan\n",
      "big\n",
      "recommend\n",
      "differ\n",
      "didnt\n",
      "around\n",
      "final\n",
      "must\n",
      "got\n",
      "last\n",
      "effect\n",
      "entertain\n",
      "place\n",
      "done\n",
      "cant\n",
      "need\n",
      "ive\n",
      "tell\n",
      "happen\n",
      "origin\n",
      "almost\n",
      "friend\n",
      "sinc\n",
      "put\n",
      "begin\n",
      "fun\n",
      "help\n",
      "although\n",
      "girl\n",
      "move\n",
      "each\n",
      "action\n",
      "true\n",
      "yet\n",
      "job\n",
      "enough\n",
      "keep\n",
      "sure\n",
      "line\n",
      "cours\n",
      "moment\n",
      "expect\n",
      "screen\n",
      "perfect\n",
      "lead\n",
      "onc\n",
      "classic\n",
      "view\n",
      "away\n",
      "far\n",
      "anyon\n",
      "woman\n",
      "worth\n",
      "kind\n",
      "whole\n",
      "found\n",
      "guy\n",
      "dvd\n",
      "isnt\n",
      "rather\n",
      "name\n",
      "seri\n",
      "later\n",
      "noth\n",
      "am\n",
      "hard\n",
      "follow\n",
      "our\n",
      "laugh\n",
      "mean\n",
      "nice\n",
      "reason\n",
      "goe\n",
      "might\n",
      "appear\n",
      "hope\n",
      "let\n",
      "shot\n",
      "includ\n",
      "run\n",
      "portray\n",
      "least\n",
      "himself\n",
      "tv\n",
      "complet\n",
      "read\n",
      "kill\n",
      "call\n",
      "john\n",
      "american\n",
      "understand\n",
      "viewer\n",
      "human\n",
      "miss\n",
      "face\n",
      "night\n",
      "favorit\n",
      "dure\n",
      "mind\n",
      "script\n",
      "chang\n",
      "power\n",
      "special\n",
      "sens\n",
      "eye\n",
      "second\n",
      "kid\n",
      "bring\n",
      "along\n",
      "three\n",
      "wife\n",
      "10\n",
      "usual\n",
      "open\n",
      "hollywood\n",
      "touch\n",
      "said\n",
      "main\n",
      "book\n",
      "left\n",
      "until\n",
      "meet\n",
      "creat\n",
      "age\n",
      "father\n",
      "support\n",
      "home\n",
      "death\n",
      "idea\n",
      "short\n",
      "came\n",
      "hand\n",
      "often\n",
      "boy\n",
      "top\n",
      "today\n",
      "high\n",
      "small\n",
      "product\n",
      "care\n",
      "horror\n",
      "problem\n",
      "gener\n",
      "heart\n",
      "brilliant\n",
      "full\n",
      "fall\n",
      "men\n",
      "version\n",
      "style\n",
      "murder\n",
      "less\n",
      "fine\n",
      "natur\n",
      "hous\n",
      "sound\n",
      "next\n",
      "base\n",
      "rest\n",
      "impress\n",
      "rate\n",
      "overal\n",
      "absolut\n",
      "given\n",
      "els\n",
      "drama\n",
      "talk\n",
      "sever\n",
      "die\n",
      "close\n",
      "war\n",
      "throughout\n",
      "head\n",
      "present\n",
      "humor\n",
      "wasnt\n",
      "except\n",
      "talent\n",
      "word\n",
      "song\n",
      "case\n",
      "review\n",
      "black\n",
      "cinema\n",
      "fight\n",
      "relationship\n",
      "imagin\n",
      "light\n",
      "dark\n",
      "success\n",
      "write\n",
      "late\n",
      "actress\n",
      "against\n",
      "deal\n",
      "side\n",
      "comment\n",
      "itself\n",
      "anim\n",
      "lost\n",
      "past\n",
      "develop\n",
      "learn\n",
      "art\n",
      "strong\n",
      "hour\n",
      "total\n",
      "stand\n",
      "instead\n",
      "written\n",
      "sort\n",
      "2\n",
      "mother\n",
      "abl\n",
      "theme\n",
      "either\n",
      "wish\n",
      "brother\n",
      "score\n",
      "matter\n",
      "wrong\n",
      "ye\n",
      "mr\n",
      "hit\n",
      "women\n",
      "thank\n",
      "recent\n",
      "soon\n",
      "ask\n",
      "felt\n",
      "camera\n",
      "mention\n",
      "wont\n",
      "import\n",
      "twist\n",
      "under\n",
      "event\n",
      "went\n",
      "citi\n",
      "children\n",
      "happi\n",
      "number\n",
      "type\n",
      "money\n",
      "behind\n",
      "attempt\n",
      "ago\n",
      "wait\n",
      "youll\n",
      "son\n",
      "id\n",
      "white\n",
      "school\n",
      "charm\n",
      "writer\n",
      "video\n",
      "stop\n",
      "question\n",
      "add\n",
      "element\n",
      "disappoint\n",
      "pace\n",
      "dead\n",
      "hold\n",
      "robert\n",
      "jame\n",
      "lack\n",
      "critic\n",
      "told\n",
      "dream\n",
      "evil\n",
      "visual\n",
      "heard\n",
      "daughter\n",
      "superb\n",
      "child\n",
      "order\n",
      "credit\n",
      "somewhat\n",
      "return\n",
      "known\n",
      "comic\n",
      "stay\n",
      "provid\n",
      "major\n",
      "strang\n",
      "career\n",
      "similar\n",
      "basic\n",
      "gave\n",
      "michael\n",
      "took\n",
      "note\n",
      "form\n",
      "opinion\n",
      "rare\n",
      "allow\n",
      "offer\n",
      "remind\n",
      "anyway\n",
      "greatest\n",
      "whose\n",
      "oscar\n",
      "level\n",
      "tale\n",
      "ill\n",
      "alon\n",
      "husband\n",
      "guess\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37cc4c2a5d1146919deb2ac446b1f8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "calculate document vectors:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error i think it's one of the greatest movies which are ever made ,  and i've seen many .  .  .  the book is better ,  but it's still a very good movie ! \n",
      "error all this talk about this being a bad movie is nonsense .  as a matter of fact this is the best movie i've ever seen .  it's an excellent story and the actors in the movie are some of the best .  i would not give criticism to any of the actors .  that movie is the best and it will always stay that way . \n",
      "error smallville episode justice is the best episode of smallville  !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !  it's my favorite episode of smallville !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   ! \n",
      "error smallville episode justice is the best episode of smallville  !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !  it's my favorite episode of smallville !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   ! \n",
      "error this is actually one of my favorite films ,  i would recommend that everyone watches it .  there is some great acting in it and it shows that not all  \" good \"  films are american .  .  .  . \n",
      "error i thought it was an original story ,  very nicely told .  i think all you people are expecting too much .  i mean .  .  . it's just a made for television movie !  what are you expecting ?  some great wonderful dramtic piece ?  i thought it was a really great story for a made for television movie .  .  .  . and that's my opinion . \n",
      "error it's highly stylized ,  but this movie shows that real people appear on these shows and what seems like good fun and a chance to appear on television can have serious consequences .   yes ,  i's mostly comedy ,  but there are some sad moments . \n",
      "Finish building dataset!\n",
      "Number of documents:10000\n",
      "Number of words:4127\n"
     ]
    }
   ],
   "source": [
    "# load and build torch dataset\n",
    "data_file_path = '../data/IMDB.txt'\n",
    "\n",
    "print(\"Building dataset....\")\n",
    "dataset = CBowDataset(\n",
    "                    raw_data_file_path=data_file_path,\n",
    "                    word2embedding=word2embedding,\n",
    "                    skip_header=False,\n",
    "                    n_document = config[\"n_document\"],\n",
    "                    min_word_freq_threshold = config[\"min_word_freq_threshold\"],\n",
    "                    topk_word_freq_threshold = config[\"topk_word_freq_threshold\"],\n",
    "                    document_vector_agg = config[\"document_vector_agg\"],\n",
    "                    select_topk_TFIDF = config[\"select_topk_TFIDF\"]\n",
    "                    )\n",
    "print(\"Finish building dataset!\")\n",
    "print(f\"Number of documents:{len(dataset.documents)}\")\n",
    "print(f\"Number of words:{dataset.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba4758da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# check test doc vectors' correctness\n",
    "word_vectors = np.array(dataset.vocab.word_vectors)\n",
    "word_vectors.shape\n",
    "\n",
    "pred = np.zeros(100)\n",
    "cnt = 0\n",
    "for word_idx in dataset.test_words[0]:\n",
    "    pred += word_vectors[word_idx] * dataset.vocab.IDF[dataset.vocab.itos[word_idx]]\n",
    "    cnt += 1\n",
    "print(dataset.test_vectors[0] - pred/cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47f8bc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9993, 4127)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a49d4df7135144f1ac3b0c01dce8e4bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## create tfidf_ans\n",
    "document_answers = dataset.document_answers\n",
    "\n",
    "onehot_ans = np.zeros((len(document_answers), word_vectors.shape[0]))\n",
    "tfidf_ans = np.zeros((len(document_answers), word_vectors.shape[0]))\n",
    "print(tfidf_ans.shape)\n",
    "\n",
    "for i in tqdm(range(len(document_answers))):\n",
    "    for word_idx in document_answers[i]:\n",
    "        tfidf_ans[i, word_idx] += dataset.vocab.IDF[dataset.vocab.itos[word_idx]]\n",
    "        onehot_ans[i, word_idx] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf6bf5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 doc_vectors,\n",
    "                 ans_words,\n",
    "                 ):\n",
    "        self.doc_vectors = doc_vectors\n",
    "        self.ans_words = ans_words\n",
    "        assert len(doc_vectors) == len(ans_words)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        doc_vec = torch.FloatTensor(self.doc_vectors[idx])\n",
    "        ans_w = torch.tensor(list(set(self.ans_words[idx])))\n",
    "        return doc_vec, ans_w\n",
    "\n",
    "    def collate_fn(self,batch):\n",
    "        # Batch: List of tuples [(batch1), (batch2)]\n",
    "        \n",
    "        doc_vec = torch.cat([item[0].unsqueeze(0) for item in batch], dim=0)\n",
    "        ans_w = [item[1] for item in batch]\n",
    "        ans_w = pad_sequence(ans_w, batch_first=True, padding_value=-1)\n",
    "        \n",
    "        return doc_vec, ans_w \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doc_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "457c5d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletNet(nn.Module):\n",
    "    def __init__(self, hdim):\n",
    "        super(TripletNet, self).__init__()\n",
    "        self.fc = nn.Sequential(nn.Linear(hdim, 256),\n",
    "                        nn.PReLU(),\n",
    "                        nn.Linear(256, 256),\n",
    "                        nn.PReLU(),\n",
    "                        nn.Linear(256, 2)\n",
    "                        )\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        output1 = self.fc(x1)\n",
    "        output2 = self.fc(x2)\n",
    "        output3 = self.fc(x3)\n",
    "        return output1, output2, output3\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59ec9a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Triplet loss\n",
    "    Takes embeddings of an anchor sample, a positive sample and a negative sample\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative, size_average=True):\n",
    "        distance_positive = (anchor - positive).pow(2).sum(1)  # .pow(.5)\n",
    "        distance_negative = (anchor - negative).pow(2).sum(1)  # .pow(.5)\n",
    "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean() if size_average else losses.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b60ca7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "margin = 1.\n",
    "BATCH_SIZE = 1024\n",
    "EPOCH = 20\n",
    "\n",
    "device = \"cuda:0\"\n",
    "model = TripletNet(word_dim).to(device)\n",
    "loss_fn = TripletLoss(margin).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22a934f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "                        dataset, \n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        num_workers=4,\n",
    "                        shuffle=True,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2aa0290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docvec = dataset.test_vectors\n",
    "test_ans = dataset.test_words\n",
    "test_dataset = TestDataset(test_docvec,test_ans)\n",
    "test_loader = DataLoader(test_dataset,                         \n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         num_workers=4,\n",
    "                         collate_fn=test_dataset.collate_fn)\n",
    "word_embedding_tensor = torch.FloatTensor(dataset.vocab.word_vectors).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05bb89ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_word_emb, loader,Ks = [50,100,150,200]):\n",
    "    avg_precision, avg_recall = [], []\n",
    "    for batch in test_loader:\n",
    "        batch = [item.to(device) for item in batch]\n",
    "        emb, ans = batch\n",
    "        emb = model.get_embedding(emb)\n",
    "        scores = torch.cdist(emb, test_word_emb)\n",
    "        ans_length = torch.sum((~ans.eq(-1)).float(), dim=-1)\n",
    "        mask = ~ans.eq(-1).unsqueeze(-1)\n",
    "        \n",
    "        # calculate precision and recall\n",
    "        tmp_pr, tmp_re = [],[]\n",
    "        for K in Ks:\n",
    "            top_indices = torch.argsort(scores,dim=1)[:,:K]\n",
    "            hit = top_indices.unsqueeze(-2) == ans.unsqueeze(-1)\n",
    "            hit = torch.sum((hit * mask).flatten(1),dim=-1)\n",
    "            precision = hit / K\n",
    "            recall = hit / ans_length\n",
    "            tmp_pr.append(precision)\n",
    "            tmp_re.append(recall)\n",
    "        tmp_pr = torch.stack(tmp_pr).T.detach().cpu().numpy().tolist()\n",
    "        tmp_re = torch.stack(tmp_re).T.detach().cpu().numpy().tolist()\n",
    "        avg_precision.extend(tmp_pr)\n",
    "        avg_recall.extend(tmp_re)\n",
    "        \n",
    "    avg_precision = np.mean(avg_precision,axis=0)\n",
    "    avg_recall = np.mean(avg_recall, axis=0)\n",
    "    for idx, kval in enumerate(Ks):\n",
    "        print(f\"[K={kval}] Precision:{avg_precision[idx]:.4f} Recall:{avg_recall[idx]:.4f}\")\n",
    "    return avg_precision, avg_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c124d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(EPOCH):\n",
    "#     avg_loss = []\n",
    "#     model.train()\n",
    "#     for batch in tqdm(train_loader):\n",
    "#         batch = [item.to(device) for item in batch]\n",
    "#         doc_id,pos_w,neg_w = batch\n",
    "#         optimizer.zero_grad()\n",
    "#         loss = loss_fn(*model(doc_id,pos_w,neg_w))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         avg_loss.append(loss.item())\n",
    "#     avg_loss = np.mean(avg_loss)\n",
    "#     print(f\"Loss:{avg_loss:4f}\")\n",
    "    \n",
    "#     # evaluate\n",
    "#     model.eval()\n",
    "#     test_word_emb = model.get_embedding(word_embedding_tensor)\n",
    "#     res = evaluate(test_word_emb,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bc50afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "def evaluate_NDCG(test_word_emb, loader, topk=None):\n",
    "    NDCGs = defaultdict(list)\n",
    "    \n",
    "    for batch in (test_loader):\n",
    "        batch = [item.to(device) for item in batch]\n",
    "        emb, ans = batch\n",
    "        TFIDF_ans = np.zeros((len(ans), test_word_emb.shape[0]))\n",
    "        for i in range(len(ans)):\n",
    "            ans_row = ans[i]\n",
    "            ans_row = ans_row[~ans_row.eq(-1)]\n",
    "            ans_row = ans_row[~ans_row.eq(0)]\n",
    "            for word_id in ans_row:\n",
    "                word_id = word_id.item()\n",
    "                word = dataset.vocab.itos[word_id]\n",
    "                TFIDF_ans[i][word_id] += dataset.vocab.IDF[word]\n",
    "             \n",
    "        emb = model.get_embedding(emb)\n",
    "        scores = -torch.cdist(emb, test_word_emb).cpu().detach().numpy()\n",
    "        true_relevance = TFIDF_ans\n",
    "\n",
    "        NDCGs['top50'].append(ndcg_score(true_relevance, scores, k=50))\n",
    "        NDCGs['top100'].append(ndcg_score(true_relevance, scores, k=100))\n",
    "        NDCGs['top200'].append(ndcg_score(true_relevance, scores, k=200))\n",
    "        NDCGs['ALL'].append(ndcg_score(true_relevance, scores, k=None))\n",
    "    \n",
    "    print('NDCG top50', np.mean(NDCGs['top50']))\n",
    "    print('NDCG top100', np.mean(NDCGs['top100']))\n",
    "    print('NDCG top200', np.mean(NDCGs['top200']))\n",
    "    print('NDCG ALL', np.mean(NDCGs['ALL']))\n",
    "    return NDCGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8f96c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_history = []\n",
    "\n",
    "# for epoch in range(EPOCH):\n",
    "#     avg_loss = []\n",
    "#     model.train()\n",
    "#     for batch in tqdm(train_loader):\n",
    "#         batch = [item.to(device) for item in batch]\n",
    "#         doc_id,pos_w,neg_w = batch\n",
    "#         optimizer.zero_grad()\n",
    "#         loss = loss_fn(*model(doc_id,pos_w,neg_w))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         avg_loss.append(loss.item())\n",
    "#     avg_loss = np.mean(avg_loss)\n",
    "#     print(f\"Loss:{avg_loss:4f}\")\n",
    "    \n",
    "#     # evaluate\n",
    "#     model.eval()\n",
    "#     test_word_emb = model.get_embedding(word_embedding_tensor)\n",
    "#     ndcg_res = evaluate_NDCG(test_word_emb,test_loader)\n",
    "#     validation_history.append(ndcg_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d31a21",
   "metadata": {},
   "source": [
    "## Top K freq word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1efec49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('game', 783),\n",
       " ('season', 671),\n",
       " ('david', 549),\n",
       " ('ladi', 538),\n",
       " ('jack', 536),\n",
       " ('killer', 527),\n",
       " ('sex', 524),\n",
       " ('town', 524),\n",
       " ('king', 512),\n",
       " ('novel', 496)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = [(word, freq) for word, freq in dataset.vocab.word_freq_in_corpus.items()]\n",
    "word_freq.sort(key=lambda x:x[1], reverse=True)\n",
    "word_freq[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a04793e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd8385f3f5d541daa8fad8322df8e599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 50 word\n",
      "percision 0.03472736368184093\n",
      "recall 0.06114075689135594\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "385e1a46f4f7416690f72e15b2dbfb80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 100 word\n",
      "percision 0.03465732866433217\n",
      "recall 0.12218305736433438\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd7c71118974261954b7d83e741b2c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 200 word\n",
      "percision 0.0316983491745873\n",
      "recall 0.22272235160505402\n"
     ]
    }
   ],
   "source": [
    "def topk_word_evaluation(k=50):\n",
    "    topk_word = [word for (word, freq) in word_freq[:k]]\n",
    "\n",
    "    pr, re = [], []\n",
    "    for ans in tqdm(test_ans):\n",
    "        ans = set(ans)\n",
    "        ans = [dataset.vocab.itos[a] for a in ans]\n",
    "\n",
    "        hit = []\n",
    "        for word in ans:\n",
    "            if word in topk_word:\n",
    "                hit.append(word)\n",
    "\n",
    "        precision = len(hit) / k\n",
    "        recall = len(hit) / len(ans)\n",
    "        pr.append(precision)\n",
    "        re.append(recall)\n",
    "\n",
    "    print('top {} word'.format(k))\n",
    "    print('percision', np.mean(pr))\n",
    "    print('recall', np.mean(re))\n",
    "\n",
    "topk_word_evaluation(k=50)\n",
    "topk_word_evaluation(k=100)\n",
    "topk_word_evaluation(k=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26308789",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2097d67d27ab4d4b9d1cacadda35cb1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 50 NDCG:0.034637394512390236\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99fecf3084dd41febe189d7225104ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 100 NDCG:0.05239686405893681\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eaadbfcd2b14c8cbd55636d3dc5b950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 200 NDCG:0.07946306791732662\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96933204d1684e23835c7be451dadd4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top None NDCG:0.28615093379302187\n"
     ]
    }
   ],
   "source": [
    "def topk_word_evaluation_NDCG(k=50):\n",
    "    freq_word =[word for (word, freq) in word_freq]\n",
    "    freq_word_idx = [dataset.vocab.stoi[word] for word in freq_word if word in dataset.vocab.stoi]\n",
    "    \n",
    "    scores = np.zeros(len(dataset.vocab.word_vectors))\n",
    "    for rank, idx in enumerate(freq_word_idx):\n",
    "        scores[idx] = len(dataset.vocab.word_vectors) - rank\n",
    "    \n",
    "    NDCGs = []\n",
    "    \n",
    "    for ans in tqdm(test_ans):\n",
    "        TFIDF_ans = np.zeros(len(dataset.vocab.word_vectors))\n",
    "        \n",
    "        for word_idx in ans:\n",
    "            if word_idx == 0:\n",
    "                continue\n",
    "            word = dataset.vocab.itos[word_idx]\n",
    "            TFIDF_ans[word_idx] += dataset.vocab.IDF[word]\n",
    "\n",
    "        NDCG_score = ndcg_score(TFIDF_ans.reshape(1,-1), scores.reshape(1,-1), k=k)\n",
    "        NDCGs.append(NDCG_score)\n",
    "\n",
    "    print('top {} NDCG:{}'.format(k, np.mean(NDCGs)))\n",
    "\n",
    "topk_word_evaluation_NDCG(k=50)\n",
    "topk_word_evaluation_NDCG(k=100)\n",
    "topk_word_evaluation_NDCG(k=200)\n",
    "topk_word_evaluation_NDCG(k=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4fcb3a",
   "metadata": {},
   "source": [
    "## Linear regression with GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f05d86f",
   "metadata": {},
   "source": [
    "1. train lasso\n",
    "    1. with mse\n",
    "    2. with BCE to D\n",
    "2. train D\n",
    "    1. positive\n",
    "    2. negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "854f9586",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 doc_vectors,\n",
    "                 tfidf_ans,\n",
    "                 doc_ans_w,\n",
    "                 ):\n",
    "        self.doc_vectors = doc_vectors\n",
    "        self.tfidf_ans = tfidf_ans\n",
    "        self.doc_ans_w = doc_ans_w\n",
    "        assert len(doc_vectors) == len(tfidf_ans)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # doc vec, tfidf-ans, doc_id\n",
    "        doc_vec = torch.FloatTensor(self.doc_vectors[idx])\n",
    "        tfidf_ans = torch.FloatTensor(self.tfidf_ans[idx])\n",
    "        doc_ans_w = torch.FloatTensor(self.doc_ans_w[idx])\n",
    "                \n",
    "        return doc_vec, tfidf_ans, idx, doc_ans_w\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doc_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0aa07e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lasso(nn.Module):\n",
    "    \"\"\"\n",
    "    Input shape: (N, 3, 64, 64)\n",
    "    Output shape: (N, )\n",
    "    \"\"\"\n",
    "    def __init__(self, num_doc, num_words, L1=0, L2=0):\n",
    "        super(Lasso, self).__init__()\n",
    "        weight = torch.zeros(num_doc, num_words)\n",
    "        self.emb = torch.nn.Embedding.from_pretrained(weight, freeze=False, max_norm=None, norm_type=1)#(num_doc, num_words)\n",
    "        \n",
    "    def forward(self, doc_ids, word_vectors):\n",
    "        return self.emb(doc_ids) @ word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3ad46742",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_words, h_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(num_words, h_dim) \n",
    "        self.fc2 = nn.Linear(h_dim, h_dim)\n",
    "#         self.fc3 = nn.Linear(h_dim, h_dim)\n",
    "        self.fc4 = nn.Linear(h_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "#         x = F.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c3089bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_GAN_NDCG(model, train_loader, verbose=1):\n",
    "    results = {}\n",
    "    model.eval()\n",
    "    \n",
    "    scores = np.array(model.emb.weight.data)\n",
    "    true_relevance = train_loader.dataset.tfidf_ans\n",
    "        \n",
    "    results['ndcg@50'] = (ndcg_score(true_relevance, scores, k=50))\n",
    "    results['ndcg@100'] = (ndcg_score(true_relevance, scores, k=100))\n",
    "    results['ndcg@200'] = (ndcg_score(true_relevance, scores, k=200))\n",
    "    results['ndcg@all'] = (ndcg_score(true_relevance, scores, k=None))\n",
    "    \n",
    "    if verbose == 1:\n",
    "        print('NDCG top50', results['ndcg@50'])\n",
    "        print('NDCG top100', results['ndcg@100'])\n",
    "        print('NDCG top200', results['ndcg@200'])\n",
    "        print('NDCG ALL', results['ndcg@all'])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3ac91f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "document_vectors = np.array(dataset.document_vectors)\n",
    "document_answers_w = np.array(dataset.document_answers_w).reshape(-1, 1)\n",
    "\n",
    "len(document_vectors)\n",
    "train_test_split_ratio = 0.005\n",
    "select_num = int(len(document_vectors) * train_test_split_ratio)\n",
    "\n",
    "train_dataset = GANDataset(document_vectors[:select_num], tfidf_ans[:select_num], document_answers_w[:select_num])\n",
    "valid_dataset = GANDataset(document_vectors[select_num:], tfidf_ans[select_num:], document_answers_w[select_num:])\n",
    "\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, pin_memory=True)\n",
    "test_loader  = torch.utils.data.DataLoader(valid_dataset, batch_size=10, shuffle=True, pin_memory=True)\n",
    "\n",
    "print(select_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0ad1ef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_tensor = torch.FloatTensor(word_vectors)\n",
    "word_vectors_tensor.shape\n",
    "\n",
    "test_word_weight_tensor = torch.FloatTensor(tfidf_ans[select_num:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31d81ee",
   "metadata": {},
   "source": [
    "## start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9500eae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc578aa0d1046ed8853065427ec4b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([9, 1])) that is different to the input size (torch.Size([9])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "# setting\n",
    "lr = 0.02\n",
    "momentum = 0.99\n",
    "weight_decay = 0\n",
    "nesterov = False # True\n",
    "\n",
    "bs = 10\n",
    "n_critic = 1\n",
    "\n",
    "n_epoch = 20000\n",
    "\n",
    "loss_weight = [0, 1, 1e-3]\n",
    "weight_sum = 3\n",
    "weight_sum_mul = 0.8\n",
    "\n",
    "clip_value = 0\n",
    "\n",
    "verbose = 0\n",
    "\n",
    "model = Lasso(num_doc=select_num, num_words=word_vectors.shape[0])\n",
    "D = Discriminator(num_words=word_vectors.shape[0], h_dim=64)\n",
    "\n",
    "model.train()\n",
    "D.train()\n",
    "\n",
    "# opt_G = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "# opt_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "opt_G = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay, nesterov=nesterov)\n",
    "opt_D = torch.optim.SGD(D.parameters(), lr=lr*10)\n",
    "\n",
    "G_criterion = nn.MSELoss(reduction='mean')\n",
    "D_criterion = nn.BCELoss()\n",
    "\n",
    "results = []\n",
    "step = 0\n",
    "for epoch in tqdm(range(n_epoch)):\n",
    "    \n",
    "    D_loss = []\n",
    "    G_loss_D = []\n",
    "    G_loss_MSE = []\n",
    "    \n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        doc_embs, _, doc_ids, doc_ans_w = data\n",
    "\n",
    "        perm = torch.randperm(test_word_weight_tensor.size(0))\n",
    "        true_word_weights_sample = test_word_weight_tensor[perm[:bs]]\n",
    "\n",
    "        # Label\n",
    "        r_label = torch.ones((bs))\n",
    "        f_label = torch.zeros((doc_embs.size(0)))\n",
    "\n",
    "        r_logit = D(true_word_weights_sample.detach())\n",
    "        f_logit = D(model.emb(doc_ids).detach())\n",
    "        \n",
    "        # Compute the loss for the discriminator.\n",
    "        # r_loss = D_criterion(r_logit.squeeze(), r_label)\n",
    "        # f_loss = D_criterion(f_logit.squeeze(), f_label)\n",
    "        # loss_D = (r_loss + f_loss) / 2\n",
    "\n",
    "        # WGAN Loss\n",
    "        loss_D = -torch.mean(r_logit) + torch.mean(f_logit)\n",
    "\n",
    "        # Model backwarding\n",
    "        D.zero_grad()\n",
    "        loss_D.backward()\n",
    "\n",
    "        # Update the discriminator.\n",
    "        opt_D.step()\n",
    "        \n",
    "        D_loss.append(loss_D.item())\n",
    "        \"\"\" Clip weights of discriminator. \"\"\"\n",
    "        for p in D.parameters():\n",
    "            p.data.clamp_(-10, 10)\n",
    "\n",
    "        # ============================================\n",
    "        #  Train G\n",
    "        # ============================================\n",
    "        \n",
    "        if (epoch % n_critic == 0) and (epoch > 500):\n",
    "            # Generate some fake images.\n",
    "            f_imgs = model.emb(doc_ids)\n",
    "            f_logit = D(f_imgs)\n",
    "            \n",
    "            # Compute the loss for the generator.\n",
    "            # loss_G_Dis = D_criterion(f_logit.squeeze(), torch.ones((doc_embs.size(0))))\n",
    "            # WGAN Loss\n",
    "            loss_G_Dis = -torch.mean(f_logit)\n",
    "            \n",
    "            # MSE loss\n",
    "            pred_doc_embs = model(doc_ids, word_vectors_tensor)     \n",
    "            loss_G_MSE = G_criterion(pred_doc_embs, doc_embs)\n",
    "            \n",
    "            emb_w_sum = torch.sum(model.emb(doc_ids), axis=1)\n",
    "            \n",
    "            loss_G_SUM = G_criterion(emb_w_sum, doc_ans_w * weight_sum_mul)\n",
    "#             loss_G_SUM = G_criterion(emb_w_sum, torch.zeros(emb_w_sum.size(0)) + weight_sum)\n",
    "            \n",
    "            loss_G = loss_G_Dis * loss_weight[0] + loss_G_MSE * loss_weight[1] + loss_G_SUM * loss_weight[2]\n",
    "\n",
    "            # Model backwarding\n",
    "            model.zero_grad()\n",
    "            loss_G.backward()\n",
    "\n",
    "            # Update the generator.\n",
    "            opt_G.step()\n",
    "            \n",
    "            G_loss_D.append(loss_G_Dis.item())\n",
    "            G_loss_MSE.append(loss_G_MSE.item())\n",
    "            \n",
    "            \n",
    "            for p in model.parameters():\n",
    "                p.data.clamp_(clip_value, 100)\n",
    "        step += 1\n",
    "        \n",
    "    if epoch % 100 == 0:\n",
    "        res = {}\n",
    "        res['epoch'] = epoch\n",
    "        res['loss_D'] = np.mean(D_loss)\n",
    "        res['loss_G_D'] = np.mean(G_loss_D)\n",
    "        res['loss_G_MSE'] = np.mean(G_loss_MSE)\n",
    "\n",
    "        if verbose==1:\n",
    "            print('epoch', res['epoch'])\n",
    "            print('loss D', res['loss_D'])\n",
    "            print('loss G D', res['loss_G_D'])\n",
    "            print('loss G MSE', res['loss_G_MSE'])\n",
    "\n",
    "        res_ndcg = evaluate_GAN_NDCG(model, train_loader, verbose)\n",
    "        res.update(res_ndcg)\n",
    "        results.append(res)\n",
    "        \n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "57d2d7bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss_D</th>\n",
       "      <th>loss_G_D</th>\n",
       "      <th>loss_G_MSE</th>\n",
       "      <th>ndcg@50</th>\n",
       "      <th>ndcg@100</th>\n",
       "      <th>ndcg@200</th>\n",
       "      <th>ndcg@all</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.001895</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008099</td>\n",
       "      <td>0.012895</td>\n",
       "      <td>0.021415</td>\n",
       "      <td>0.247514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>-0.980454</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008099</td>\n",
       "      <td>0.012895</td>\n",
       "      <td>0.021415</td>\n",
       "      <td>0.247514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>-0.999146</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008099</td>\n",
       "      <td>0.012895</td>\n",
       "      <td>0.021415</td>\n",
       "      <td>0.247514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>-0.999444</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008099</td>\n",
       "      <td>0.012895</td>\n",
       "      <td>0.021415</td>\n",
       "      <td>0.247514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>-0.999631</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008099</td>\n",
       "      <td>0.012895</td>\n",
       "      <td>0.021415</td>\n",
       "      <td>0.247514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>-0.999481</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008099</td>\n",
       "      <td>0.012895</td>\n",
       "      <td>0.021415</td>\n",
       "      <td>0.247514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>-0.999851</td>\n",
       "      <td>-1.481614e-04</td>\n",
       "      <td>0.057759</td>\n",
       "      <td>0.598208</td>\n",
       "      <td>0.624929</td>\n",
       "      <td>0.661603</td>\n",
       "      <td>0.745640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>-0.999827</td>\n",
       "      <td>-1.571228e-04</td>\n",
       "      <td>0.002023</td>\n",
       "      <td>0.618567</td>\n",
       "      <td>0.649635</td>\n",
       "      <td>0.685382</td>\n",
       "      <td>0.763016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>-0.999667</td>\n",
       "      <td>-3.276147e-04</td>\n",
       "      <td>0.001237</td>\n",
       "      <td>0.628725</td>\n",
       "      <td>0.660311</td>\n",
       "      <td>0.692246</td>\n",
       "      <td>0.767730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>-0.999892</td>\n",
       "      <td>-8.614675e-05</td>\n",
       "      <td>0.013078</td>\n",
       "      <td>0.637811</td>\n",
       "      <td>0.664810</td>\n",
       "      <td>0.695085</td>\n",
       "      <td>0.771315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>-0.999899</td>\n",
       "      <td>-6.971505e-05</td>\n",
       "      <td>0.066503</td>\n",
       "      <td>0.642192</td>\n",
       "      <td>0.667077</td>\n",
       "      <td>0.696589</td>\n",
       "      <td>0.773507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>-0.999926</td>\n",
       "      <td>-6.942534e-05</td>\n",
       "      <td>0.012958</td>\n",
       "      <td>0.648879</td>\n",
       "      <td>0.671361</td>\n",
       "      <td>0.702396</td>\n",
       "      <td>0.778186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>-0.999832</td>\n",
       "      <td>-1.659752e-04</td>\n",
       "      <td>0.025448</td>\n",
       "      <td>0.652019</td>\n",
       "      <td>0.673274</td>\n",
       "      <td>0.704315</td>\n",
       "      <td>0.778939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>-0.999841</td>\n",
       "      <td>-9.283122e-05</td>\n",
       "      <td>0.064390</td>\n",
       "      <td>0.655234</td>\n",
       "      <td>0.676000</td>\n",
       "      <td>0.705575</td>\n",
       "      <td>0.780254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>-0.999943</td>\n",
       "      <td>-5.719247e-05</td>\n",
       "      <td>0.011613</td>\n",
       "      <td>0.659305</td>\n",
       "      <td>0.679989</td>\n",
       "      <td>0.707939</td>\n",
       "      <td>0.782533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>-0.999946</td>\n",
       "      <td>-5.379543e-05</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.661165</td>\n",
       "      <td>0.683345</td>\n",
       "      <td>0.710238</td>\n",
       "      <td>0.784268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>-0.979969</td>\n",
       "      <td>-3.478895e-05</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.662769</td>\n",
       "      <td>0.685343</td>\n",
       "      <td>0.711048</td>\n",
       "      <td>0.785293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700</th>\n",
       "      <td>-0.987525</td>\n",
       "      <td>-6.983025e-05</td>\n",
       "      <td>0.007694</td>\n",
       "      <td>0.663243</td>\n",
       "      <td>0.687253</td>\n",
       "      <td>0.712262</td>\n",
       "      <td>0.785819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>-0.999962</td>\n",
       "      <td>-3.305814e-05</td>\n",
       "      <td>0.001716</td>\n",
       "      <td>0.664761</td>\n",
       "      <td>0.688913</td>\n",
       "      <td>0.714079</td>\n",
       "      <td>0.787815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>-0.999968</td>\n",
       "      <td>-3.064398e-05</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.667207</td>\n",
       "      <td>0.691146</td>\n",
       "      <td>0.716086</td>\n",
       "      <td>0.789366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>-0.999963</td>\n",
       "      <td>-3.129975e-05</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.668517</td>\n",
       "      <td>0.692860</td>\n",
       "      <td>0.716680</td>\n",
       "      <td>0.789434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2100</th>\n",
       "      <td>-0.999938</td>\n",
       "      <td>-2.836906e-05</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.666828</td>\n",
       "      <td>0.691841</td>\n",
       "      <td>0.715420</td>\n",
       "      <td>0.788382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>-0.999975</td>\n",
       "      <td>-2.442285e-05</td>\n",
       "      <td>0.003854</td>\n",
       "      <td>0.668634</td>\n",
       "      <td>0.693615</td>\n",
       "      <td>0.716249</td>\n",
       "      <td>0.789123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>-0.999975</td>\n",
       "      <td>-2.458235e-05</td>\n",
       "      <td>0.004347</td>\n",
       "      <td>0.669969</td>\n",
       "      <td>0.694169</td>\n",
       "      <td>0.716475</td>\n",
       "      <td>0.789410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2400</th>\n",
       "      <td>-0.999963</td>\n",
       "      <td>-2.236971e-05</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>0.671188</td>\n",
       "      <td>0.695388</td>\n",
       "      <td>0.717818</td>\n",
       "      <td>0.790356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2500</th>\n",
       "      <td>-0.999954</td>\n",
       "      <td>-2.063600e-05</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.672226</td>\n",
       "      <td>0.696296</td>\n",
       "      <td>0.717875</td>\n",
       "      <td>0.790530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600</th>\n",
       "      <td>-0.999973</td>\n",
       "      <td>-2.100737e-05</td>\n",
       "      <td>0.002595</td>\n",
       "      <td>0.674065</td>\n",
       "      <td>0.699523</td>\n",
       "      <td>0.720714</td>\n",
       "      <td>0.793529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2700</th>\n",
       "      <td>-0.999978</td>\n",
       "      <td>-2.020010e-05</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.676287</td>\n",
       "      <td>0.699426</td>\n",
       "      <td>0.720928</td>\n",
       "      <td>0.794276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2800</th>\n",
       "      <td>-0.999979</td>\n",
       "      <td>-1.878868e-05</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.677030</td>\n",
       "      <td>0.700699</td>\n",
       "      <td>0.720763</td>\n",
       "      <td>0.794283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2900</th>\n",
       "      <td>-0.999981</td>\n",
       "      <td>-1.777276e-05</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.679120</td>\n",
       "      <td>0.702603</td>\n",
       "      <td>0.722067</td>\n",
       "      <td>0.795585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>-0.999983</td>\n",
       "      <td>-1.663363e-05</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.681072</td>\n",
       "      <td>0.703597</td>\n",
       "      <td>0.723405</td>\n",
       "      <td>0.796611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3100</th>\n",
       "      <td>-0.999984</td>\n",
       "      <td>-1.642199e-05</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.683298</td>\n",
       "      <td>0.704857</td>\n",
       "      <td>0.722955</td>\n",
       "      <td>0.797554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3200</th>\n",
       "      <td>-0.999981</td>\n",
       "      <td>-1.506192e-05</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.683681</td>\n",
       "      <td>0.705884</td>\n",
       "      <td>0.723912</td>\n",
       "      <td>0.797979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3300</th>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-1.440353e-05</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.684795</td>\n",
       "      <td>0.706261</td>\n",
       "      <td>0.724660</td>\n",
       "      <td>0.798970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3400</th>\n",
       "      <td>-0.999981</td>\n",
       "      <td>-1.390213e-05</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.685209</td>\n",
       "      <td>0.706953</td>\n",
       "      <td>0.725334</td>\n",
       "      <td>0.799086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3500</th>\n",
       "      <td>-0.999984</td>\n",
       "      <td>-1.393705e-05</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.686541</td>\n",
       "      <td>0.707434</td>\n",
       "      <td>0.726668</td>\n",
       "      <td>0.800007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3600</th>\n",
       "      <td>-0.999983</td>\n",
       "      <td>-1.327090e-05</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.687824</td>\n",
       "      <td>0.707902</td>\n",
       "      <td>0.726929</td>\n",
       "      <td>0.800534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3700</th>\n",
       "      <td>-0.999982</td>\n",
       "      <td>-1.775959e-05</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.687920</td>\n",
       "      <td>0.707978</td>\n",
       "      <td>0.727257</td>\n",
       "      <td>0.800825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3800</th>\n",
       "      <td>-0.999984</td>\n",
       "      <td>-1.590376e-05</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.688226</td>\n",
       "      <td>0.708905</td>\n",
       "      <td>0.727922</td>\n",
       "      <td>0.801124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3900</th>\n",
       "      <td>-0.999979</td>\n",
       "      <td>-1.419336e-05</td>\n",
       "      <td>0.005134</td>\n",
       "      <td>0.690259</td>\n",
       "      <td>0.710300</td>\n",
       "      <td>0.727647</td>\n",
       "      <td>0.801679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>-0.999988</td>\n",
       "      <td>-1.206503e-05</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.691521</td>\n",
       "      <td>0.711594</td>\n",
       "      <td>0.730014</td>\n",
       "      <td>0.803892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4100</th>\n",
       "      <td>-0.999987</td>\n",
       "      <td>-9.696865e-06</td>\n",
       "      <td>0.004859</td>\n",
       "      <td>0.689147</td>\n",
       "      <td>0.709410</td>\n",
       "      <td>0.727404</td>\n",
       "      <td>0.802259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4200</th>\n",
       "      <td>-0.999990</td>\n",
       "      <td>-1.013399e-05</td>\n",
       "      <td>0.000718</td>\n",
       "      <td>0.693125</td>\n",
       "      <td>0.712091</td>\n",
       "      <td>0.729587</td>\n",
       "      <td>0.803551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4300</th>\n",
       "      <td>-0.999990</td>\n",
       "      <td>-1.025327e-05</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.693542</td>\n",
       "      <td>0.713452</td>\n",
       "      <td>0.731111</td>\n",
       "      <td>0.804632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4400</th>\n",
       "      <td>-0.979988</td>\n",
       "      <td>-1.004536e-05</td>\n",
       "      <td>0.006413</td>\n",
       "      <td>0.692820</td>\n",
       "      <td>0.712718</td>\n",
       "      <td>0.729258</td>\n",
       "      <td>0.803478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4500</th>\n",
       "      <td>-0.999973</td>\n",
       "      <td>-9.291866e-06</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.692709</td>\n",
       "      <td>0.712839</td>\n",
       "      <td>0.729334</td>\n",
       "      <td>0.803756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-9.062495e-06</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.693701</td>\n",
       "      <td>0.713609</td>\n",
       "      <td>0.729081</td>\n",
       "      <td>0.803643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4700</th>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-9.006162e-06</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.693027</td>\n",
       "      <td>0.714008</td>\n",
       "      <td>0.730181</td>\n",
       "      <td>0.804149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4800</th>\n",
       "      <td>-0.999987</td>\n",
       "      <td>-8.679513e-06</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.697507</td>\n",
       "      <td>0.718177</td>\n",
       "      <td>0.732386</td>\n",
       "      <td>0.807180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4900</th>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-8.436908e-06</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>0.695901</td>\n",
       "      <td>0.716097</td>\n",
       "      <td>0.730914</td>\n",
       "      <td>0.805092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-8.193574e-06</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.696150</td>\n",
       "      <td>0.715554</td>\n",
       "      <td>0.730629</td>\n",
       "      <td>0.805226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5100</th>\n",
       "      <td>-0.999931</td>\n",
       "      <td>-8.146164e-06</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.697915</td>\n",
       "      <td>0.717609</td>\n",
       "      <td>0.731821</td>\n",
       "      <td>0.806324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5200</th>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-7.751985e-06</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.698124</td>\n",
       "      <td>0.717680</td>\n",
       "      <td>0.733400</td>\n",
       "      <td>0.806987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5300</th>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-7.546771e-06</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.699087</td>\n",
       "      <td>0.718002</td>\n",
       "      <td>0.732345</td>\n",
       "      <td>0.806622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5400</th>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-7.583595e-06</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.699050</td>\n",
       "      <td>0.717560</td>\n",
       "      <td>0.732288</td>\n",
       "      <td>0.806596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5500</th>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-7.348020e-06</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.698962</td>\n",
       "      <td>0.717541</td>\n",
       "      <td>0.732603</td>\n",
       "      <td>0.806602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5600</th>\n",
       "      <td>-0.979993</td>\n",
       "      <td>-7.219431e-06</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.698495</td>\n",
       "      <td>0.717419</td>\n",
       "      <td>0.732127</td>\n",
       "      <td>0.806717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5700</th>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-7.099346e-06</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.698432</td>\n",
       "      <td>0.717563</td>\n",
       "      <td>0.732403</td>\n",
       "      <td>0.806965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5800</th>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-6.993651e-06</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.698641</td>\n",
       "      <td>0.717769</td>\n",
       "      <td>0.732334</td>\n",
       "      <td>0.806699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5900</th>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-6.790196e-06</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.698940</td>\n",
       "      <td>0.717593</td>\n",
       "      <td>0.733056</td>\n",
       "      <td>0.807314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6000</th>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-6.665833e-06</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.699254</td>\n",
       "      <td>0.717891</td>\n",
       "      <td>0.732904</td>\n",
       "      <td>0.807176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6100</th>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-5.892565e-06</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.699422</td>\n",
       "      <td>0.717875</td>\n",
       "      <td>0.733460</td>\n",
       "      <td>0.807327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6200</th>\n",
       "      <td>-0.999988</td>\n",
       "      <td>-5.788800e-06</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.699438</td>\n",
       "      <td>0.718114</td>\n",
       "      <td>0.733585</td>\n",
       "      <td>0.807086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6300</th>\n",
       "      <td>-0.999976</td>\n",
       "      <td>-5.782969e-06</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.700634</td>\n",
       "      <td>0.718157</td>\n",
       "      <td>0.733908</td>\n",
       "      <td>0.807354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6400</th>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-5.704556e-06</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.701055</td>\n",
       "      <td>0.717915</td>\n",
       "      <td>0.734239</td>\n",
       "      <td>0.807294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6500</th>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-4.077210e-06</td>\n",
       "      <td>0.002250</td>\n",
       "      <td>0.700619</td>\n",
       "      <td>0.717282</td>\n",
       "      <td>0.733611</td>\n",
       "      <td>0.807158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6600</th>\n",
       "      <td>-0.999969</td>\n",
       "      <td>-4.255765e-06</td>\n",
       "      <td>0.016797</td>\n",
       "      <td>0.701421</td>\n",
       "      <td>0.718941</td>\n",
       "      <td>0.732778</td>\n",
       "      <td>0.807804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6700</th>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-5.028838e-06</td>\n",
       "      <td>0.163303</td>\n",
       "      <td>0.697829</td>\n",
       "      <td>0.714767</td>\n",
       "      <td>0.727196</td>\n",
       "      <td>0.804484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6800</th>\n",
       "      <td>-0.999995</td>\n",
       "      <td>-4.137961e-06</td>\n",
       "      <td>0.025594</td>\n",
       "      <td>0.699843</td>\n",
       "      <td>0.715822</td>\n",
       "      <td>0.730331</td>\n",
       "      <td>0.806006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6900</th>\n",
       "      <td>-0.999911</td>\n",
       "      <td>-4.179785e-06</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>0.700660</td>\n",
       "      <td>0.718077</td>\n",
       "      <td>0.733303</td>\n",
       "      <td>0.807967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7000</th>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-4.343177e-06</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.701019</td>\n",
       "      <td>0.718265</td>\n",
       "      <td>0.733110</td>\n",
       "      <td>0.808040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7100</th>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-4.359742e-06</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.700897</td>\n",
       "      <td>0.718305</td>\n",
       "      <td>0.733365</td>\n",
       "      <td>0.807924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7200</th>\n",
       "      <td>-0.999995</td>\n",
       "      <td>-4.350397e-06</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.701409</td>\n",
       "      <td>0.718631</td>\n",
       "      <td>0.734648</td>\n",
       "      <td>0.808108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7300</th>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-4.409692e-06</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.701492</td>\n",
       "      <td>0.718710</td>\n",
       "      <td>0.732758</td>\n",
       "      <td>0.807606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7400</th>\n",
       "      <td>-0.999995</td>\n",
       "      <td>-4.363766e-06</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.701592</td>\n",
       "      <td>0.718532</td>\n",
       "      <td>0.734411</td>\n",
       "      <td>0.808674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7500</th>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-4.362536e-06</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.701802</td>\n",
       "      <td>0.719079</td>\n",
       "      <td>0.733914</td>\n",
       "      <td>0.808346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7600</th>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-4.334925e-06</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.702854</td>\n",
       "      <td>0.719897</td>\n",
       "      <td>0.735949</td>\n",
       "      <td>0.809448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7700</th>\n",
       "      <td>-0.999995</td>\n",
       "      <td>-4.296562e-06</td>\n",
       "      <td>0.001050</td>\n",
       "      <td>0.702274</td>\n",
       "      <td>0.720163</td>\n",
       "      <td>0.735291</td>\n",
       "      <td>0.809653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7800</th>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-4.239721e-06</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>0.702944</td>\n",
       "      <td>0.720808</td>\n",
       "      <td>0.735706</td>\n",
       "      <td>0.810123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7900</th>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-3.104945e-06</td>\n",
       "      <td>0.000371</td>\n",
       "      <td>0.701371</td>\n",
       "      <td>0.719686</td>\n",
       "      <td>0.735436</td>\n",
       "      <td>0.809691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8000</th>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-3.195423e-06</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>0.704266</td>\n",
       "      <td>0.720279</td>\n",
       "      <td>0.736711</td>\n",
       "      <td>0.810391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8100</th>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-3.275651e-06</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.704242</td>\n",
       "      <td>0.720378</td>\n",
       "      <td>0.737358</td>\n",
       "      <td>0.810603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8200</th>\n",
       "      <td>-0.999995</td>\n",
       "      <td>-3.348633e-06</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.703884</td>\n",
       "      <td>0.720690</td>\n",
       "      <td>0.736494</td>\n",
       "      <td>0.811188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8300</th>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-3.346440e-06</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.703603</td>\n",
       "      <td>0.720962</td>\n",
       "      <td>0.735270</td>\n",
       "      <td>0.809779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8400</th>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-3.419109e-06</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.704057</td>\n",
       "      <td>0.720666</td>\n",
       "      <td>0.735047</td>\n",
       "      <td>0.809946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8500</th>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-3.612733e-06</td>\n",
       "      <td>0.030314</td>\n",
       "      <td>0.703526</td>\n",
       "      <td>0.721305</td>\n",
       "      <td>0.735695</td>\n",
       "      <td>0.809569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8600</th>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-3.402795e-06</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.705365</td>\n",
       "      <td>0.722106</td>\n",
       "      <td>0.737104</td>\n",
       "      <td>0.811425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8700</th>\n",
       "      <td>-0.999990</td>\n",
       "      <td>-3.405827e-06</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.705275</td>\n",
       "      <td>0.721846</td>\n",
       "      <td>0.736562</td>\n",
       "      <td>0.811039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8800</th>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-3.454762e-06</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.704803</td>\n",
       "      <td>0.721165</td>\n",
       "      <td>0.735939</td>\n",
       "      <td>0.810799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8900</th>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-3.414307e-06</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.704669</td>\n",
       "      <td>0.720848</td>\n",
       "      <td>0.736144</td>\n",
       "      <td>0.810313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9000</th>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-3.401304e-06</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.704176</td>\n",
       "      <td>0.720231</td>\n",
       "      <td>0.735530</td>\n",
       "      <td>0.809840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9100</th>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-3.400564e-06</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.704521</td>\n",
       "      <td>0.720507</td>\n",
       "      <td>0.736276</td>\n",
       "      <td>0.810035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9200</th>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-3.288954e-06</td>\n",
       "      <td>0.001610</td>\n",
       "      <td>0.705091</td>\n",
       "      <td>0.719938</td>\n",
       "      <td>0.734944</td>\n",
       "      <td>0.809942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9300</th>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-3.274032e-06</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.704844</td>\n",
       "      <td>0.719600</td>\n",
       "      <td>0.735862</td>\n",
       "      <td>0.809759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9400</th>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-3.267503e-06</td>\n",
       "      <td>0.003189</td>\n",
       "      <td>0.704579</td>\n",
       "      <td>0.719716</td>\n",
       "      <td>0.734549</td>\n",
       "      <td>0.809275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9500</th>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-3.244874e-06</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.704632</td>\n",
       "      <td>0.720040</td>\n",
       "      <td>0.734715</td>\n",
       "      <td>0.809202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9600</th>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-3.225387e-06</td>\n",
       "      <td>0.007133</td>\n",
       "      <td>0.704850</td>\n",
       "      <td>0.721304</td>\n",
       "      <td>0.735851</td>\n",
       "      <td>0.809677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9700</th>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-3.184694e-06</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>0.705225</td>\n",
       "      <td>0.721072</td>\n",
       "      <td>0.735725</td>\n",
       "      <td>0.810889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9800</th>\n",
       "      <td>-0.979997</td>\n",
       "      <td>-3.191098e-06</td>\n",
       "      <td>0.000216</td>\n",
       "      <td>0.706138</td>\n",
       "      <td>0.721304</td>\n",
       "      <td>0.736473</td>\n",
       "      <td>0.811057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9900</th>\n",
       "      <td>-0.999981</td>\n",
       "      <td>-1.932523e-05</td>\n",
       "      <td>0.180582</td>\n",
       "      <td>0.704856</td>\n",
       "      <td>0.720418</td>\n",
       "      <td>0.736274</td>\n",
       "      <td>0.810704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>-0.999879</td>\n",
       "      <td>-2.087147e-06</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.705818</td>\n",
       "      <td>0.721734</td>\n",
       "      <td>0.736547</td>\n",
       "      <td>0.811249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10100</th>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-2.172745e-06</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.704207</td>\n",
       "      <td>0.719536</td>\n",
       "      <td>0.734362</td>\n",
       "      <td>0.809539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10200</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-2.236756e-06</td>\n",
       "      <td>0.008838</td>\n",
       "      <td>0.703437</td>\n",
       "      <td>0.719637</td>\n",
       "      <td>0.734462</td>\n",
       "      <td>0.809422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10300</th>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-2.287886e-06</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.704742</td>\n",
       "      <td>0.720339</td>\n",
       "      <td>0.734826</td>\n",
       "      <td>0.810216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10400</th>\n",
       "      <td>-0.999995</td>\n",
       "      <td>-2.331928e-06</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.705090</td>\n",
       "      <td>0.720545</td>\n",
       "      <td>0.736334</td>\n",
       "      <td>0.811485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10500</th>\n",
       "      <td>-0.979998</td>\n",
       "      <td>-2.389250e-06</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.705450</td>\n",
       "      <td>0.720671</td>\n",
       "      <td>0.736048</td>\n",
       "      <td>0.810660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10600</th>\n",
       "      <td>-0.999987</td>\n",
       "      <td>-2.409708e-06</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.705150</td>\n",
       "      <td>0.720235</td>\n",
       "      <td>0.735530</td>\n",
       "      <td>0.811066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10700</th>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-2.433437e-06</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.705168</td>\n",
       "      <td>0.720235</td>\n",
       "      <td>0.735321</td>\n",
       "      <td>0.810187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10800</th>\n",
       "      <td>-0.999988</td>\n",
       "      <td>-2.434245e-06</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.705237</td>\n",
       "      <td>0.720276</td>\n",
       "      <td>0.735780</td>\n",
       "      <td>0.811242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10900</th>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-2.438869e-06</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>0.705273</td>\n",
       "      <td>0.719962</td>\n",
       "      <td>0.735621</td>\n",
       "      <td>0.810939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11000</th>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-2.473101e-06</td>\n",
       "      <td>0.008306</td>\n",
       "      <td>0.705502</td>\n",
       "      <td>0.720600</td>\n",
       "      <td>0.736771</td>\n",
       "      <td>0.811163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11100</th>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-2.470467e-06</td>\n",
       "      <td>0.003304</td>\n",
       "      <td>0.705435</td>\n",
       "      <td>0.720032</td>\n",
       "      <td>0.734227</td>\n",
       "      <td>0.810481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11200</th>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-2.469569e-06</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.705130</td>\n",
       "      <td>0.720406</td>\n",
       "      <td>0.734220</td>\n",
       "      <td>0.810058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11300</th>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-2.443314e-06</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.705187</td>\n",
       "      <td>0.720457</td>\n",
       "      <td>0.734880</td>\n",
       "      <td>0.810390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11400</th>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-2.447085e-06</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.705277</td>\n",
       "      <td>0.720542</td>\n",
       "      <td>0.734753</td>\n",
       "      <td>0.810572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11500</th>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-2.612672e-06</td>\n",
       "      <td>0.054365</td>\n",
       "      <td>0.704500</td>\n",
       "      <td>0.718630</td>\n",
       "      <td>0.734237</td>\n",
       "      <td>0.810213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11600</th>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-2.456037e-06</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.705513</td>\n",
       "      <td>0.719424</td>\n",
       "      <td>0.733553</td>\n",
       "      <td>0.810135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11700</th>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-2.446187e-06</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.705752</td>\n",
       "      <td>0.719470</td>\n",
       "      <td>0.733824</td>\n",
       "      <td>0.810273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11800</th>\n",
       "      <td>-0.999995</td>\n",
       "      <td>-2.432680e-06</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.705762</td>\n",
       "      <td>0.719648</td>\n",
       "      <td>0.733622</td>\n",
       "      <td>0.809983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11900</th>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-2.379691e-06</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.705531</td>\n",
       "      <td>0.719296</td>\n",
       "      <td>0.734490</td>\n",
       "      <td>0.810103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12000</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-2.368193e-06</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.705791</td>\n",
       "      <td>0.719554</td>\n",
       "      <td>0.733545</td>\n",
       "      <td>0.810142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12100</th>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-2.389424e-06</td>\n",
       "      <td>0.005456</td>\n",
       "      <td>0.706302</td>\n",
       "      <td>0.720104</td>\n",
       "      <td>0.734532</td>\n",
       "      <td>0.810287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12200</th>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-1.907995e-06</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.706405</td>\n",
       "      <td>0.720186</td>\n",
       "      <td>0.734633</td>\n",
       "      <td>0.810151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12300</th>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-1.931477e-06</td>\n",
       "      <td>0.004360</td>\n",
       "      <td>0.706400</td>\n",
       "      <td>0.720546</td>\n",
       "      <td>0.733884</td>\n",
       "      <td>0.810559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12400</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.945124e-06</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>0.705781</td>\n",
       "      <td>0.718966</td>\n",
       "      <td>0.732553</td>\n",
       "      <td>0.810109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12500</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.955255e-06</td>\n",
       "      <td>0.002584</td>\n",
       "      <td>0.705930</td>\n",
       "      <td>0.719926</td>\n",
       "      <td>0.735149</td>\n",
       "      <td>0.810470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12600</th>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-1.727855e-06</td>\n",
       "      <td>0.003607</td>\n",
       "      <td>0.704352</td>\n",
       "      <td>0.718991</td>\n",
       "      <td>0.733738</td>\n",
       "      <td>0.810401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12700</th>\n",
       "      <td>-0.979993</td>\n",
       "      <td>-1.747701e-06</td>\n",
       "      <td>0.022876</td>\n",
       "      <td>0.704944</td>\n",
       "      <td>0.718060</td>\n",
       "      <td>0.732058</td>\n",
       "      <td>0.809847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12800</th>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-1.773110e-06</td>\n",
       "      <td>0.008202</td>\n",
       "      <td>0.706232</td>\n",
       "      <td>0.721181</td>\n",
       "      <td>0.736342</td>\n",
       "      <td>0.811215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12900</th>\n",
       "      <td>-0.999995</td>\n",
       "      <td>-1.787298e-06</td>\n",
       "      <td>0.011965</td>\n",
       "      <td>0.706300</td>\n",
       "      <td>0.720945</td>\n",
       "      <td>0.734707</td>\n",
       "      <td>0.811000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13000</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.815864e-06</td>\n",
       "      <td>0.015554</td>\n",
       "      <td>0.706347</td>\n",
       "      <td>0.721628</td>\n",
       "      <td>0.736033</td>\n",
       "      <td>0.811153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13100</th>\n",
       "      <td>-0.999979</td>\n",
       "      <td>-1.825225e-06</td>\n",
       "      <td>0.002915</td>\n",
       "      <td>0.706624</td>\n",
       "      <td>0.721055</td>\n",
       "      <td>0.734328</td>\n",
       "      <td>0.810817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13200</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.843091e-06</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.706415</td>\n",
       "      <td>0.720345</td>\n",
       "      <td>0.734947</td>\n",
       "      <td>0.811133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13300</th>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-1.859893e-06</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.707233</td>\n",
       "      <td>0.721078</td>\n",
       "      <td>0.734797</td>\n",
       "      <td>0.810770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13400</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.867351e-06</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.706385</td>\n",
       "      <td>0.721108</td>\n",
       "      <td>0.735290</td>\n",
       "      <td>0.810932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13500</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.869064e-06</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.706527</td>\n",
       "      <td>0.721449</td>\n",
       "      <td>0.735315</td>\n",
       "      <td>0.810660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13600</th>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-1.875058e-06</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.708046</td>\n",
       "      <td>0.722940</td>\n",
       "      <td>0.736431</td>\n",
       "      <td>0.812652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13700</th>\n",
       "      <td>-0.999980</td>\n",
       "      <td>-1.895430e-06</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.707805</td>\n",
       "      <td>0.722844</td>\n",
       "      <td>0.735771</td>\n",
       "      <td>0.811940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13800</th>\n",
       "      <td>-0.999995</td>\n",
       "      <td>-1.898428e-06</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.707810</td>\n",
       "      <td>0.722798</td>\n",
       "      <td>0.736292</td>\n",
       "      <td>0.811898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13900</th>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-1.899244e-06</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.707751</td>\n",
       "      <td>0.722418</td>\n",
       "      <td>0.735352</td>\n",
       "      <td>0.811629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14000</th>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-1.898032e-06</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.707806</td>\n",
       "      <td>0.722448</td>\n",
       "      <td>0.735350</td>\n",
       "      <td>0.811786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14100</th>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-1.931867e-06</td>\n",
       "      <td>0.030498</td>\n",
       "      <td>0.707766</td>\n",
       "      <td>0.722670</td>\n",
       "      <td>0.736120</td>\n",
       "      <td>0.812067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14200</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.893726e-06</td>\n",
       "      <td>0.005563</td>\n",
       "      <td>0.707978</td>\n",
       "      <td>0.722096</td>\n",
       "      <td>0.735014</td>\n",
       "      <td>0.812044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14300</th>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-1.887322e-06</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>0.708019</td>\n",
       "      <td>0.721901</td>\n",
       "      <td>0.736308</td>\n",
       "      <td>0.812273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14400</th>\n",
       "      <td>-0.979998</td>\n",
       "      <td>-1.905503e-06</td>\n",
       "      <td>0.024703</td>\n",
       "      <td>0.706655</td>\n",
       "      <td>0.722110</td>\n",
       "      <td>0.734877</td>\n",
       "      <td>0.812583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14500</th>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-1.878936e-06</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.706164</td>\n",
       "      <td>0.721683</td>\n",
       "      <td>0.735652</td>\n",
       "      <td>0.811652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14600</th>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-1.658318e-06</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>0.708816</td>\n",
       "      <td>0.723226</td>\n",
       "      <td>0.736902</td>\n",
       "      <td>0.813731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14700</th>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-1.646587e-06</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.707453</td>\n",
       "      <td>0.722038</td>\n",
       "      <td>0.736368</td>\n",
       "      <td>0.812785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14800</th>\n",
       "      <td>-0.999984</td>\n",
       "      <td>-1.644914e-06</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.708094</td>\n",
       "      <td>0.722293</td>\n",
       "      <td>0.734724</td>\n",
       "      <td>0.812397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14900</th>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-1.649880e-06</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.708085</td>\n",
       "      <td>0.722383</td>\n",
       "      <td>0.735622</td>\n",
       "      <td>0.811917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.656846e-06</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.708823</td>\n",
       "      <td>0.723145</td>\n",
       "      <td>0.737165</td>\n",
       "      <td>0.812706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15100</th>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-1.661490e-06</td>\n",
       "      <td>0.003268</td>\n",
       "      <td>0.707910</td>\n",
       "      <td>0.722769</td>\n",
       "      <td>0.735856</td>\n",
       "      <td>0.812249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15200</th>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-1.661770e-06</td>\n",
       "      <td>0.001477</td>\n",
       "      <td>0.707681</td>\n",
       "      <td>0.723027</td>\n",
       "      <td>0.737880</td>\n",
       "      <td>0.813041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15300</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.624850e-06</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.709045</td>\n",
       "      <td>0.723743</td>\n",
       "      <td>0.738152</td>\n",
       "      <td>0.813436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15400</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.623063e-06</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.708746</td>\n",
       "      <td>0.723150</td>\n",
       "      <td>0.737953</td>\n",
       "      <td>0.812965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15500</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.620553e-06</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.709111</td>\n",
       "      <td>0.723375</td>\n",
       "      <td>0.737429</td>\n",
       "      <td>0.813304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15600</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.635409e-06</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.709233</td>\n",
       "      <td>0.723290</td>\n",
       "      <td>0.738028</td>\n",
       "      <td>0.812957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15700</th>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-1.620744e-06</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.709268</td>\n",
       "      <td>0.723156</td>\n",
       "      <td>0.739042</td>\n",
       "      <td>0.813601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15800</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.623299e-06</td>\n",
       "      <td>0.017088</td>\n",
       "      <td>0.708856</td>\n",
       "      <td>0.722537</td>\n",
       "      <td>0.737312</td>\n",
       "      <td>0.812534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15900</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.633719e-06</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.708943</td>\n",
       "      <td>0.722986</td>\n",
       "      <td>0.737615</td>\n",
       "      <td>0.812746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16000</th>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-1.613158e-06</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.709083</td>\n",
       "      <td>0.722908</td>\n",
       "      <td>0.737532</td>\n",
       "      <td>0.812329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16100</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.610409e-06</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.708974</td>\n",
       "      <td>0.722653</td>\n",
       "      <td>0.737595</td>\n",
       "      <td>0.812978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16200</th>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-1.539681e-06</td>\n",
       "      <td>0.046228</td>\n",
       "      <td>0.706528</td>\n",
       "      <td>0.720450</td>\n",
       "      <td>0.735640</td>\n",
       "      <td>0.811548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16300</th>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-1.551063e-06</td>\n",
       "      <td>0.001798</td>\n",
       "      <td>0.708176</td>\n",
       "      <td>0.722294</td>\n",
       "      <td>0.737972</td>\n",
       "      <td>0.813147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16400</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.534144e-06</td>\n",
       "      <td>0.003946</td>\n",
       "      <td>0.708795</td>\n",
       "      <td>0.721969</td>\n",
       "      <td>0.737099</td>\n",
       "      <td>0.812506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16500</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.528574e-06</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.708401</td>\n",
       "      <td>0.722122</td>\n",
       "      <td>0.736898</td>\n",
       "      <td>0.812789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16600</th>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-1.531863e-06</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.708819</td>\n",
       "      <td>0.723333</td>\n",
       "      <td>0.736663</td>\n",
       "      <td>0.812776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16700</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.536983e-06</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.708558</td>\n",
       "      <td>0.721794</td>\n",
       "      <td>0.737944</td>\n",
       "      <td>0.813626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16800</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.553405e-06</td>\n",
       "      <td>0.015128</td>\n",
       "      <td>0.709097</td>\n",
       "      <td>0.721820</td>\n",
       "      <td>0.737917</td>\n",
       "      <td>0.813196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16900</th>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-1.526106e-06</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.710035</td>\n",
       "      <td>0.722554</td>\n",
       "      <td>0.738138</td>\n",
       "      <td>0.813431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17000</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.527011e-06</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.710033</td>\n",
       "      <td>0.722671</td>\n",
       "      <td>0.737755</td>\n",
       "      <td>0.812778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17100</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.523557e-06</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.710200</td>\n",
       "      <td>0.722987</td>\n",
       "      <td>0.738043</td>\n",
       "      <td>0.814074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17200</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.518733e-06</td>\n",
       "      <td>0.000429</td>\n",
       "      <td>0.709145</td>\n",
       "      <td>0.722544</td>\n",
       "      <td>0.737115</td>\n",
       "      <td>0.813470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17300</th>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-1.514764e-06</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.710060</td>\n",
       "      <td>0.723062</td>\n",
       "      <td>0.738142</td>\n",
       "      <td>0.813436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17400</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.523140e-06</td>\n",
       "      <td>0.001546</td>\n",
       "      <td>0.710669</td>\n",
       "      <td>0.724200</td>\n",
       "      <td>0.738215</td>\n",
       "      <td>0.813835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17500</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.508513e-06</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.710258</td>\n",
       "      <td>0.723474</td>\n",
       "      <td>0.737145</td>\n",
       "      <td>0.813216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17600</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.506957e-06</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.710319</td>\n",
       "      <td>0.723281</td>\n",
       "      <td>0.737092</td>\n",
       "      <td>0.813602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17700</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.502204e-06</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.710569</td>\n",
       "      <td>0.723380</td>\n",
       "      <td>0.738913</td>\n",
       "      <td>0.814078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17800</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.503031e-06</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.710083</td>\n",
       "      <td>0.723571</td>\n",
       "      <td>0.737582</td>\n",
       "      <td>0.813231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17900</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.495682e-06</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.710050</td>\n",
       "      <td>0.723284</td>\n",
       "      <td>0.739139</td>\n",
       "      <td>0.814031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18000</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.494632e-06</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.709815</td>\n",
       "      <td>0.723294</td>\n",
       "      <td>0.738642</td>\n",
       "      <td>0.813757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18100</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.490748e-06</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>0.709398</td>\n",
       "      <td>0.722174</td>\n",
       "      <td>0.736248</td>\n",
       "      <td>0.812143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18200</th>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-1.064179e-06</td>\n",
       "      <td>0.002786</td>\n",
       "      <td>0.708778</td>\n",
       "      <td>0.722143</td>\n",
       "      <td>0.736779</td>\n",
       "      <td>0.812435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18300</th>\n",
       "      <td>-0.999999</td>\n",
       "      <td>-1.086560e-06</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.709054</td>\n",
       "      <td>0.722552</td>\n",
       "      <td>0.736490</td>\n",
       "      <td>0.812237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18400</th>\n",
       "      <td>-0.999999</td>\n",
       "      <td>-1.106128e-06</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.708639</td>\n",
       "      <td>0.722662</td>\n",
       "      <td>0.737734</td>\n",
       "      <td>0.813186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18500</th>\n",
       "      <td>-0.999998</td>\n",
       "      <td>-1.117018e-06</td>\n",
       "      <td>0.039132</td>\n",
       "      <td>0.707167</td>\n",
       "      <td>0.720376</td>\n",
       "      <td>0.734185</td>\n",
       "      <td>0.811222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18600</th>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-9.104706e-07</td>\n",
       "      <td>0.061368</td>\n",
       "      <td>0.705835</td>\n",
       "      <td>0.718857</td>\n",
       "      <td>0.732102</td>\n",
       "      <td>0.810084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18700</th>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-9.448179e-07</td>\n",
       "      <td>0.002272</td>\n",
       "      <td>0.706719</td>\n",
       "      <td>0.720585</td>\n",
       "      <td>0.735663</td>\n",
       "      <td>0.811667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18800</th>\n",
       "      <td>-0.999993</td>\n",
       "      <td>-9.779890e-07</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>0.706814</td>\n",
       "      <td>0.720700</td>\n",
       "      <td>0.736319</td>\n",
       "      <td>0.811853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18900</th>\n",
       "      <td>-0.999996</td>\n",
       "      <td>-9.973886e-07</td>\n",
       "      <td>0.001831</td>\n",
       "      <td>0.707466</td>\n",
       "      <td>0.722193</td>\n",
       "      <td>0.737088</td>\n",
       "      <td>0.811926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19000</th>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-1.017968e-06</td>\n",
       "      <td>0.014048</td>\n",
       "      <td>0.707347</td>\n",
       "      <td>0.721890</td>\n",
       "      <td>0.735635</td>\n",
       "      <td>0.812254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19100</th>\n",
       "      <td>-0.999997</td>\n",
       "      <td>-1.043312e-06</td>\n",
       "      <td>0.019652</td>\n",
       "      <td>0.706276</td>\n",
       "      <td>0.720079</td>\n",
       "      <td>0.735001</td>\n",
       "      <td>0.811764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19200</th>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-1.003088e-06</td>\n",
       "      <td>0.001471</td>\n",
       "      <td>0.707178</td>\n",
       "      <td>0.721039</td>\n",
       "      <td>0.736555</td>\n",
       "      <td>0.812231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19300</th>\n",
       "      <td>-0.999999</td>\n",
       "      <td>-1.011526e-06</td>\n",
       "      <td>0.011897</td>\n",
       "      <td>0.708639</td>\n",
       "      <td>0.722390</td>\n",
       "      <td>0.738049</td>\n",
       "      <td>0.813378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19400</th>\n",
       "      <td>-0.999999</td>\n",
       "      <td>-8.357891e-07</td>\n",
       "      <td>0.000888</td>\n",
       "      <td>0.709016</td>\n",
       "      <td>0.721122</td>\n",
       "      <td>0.735943</td>\n",
       "      <td>0.812294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19500</th>\n",
       "      <td>-0.999999</td>\n",
       "      <td>-8.698999e-07</td>\n",
       "      <td>0.012454</td>\n",
       "      <td>0.708680</td>\n",
       "      <td>0.721423</td>\n",
       "      <td>0.735289</td>\n",
       "      <td>0.811919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19600</th>\n",
       "      <td>-0.999995</td>\n",
       "      <td>-8.910511e-07</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>0.708772</td>\n",
       "      <td>0.721769</td>\n",
       "      <td>0.735212</td>\n",
       "      <td>0.811910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19700</th>\n",
       "      <td>-0.999994</td>\n",
       "      <td>-9.164234e-07</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.709934</td>\n",
       "      <td>0.722456</td>\n",
       "      <td>0.734728</td>\n",
       "      <td>0.812593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19800</th>\n",
       "      <td>-0.979989</td>\n",
       "      <td>-9.318167e-07</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.710116</td>\n",
       "      <td>0.722454</td>\n",
       "      <td>0.737071</td>\n",
       "      <td>0.812742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19900</th>\n",
       "      <td>-0.999985</td>\n",
       "      <td>-9.544741e-07</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.709657</td>\n",
       "      <td>0.722516</td>\n",
       "      <td>0.737027</td>\n",
       "      <td>0.813124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss_D      loss_G_D  loss_G_MSE   ndcg@50  ndcg@100  ndcg@200  \\\n",
       "epoch                                                                     \n",
       "0     -0.001895           NaN         NaN  0.008099  0.012895  0.021415   \n",
       "100   -0.980454           NaN         NaN  0.008099  0.012895  0.021415   \n",
       "200   -0.999146           NaN         NaN  0.008099  0.012895  0.021415   \n",
       "300   -0.999444           NaN         NaN  0.008099  0.012895  0.021415   \n",
       "400   -0.999631           NaN         NaN  0.008099  0.012895  0.021415   \n",
       "500   -0.999481           NaN         NaN  0.008099  0.012895  0.021415   \n",
       "600   -0.999851 -1.481614e-04    0.057759  0.598208  0.624929  0.661603   \n",
       "700   -0.999827 -1.571228e-04    0.002023  0.618567  0.649635  0.685382   \n",
       "800   -0.999667 -3.276147e-04    0.001237  0.628725  0.660311  0.692246   \n",
       "900   -0.999892 -8.614675e-05    0.013078  0.637811  0.664810  0.695085   \n",
       "1000  -0.999899 -6.971505e-05    0.066503  0.642192  0.667077  0.696589   \n",
       "1100  -0.999926 -6.942534e-05    0.012958  0.648879  0.671361  0.702396   \n",
       "1200  -0.999832 -1.659752e-04    0.025448  0.652019  0.673274  0.704315   \n",
       "1300  -0.999841 -9.283122e-05    0.064390  0.655234  0.676000  0.705575   \n",
       "1400  -0.999943 -5.719247e-05    0.011613  0.659305  0.679989  0.707939   \n",
       "1500  -0.999946 -5.379543e-05    0.001786  0.661165  0.683345  0.710238   \n",
       "1600  -0.979969 -3.478895e-05    0.000083  0.662769  0.685343  0.711048   \n",
       "1700  -0.987525 -6.983025e-05    0.007694  0.663243  0.687253  0.712262   \n",
       "1800  -0.999962 -3.305814e-05    0.001716  0.664761  0.688913  0.714079   \n",
       "1900  -0.999968 -3.064398e-05    0.000069  0.667207  0.691146  0.716086   \n",
       "2000  -0.999963 -3.129975e-05    0.000064  0.668517  0.692860  0.716680   \n",
       "2100  -0.999938 -2.836906e-05    0.000752  0.666828  0.691841  0.715420   \n",
       "2200  -0.999975 -2.442285e-05    0.003854  0.668634  0.693615  0.716249   \n",
       "2300  -0.999975 -2.458235e-05    0.004347  0.669969  0.694169  0.716475   \n",
       "2400  -0.999963 -2.236971e-05    0.000533  0.671188  0.695388  0.717818   \n",
       "2500  -0.999954 -2.063600e-05    0.000059  0.672226  0.696296  0.717875   \n",
       "2600  -0.999973 -2.100737e-05    0.002595  0.674065  0.699523  0.720714   \n",
       "2700  -0.999978 -2.020010e-05    0.000077  0.676287  0.699426  0.720928   \n",
       "2800  -0.999979 -1.878868e-05    0.000052  0.677030  0.700699  0.720763   \n",
       "2900  -0.999981 -1.777276e-05    0.000052  0.679120  0.702603  0.722067   \n",
       "3000  -0.999983 -1.663363e-05    0.000050  0.681072  0.703597  0.723405   \n",
       "3100  -0.999984 -1.642199e-05    0.000048  0.683298  0.704857  0.722955   \n",
       "3200  -0.999981 -1.506192e-05    0.000047  0.683681  0.705884  0.723912   \n",
       "3300  -0.999986 -1.440353e-05    0.000046  0.684795  0.706261  0.724660   \n",
       "3400  -0.999981 -1.390213e-05    0.000046  0.685209  0.706953  0.725334   \n",
       "3500  -0.999984 -1.393705e-05    0.000045  0.686541  0.707434  0.726668   \n",
       "3600  -0.999983 -1.327090e-05    0.000044  0.687824  0.707902  0.726929   \n",
       "3700  -0.999982 -1.775959e-05    0.000044  0.687920  0.707978  0.727257   \n",
       "3800  -0.999984 -1.590376e-05    0.000043  0.688226  0.708905  0.727922   \n",
       "3900  -0.999979 -1.419336e-05    0.005134  0.690259  0.710300  0.727647   \n",
       "4000  -0.999988 -1.206503e-05    0.000045  0.691521  0.711594  0.730014   \n",
       "4100  -0.999987 -9.696865e-06    0.004859  0.689147  0.709410  0.727404   \n",
       "4200  -0.999990 -1.013399e-05    0.000718  0.693125  0.712091  0.729587   \n",
       "4300  -0.999990 -1.025327e-05    0.000711  0.693542  0.713452  0.731111   \n",
       "4400  -0.979988 -1.004536e-05    0.006413  0.692820  0.712718  0.729258   \n",
       "4500  -0.999973 -9.291866e-06    0.000193  0.692709  0.712839  0.729334   \n",
       "4600  -0.999991 -9.062495e-06    0.000052  0.693701  0.713609  0.729081   \n",
       "4700  -0.999986 -9.006162e-06    0.000081  0.693027  0.714008  0.730181   \n",
       "4800  -0.999987 -8.679513e-06    0.000062  0.697507  0.718177  0.732386   \n",
       "4900  -0.999992 -8.436908e-06    0.000571  0.695901  0.716097  0.730914   \n",
       "5000  -0.999989 -8.193574e-06    0.000131  0.696150  0.715554  0.730629   \n",
       "5100  -0.999931 -8.146164e-06    0.000391  0.697915  0.717609  0.731821   \n",
       "5200  -0.999992 -7.751985e-06    0.000039  0.698124  0.717680  0.733400   \n",
       "5300  -0.999992 -7.546771e-06    0.000038  0.699087  0.718002  0.732345   \n",
       "5400  -0.999991 -7.583595e-06    0.000037  0.699050  0.717560  0.732288   \n",
       "5500  -0.999986 -7.348020e-06    0.000036  0.698962  0.717541  0.732603   \n",
       "5600  -0.979993 -7.219431e-06    0.000036  0.698495  0.717419  0.732127   \n",
       "5700  -0.999993 -7.099346e-06    0.000035  0.698432  0.717563  0.732403   \n",
       "5800  -0.999993 -6.993651e-06    0.000036  0.698641  0.717769  0.732334   \n",
       "5900  -0.999989 -6.790196e-06    0.000039  0.698940  0.717593  0.733056   \n",
       "6000  -0.999993 -6.665833e-06    0.000341  0.699254  0.717891  0.732904   \n",
       "6100  -0.999993 -5.892565e-06    0.000082  0.699422  0.717875  0.733460   \n",
       "6200  -0.999988 -5.788800e-06    0.000222  0.699438  0.718114  0.733585   \n",
       "6300  -0.999976 -5.782969e-06    0.000051  0.700634  0.718157  0.733908   \n",
       "6400  -0.999994 -5.704556e-06    0.000135  0.701055  0.717915  0.734239   \n",
       "6500  -0.999992 -4.077210e-06    0.002250  0.700619  0.717282  0.733611   \n",
       "6600  -0.999969 -4.255765e-06    0.016797  0.701421  0.718941  0.732778   \n",
       "6700  -0.999993 -5.028838e-06    0.163303  0.697829  0.714767  0.727196   \n",
       "6800  -0.999995 -4.137961e-06    0.025594  0.699843  0.715822  0.730331   \n",
       "6900  -0.999911 -4.179785e-06    0.000390  0.700660  0.718077  0.733303   \n",
       "7000  -0.999996 -4.343177e-06    0.000033  0.701019  0.718265  0.733110   \n",
       "7100  -0.999996 -4.359742e-06    0.000033  0.700897  0.718305  0.733365   \n",
       "7200  -0.999995 -4.350397e-06    0.000033  0.701409  0.718631  0.734648   \n",
       "7300  -0.999996 -4.409692e-06    0.000032  0.701492  0.718710  0.732758   \n",
       "7400  -0.999995 -4.363766e-06    0.000032  0.701592  0.718532  0.734411   \n",
       "7500  -0.999996 -4.362536e-06    0.000050  0.701802  0.719079  0.733914   \n",
       "7600  -0.999996 -4.334925e-06    0.000600  0.702854  0.719897  0.735949   \n",
       "7700  -0.999995 -4.296562e-06    0.001050  0.702274  0.720163  0.735291   \n",
       "7800  -0.999994 -4.239721e-06    0.001101  0.702944  0.720808  0.735706   \n",
       "7900  -0.999996 -3.104945e-06    0.000371  0.701371  0.719686  0.735436   \n",
       "8000  -0.999997 -3.195423e-06    0.000541  0.704266  0.720279  0.736711   \n",
       "8100  -0.999986 -3.275651e-06    0.000067  0.704242  0.720378  0.737358   \n",
       "8200  -0.999995 -3.348633e-06    0.000031  0.703884  0.720690  0.736494   \n",
       "8300  -0.999996 -3.346440e-06    0.000031  0.703603  0.720962  0.735270   \n",
       "8400  -0.999996 -3.419109e-06    0.000031  0.704057  0.720666  0.735047   \n",
       "8500  -0.999992 -3.612733e-06    0.030314  0.703526  0.721305  0.735695   \n",
       "8600  -0.999996 -3.402795e-06    0.000034  0.705365  0.722106  0.737104   \n",
       "8700  -0.999990 -3.405827e-06    0.000030  0.705275  0.721846  0.736562   \n",
       "8800  -0.999996 -3.454762e-06    0.000030  0.704803  0.721165  0.735939   \n",
       "8900  -0.999991 -3.414307e-06    0.000030  0.704669  0.720848  0.736144   \n",
       "9000  -0.999994 -3.401304e-06    0.000031  0.704176  0.720231  0.735530   \n",
       "9100  -0.999997 -3.400564e-06    0.000908  0.704521  0.720507  0.736276   \n",
       "9200  -0.999997 -3.288954e-06    0.001610  0.705091  0.719938  0.734944   \n",
       "9300  -0.999996 -3.274032e-06    0.000189  0.704844  0.719600  0.735862   \n",
       "9400  -0.999997 -3.267503e-06    0.003189  0.704579  0.719716  0.734549   \n",
       "9500  -0.999992 -3.244874e-06    0.000199  0.704632  0.720040  0.734715   \n",
       "9600  -0.999997 -3.225387e-06    0.007133  0.704850  0.721304  0.735851   \n",
       "9700  -0.999997 -3.184694e-06    0.000621  0.705225  0.721072  0.735725   \n",
       "9800  -0.979997 -3.191098e-06    0.000216  0.706138  0.721304  0.736473   \n",
       "9900  -0.999981 -1.932523e-05    0.180582  0.704856  0.720418  0.736274   \n",
       "10000 -0.999879 -2.087147e-06    0.000205  0.705818  0.721734  0.736547   \n",
       "10100 -0.999993 -2.172745e-06    0.000037  0.704207  0.719536  0.734362   \n",
       "10200 -0.999998 -2.236756e-06    0.008838  0.703437  0.719637  0.734462   \n",
       "10300 -0.999996 -2.287886e-06    0.000175  0.704742  0.720339  0.734826   \n",
       "10400 -0.999995 -2.331928e-06    0.000029  0.705090  0.720545  0.736334   \n",
       "10500 -0.979998 -2.389250e-06    0.000029  0.705450  0.720671  0.736048   \n",
       "10600 -0.999987 -2.409708e-06    0.000029  0.705150  0.720235  0.735530   \n",
       "10700 -0.999994 -2.433437e-06    0.000028  0.705168  0.720235  0.735321   \n",
       "10800 -0.999988 -2.434245e-06    0.000033  0.705237  0.720276  0.735780   \n",
       "10900 -0.999997 -2.438869e-06    0.000521  0.705273  0.719962  0.735621   \n",
       "11000 -0.999996 -2.473101e-06    0.008306  0.705502  0.720600  0.736771   \n",
       "11100 -0.999996 -2.470467e-06    0.003304  0.705435  0.720032  0.734227   \n",
       "11200 -0.999994 -2.469569e-06    0.000028  0.705130  0.720406  0.734220   \n",
       "11300 -0.999992 -2.443314e-06    0.000028  0.705187  0.720457  0.734880   \n",
       "11400 -0.999997 -2.447085e-06    0.000176  0.705277  0.720542  0.734753   \n",
       "11500 -0.999994 -2.612672e-06    0.054365  0.704500  0.718630  0.734237   \n",
       "11600 -0.999997 -2.456037e-06    0.000028  0.705513  0.719424  0.733553   \n",
       "11700 -0.999993 -2.446187e-06    0.000096  0.705752  0.719470  0.733824   \n",
       "11800 -0.999995 -2.432680e-06    0.000040  0.705762  0.719648  0.733622   \n",
       "11900 -0.999992 -2.379691e-06    0.000468  0.705531  0.719296  0.734490   \n",
       "12000 -0.999998 -2.368193e-06    0.000124  0.705791  0.719554  0.733545   \n",
       "12100 -0.999994 -2.389424e-06    0.005456  0.706302  0.720104  0.734532   \n",
       "12200 -0.999996 -1.907995e-06    0.000084  0.706405  0.720186  0.734633   \n",
       "12300 -0.999993 -1.931477e-06    0.004360  0.706400  0.720546  0.733884   \n",
       "12400 -0.999998 -1.945124e-06    0.001984  0.705781  0.718966  0.732553   \n",
       "12500 -0.999998 -1.955255e-06    0.002584  0.705930  0.719926  0.735149   \n",
       "12600 -0.999989 -1.727855e-06    0.003607  0.704352  0.718991  0.733738   \n",
       "12700 -0.979993 -1.747701e-06    0.022876  0.704944  0.718060  0.732058   \n",
       "12800 -0.999997 -1.773110e-06    0.008202  0.706232  0.721181  0.736342   \n",
       "12900 -0.999995 -1.787298e-06    0.011965  0.706300  0.720945  0.734707   \n",
       "13000 -0.999998 -1.815864e-06    0.015554  0.706347  0.721628  0.736033   \n",
       "13100 -0.999979 -1.825225e-06    0.002915  0.706624  0.721055  0.734328   \n",
       "13200 -0.999998 -1.843091e-06    0.000274  0.706415  0.720345  0.734947   \n",
       "13300 -0.999996 -1.859893e-06    0.000050  0.707233  0.721078  0.734797   \n",
       "13400 -0.999998 -1.867351e-06    0.000027  0.706385  0.721108  0.735290   \n",
       "13500 -0.999998 -1.869064e-06    0.000026  0.706527  0.721449  0.735315   \n",
       "13600 -0.999997 -1.875058e-06    0.000027  0.708046  0.722940  0.736431   \n",
       "13700 -0.999980 -1.895430e-06    0.002222  0.707805  0.722844  0.735771   \n",
       "13800 -0.999995 -1.898428e-06    0.000027  0.707810  0.722798  0.736292   \n",
       "13900 -0.999996 -1.899244e-06    0.000026  0.707751  0.722418  0.735352   \n",
       "14000 -0.999997 -1.898032e-06    0.000027  0.707806  0.722448  0.735350   \n",
       "14100 -0.999997 -1.931867e-06    0.030498  0.707766  0.722670  0.736120   \n",
       "14200 -0.999998 -1.893726e-06    0.005563  0.707978  0.722096  0.735014   \n",
       "14300 -0.999997 -1.887322e-06    0.000515  0.708019  0.721901  0.736308   \n",
       "14400 -0.979998 -1.905503e-06    0.024703  0.706655  0.722110  0.734877   \n",
       "14500 -0.999996 -1.878936e-06    0.000081  0.706164  0.721683  0.735652   \n",
       "14600 -0.999993 -1.658318e-06    0.001329  0.708816  0.723226  0.736902   \n",
       "14700 -0.999996 -1.646587e-06    0.000075  0.707453  0.722038  0.736368   \n",
       "14800 -0.999984 -1.644914e-06    0.000049  0.708094  0.722293  0.734724   \n",
       "14900 -0.999994 -1.649880e-06    0.000026  0.708085  0.722383  0.735622   \n",
       "15000 -0.999998 -1.656846e-06    0.001349  0.708823  0.723145  0.737165   \n",
       "15100 -0.999997 -1.661490e-06    0.003268  0.707910  0.722769  0.735856   \n",
       "15200 -0.999996 -1.661770e-06    0.001477  0.707681  0.723027  0.737880   \n",
       "15300 -0.999998 -1.624850e-06    0.000056  0.709045  0.723743  0.738152   \n",
       "15400 -0.999998 -1.623063e-06    0.000026  0.708746  0.723150  0.737953   \n",
       "15500 -0.999998 -1.620553e-06    0.000026  0.709111  0.723375  0.737429   \n",
       "15600 -0.999998 -1.635409e-06    0.000025  0.709233  0.723290  0.738028   \n",
       "15700 -0.999996 -1.620744e-06    0.000030  0.709268  0.723156  0.739042   \n",
       "15800 -0.999998 -1.623299e-06    0.017088  0.708856  0.722537  0.737312   \n",
       "15900 -0.999998 -1.633719e-06    0.000026  0.708943  0.722986  0.737615   \n",
       "16000 -0.999997 -1.613158e-06    0.000026  0.709083  0.722908  0.737532   \n",
       "16100 -0.999998 -1.610409e-06    0.000035  0.708974  0.722653  0.737595   \n",
       "16200 -0.999997 -1.539681e-06    0.046228  0.706528  0.720450  0.735640   \n",
       "16300 -0.999997 -1.551063e-06    0.001798  0.708176  0.722294  0.737972   \n",
       "16400 -0.999998 -1.534144e-06    0.003946  0.708795  0.721969  0.737099   \n",
       "16500 -0.999998 -1.528574e-06    0.001161  0.708401  0.722122  0.736898   \n",
       "16600 -0.999997 -1.531863e-06    0.000055  0.708819  0.723333  0.736663   \n",
       "16700 -0.999998 -1.536983e-06    0.000743  0.708558  0.721794  0.737944   \n",
       "16800 -0.999998 -1.553405e-06    0.015128  0.709097  0.721820  0.737917   \n",
       "16900 -0.999997 -1.526106e-06    0.000025  0.710035  0.722554  0.738138   \n",
       "17000 -0.999998 -1.527011e-06    0.000025  0.710033  0.722671  0.737755   \n",
       "17100 -0.999998 -1.523557e-06    0.000477  0.710200  0.722987  0.738043   \n",
       "17200 -0.999998 -1.518733e-06    0.000429  0.709145  0.722544  0.737115   \n",
       "17300 -0.999997 -1.514764e-06    0.000072  0.710060  0.723062  0.738142   \n",
       "17400 -0.999998 -1.523140e-06    0.001546  0.710669  0.724200  0.738215   \n",
       "17500 -0.999998 -1.508513e-06    0.000031  0.710258  0.723474  0.737145   \n",
       "17600 -0.999998 -1.506957e-06    0.000025  0.710319  0.723281  0.737092   \n",
       "17700 -0.999998 -1.502204e-06    0.000045  0.710569  0.723380  0.738913   \n",
       "17800 -0.999998 -1.503031e-06    0.000025  0.710083  0.723571  0.737582   \n",
       "17900 -0.999998 -1.495682e-06    0.000495  0.710050  0.723284  0.739139   \n",
       "18000 -0.999998 -1.494632e-06    0.000970  0.709815  0.723294  0.738642   \n",
       "18100 -0.999998 -1.490748e-06    0.001998  0.709398  0.722174  0.736248   \n",
       "18200 -0.999996 -1.064179e-06    0.002786  0.708778  0.722143  0.736779   \n",
       "18300 -0.999999 -1.086560e-06    0.001818  0.709054  0.722552  0.736490   \n",
       "18400 -0.999999 -1.106128e-06    0.000189  0.708639  0.722662  0.737734   \n",
       "18500 -0.999998 -1.117018e-06    0.039132  0.707167  0.720376  0.734185   \n",
       "18600 -0.999996 -9.104706e-07    0.061368  0.705835  0.718857  0.732102   \n",
       "18700 -0.999991 -9.448179e-07    0.002272  0.706719  0.720585  0.735663   \n",
       "18800 -0.999993 -9.779890e-07    0.001294  0.706814  0.720700  0.736319   \n",
       "18900 -0.999996 -9.973886e-07    0.001831  0.707466  0.722193  0.737088   \n",
       "19000 -0.999997 -1.017968e-06    0.014048  0.707347  0.721890  0.735635   \n",
       "19100 -0.999997 -1.043312e-06    0.019652  0.706276  0.720079  0.735001   \n",
       "19200 -0.999989 -1.003088e-06    0.001471  0.707178  0.721039  0.736555   \n",
       "19300 -0.999999 -1.011526e-06    0.011897  0.708639  0.722390  0.738049   \n",
       "19400 -0.999999 -8.357891e-07    0.000888  0.709016  0.721122  0.735943   \n",
       "19500 -0.999999 -8.698999e-07    0.012454  0.708680  0.721423  0.735289   \n",
       "19600 -0.999995 -8.910511e-07    0.000840  0.708772  0.721769  0.735212   \n",
       "19700 -0.999994 -9.164234e-07    0.000030  0.709934  0.722456  0.734728   \n",
       "19800 -0.979989 -9.318167e-07    0.000024  0.710116  0.722454  0.737071   \n",
       "19900 -0.999985 -9.544741e-07    0.000024  0.709657  0.722516  0.737027   \n",
       "\n",
       "       ndcg@all  \n",
       "epoch            \n",
       "0      0.247514  \n",
       "100    0.247514  \n",
       "200    0.247514  \n",
       "300    0.247514  \n",
       "400    0.247514  \n",
       "500    0.247514  \n",
       "600    0.745640  \n",
       "700    0.763016  \n",
       "800    0.767730  \n",
       "900    0.771315  \n",
       "1000   0.773507  \n",
       "1100   0.778186  \n",
       "1200   0.778939  \n",
       "1300   0.780254  \n",
       "1400   0.782533  \n",
       "1500   0.784268  \n",
       "1600   0.785293  \n",
       "1700   0.785819  \n",
       "1800   0.787815  \n",
       "1900   0.789366  \n",
       "2000   0.789434  \n",
       "2100   0.788382  \n",
       "2200   0.789123  \n",
       "2300   0.789410  \n",
       "2400   0.790356  \n",
       "2500   0.790530  \n",
       "2600   0.793529  \n",
       "2700   0.794276  \n",
       "2800   0.794283  \n",
       "2900   0.795585  \n",
       "3000   0.796611  \n",
       "3100   0.797554  \n",
       "3200   0.797979  \n",
       "3300   0.798970  \n",
       "3400   0.799086  \n",
       "3500   0.800007  \n",
       "3600   0.800534  \n",
       "3700   0.800825  \n",
       "3800   0.801124  \n",
       "3900   0.801679  \n",
       "4000   0.803892  \n",
       "4100   0.802259  \n",
       "4200   0.803551  \n",
       "4300   0.804632  \n",
       "4400   0.803478  \n",
       "4500   0.803756  \n",
       "4600   0.803643  \n",
       "4700   0.804149  \n",
       "4800   0.807180  \n",
       "4900   0.805092  \n",
       "5000   0.805226  \n",
       "5100   0.806324  \n",
       "5200   0.806987  \n",
       "5300   0.806622  \n",
       "5400   0.806596  \n",
       "5500   0.806602  \n",
       "5600   0.806717  \n",
       "5700   0.806965  \n",
       "5800   0.806699  \n",
       "5900   0.807314  \n",
       "6000   0.807176  \n",
       "6100   0.807327  \n",
       "6200   0.807086  \n",
       "6300   0.807354  \n",
       "6400   0.807294  \n",
       "6500   0.807158  \n",
       "6600   0.807804  \n",
       "6700   0.804484  \n",
       "6800   0.806006  \n",
       "6900   0.807967  \n",
       "7000   0.808040  \n",
       "7100   0.807924  \n",
       "7200   0.808108  \n",
       "7300   0.807606  \n",
       "7400   0.808674  \n",
       "7500   0.808346  \n",
       "7600   0.809448  \n",
       "7700   0.809653  \n",
       "7800   0.810123  \n",
       "7900   0.809691  \n",
       "8000   0.810391  \n",
       "8100   0.810603  \n",
       "8200   0.811188  \n",
       "8300   0.809779  \n",
       "8400   0.809946  \n",
       "8500   0.809569  \n",
       "8600   0.811425  \n",
       "8700   0.811039  \n",
       "8800   0.810799  \n",
       "8900   0.810313  \n",
       "9000   0.809840  \n",
       "9100   0.810035  \n",
       "9200   0.809942  \n",
       "9300   0.809759  \n",
       "9400   0.809275  \n",
       "9500   0.809202  \n",
       "9600   0.809677  \n",
       "9700   0.810889  \n",
       "9800   0.811057  \n",
       "9900   0.810704  \n",
       "10000  0.811249  \n",
       "10100  0.809539  \n",
       "10200  0.809422  \n",
       "10300  0.810216  \n",
       "10400  0.811485  \n",
       "10500  0.810660  \n",
       "10600  0.811066  \n",
       "10700  0.810187  \n",
       "10800  0.811242  \n",
       "10900  0.810939  \n",
       "11000  0.811163  \n",
       "11100  0.810481  \n",
       "11200  0.810058  \n",
       "11300  0.810390  \n",
       "11400  0.810572  \n",
       "11500  0.810213  \n",
       "11600  0.810135  \n",
       "11700  0.810273  \n",
       "11800  0.809983  \n",
       "11900  0.810103  \n",
       "12000  0.810142  \n",
       "12100  0.810287  \n",
       "12200  0.810151  \n",
       "12300  0.810559  \n",
       "12400  0.810109  \n",
       "12500  0.810470  \n",
       "12600  0.810401  \n",
       "12700  0.809847  \n",
       "12800  0.811215  \n",
       "12900  0.811000  \n",
       "13000  0.811153  \n",
       "13100  0.810817  \n",
       "13200  0.811133  \n",
       "13300  0.810770  \n",
       "13400  0.810932  \n",
       "13500  0.810660  \n",
       "13600  0.812652  \n",
       "13700  0.811940  \n",
       "13800  0.811898  \n",
       "13900  0.811629  \n",
       "14000  0.811786  \n",
       "14100  0.812067  \n",
       "14200  0.812044  \n",
       "14300  0.812273  \n",
       "14400  0.812583  \n",
       "14500  0.811652  \n",
       "14600  0.813731  \n",
       "14700  0.812785  \n",
       "14800  0.812397  \n",
       "14900  0.811917  \n",
       "15000  0.812706  \n",
       "15100  0.812249  \n",
       "15200  0.813041  \n",
       "15300  0.813436  \n",
       "15400  0.812965  \n",
       "15500  0.813304  \n",
       "15600  0.812957  \n",
       "15700  0.813601  \n",
       "15800  0.812534  \n",
       "15900  0.812746  \n",
       "16000  0.812329  \n",
       "16100  0.812978  \n",
       "16200  0.811548  \n",
       "16300  0.813147  \n",
       "16400  0.812506  \n",
       "16500  0.812789  \n",
       "16600  0.812776  \n",
       "16700  0.813626  \n",
       "16800  0.813196  \n",
       "16900  0.813431  \n",
       "17000  0.812778  \n",
       "17100  0.814074  \n",
       "17200  0.813470  \n",
       "17300  0.813436  \n",
       "17400  0.813835  \n",
       "17500  0.813216  \n",
       "17600  0.813602  \n",
       "17700  0.814078  \n",
       "17800  0.813231  \n",
       "17900  0.814031  \n",
       "18000  0.813757  \n",
       "18100  0.812143  \n",
       "18200  0.812435  \n",
       "18300  0.812237  \n",
       "18400  0.813186  \n",
       "18500  0.811222  \n",
       "18600  0.810084  \n",
       "18700  0.811667  \n",
       "18800  0.811853  \n",
       "18900  0.811926  \n",
       "19000  0.812254  \n",
       "19100  0.811764  \n",
       "19200  0.812231  \n",
       "19300  0.813378  \n",
       "19400  0.812294  \n",
       "19500  0.811919  \n",
       "19600  0.811910  \n",
       "19700  0.812593  \n",
       "19800  0.812742  \n",
       "19900  0.813124  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.groupby(by=['epoch']).mean().plot()\n",
    "results_df.groupby(by=['epoch']).mean()\n",
    "# results_df.groupby(by=['epoch']).mean().iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576448f5",
   "metadata": {},
   "source": [
    "## Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a637f480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select doc_id and k\n",
    "doc_id = 4\n",
    "topk = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d1bbafcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth\n",
      "\u001b[48;5;3mwarren\u001b[0m \u001b[48;5;3mbrook\u001b[0m \u001b[48;5;3msailor\u001b[0m \u001b[48;5;3mmel\u001b[0m \u001b[48;5;3mflesh\u001b[0m \u001b[48;5;3mslapstick\u001b[0m \u001b[48;5;3mlesli\u001b[0m \u001b[48;5;3mprice\u001b[0m \u001b[48;5;3mann\u001b[0m \u001b[48;5;3mrent\u001b[0m \u001b[48;5;3mroom\u001b[0m \u001b[48;5;3mspeak\u001b[0m \u001b[48;5;3mcut\u001b[0m inherit richard warp process interpret montag ebert roger 3rd spectacular grin pg g x mislead \u001b[48;5;3mhumour\u001b[0m couldnt mankind rapid complex spiritu notion 4th rid dean dimension krell list storm struck hors discern stone cum soap detect current \n",
      "\n",
      "prediction\n",
      "\u001b[48;5;3mwarren\u001b[0m \u001b[48;5;3mbrook\u001b[0m \u001b[48;5;3msailor\u001b[0m \u001b[48;5;3mmel\u001b[0m \u001b[48;5;3mslapstick\u001b[0m \u001b[48;5;3mflesh\u001b[0m \u001b[48;5;3mlesli\u001b[0m \u001b[48;5;3mann\u001b[0m \u001b[48;5;3mprice\u001b[0m \u001b[48;5;3mroom\u001b[0m \u001b[48;5;3mrent\u001b[0m \u001b[48;5;3mcut\u001b[0m \u001b[48;5;3mspeak\u001b[0m pay jim eat bedroom bacon character oil dana walk frank wood smile freed cabin bill cover scarlett lean spike blond disney sick lower fiction craig sue susan poem myself darker pound interview sam \u001b[48;5;3mhumour\u001b[0m swim boil jack "
     ]
    }
   ],
   "source": [
    "word_list = dataset.vocab.itos\n",
    "\n",
    "gt = [word_list[word_idx] for word_idx in np.argsort(tfidf_ans[doc_id])[::-1][:topk]]\n",
    "pred = [word_list[word_idx] for word_idx in np.argsort(model.emb.weight.data[doc_id].numpy())[::-1][:topk]]\n",
    "\n",
    "print('ground truth')\n",
    "for word in gt:\n",
    "    if word in pred:\n",
    "        print(stylize(word, colored.bg(\"yellow\")), end=' ')\n",
    "    else:\n",
    "        print(word, end=' ')\n",
    "\n",
    "print()\n",
    "print('\\nprediction')\n",
    "for word in pred:\n",
    "    if word in gt:\n",
    "        print(stylize(word, colored.bg(\"yellow\")), end=' ')\n",
    "    else:\n",
    "        print(word, end=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b33f67cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is not the typical mel brooks film .  it was much less slapstick than most of his movies and actually had a plot that was followable .  leslie ann warren made the movie ,  she is such a fantastic ,  under-rated actress .  there were some moments that could have been fleshed out a bit more ,  and some scenes that could probably have been cut to make the room to do so ,  but all in all ,  this is worth the price to rent and see it .  the acting was good overall ,  brooks himself did a good job without his characteristic speaking to directly to the audience .  again ,  warren was the best actor in the movie ,  but  \" fume \"  and  \" sailor \"  both played their parts well . '"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.documents[doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "22172491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG top50 0.999935237169117\n",
      "NDCG top100 0.999935237169117\n",
      "NDCG top200 0.999935237169117\n",
      "NDCG ALL 0.999935237169117\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "   \n",
    "scores = np.array(model.emb.weight.data)[doc_id].reshape(1, -1)\n",
    "true_relevance = train_loader.dataset.tfidf_ans[doc_id].reshape(1, -1)\n",
    "\n",
    "results['ndcg@50'] = (ndcg_score(true_relevance, scores, k=50))\n",
    "results['ndcg@100'] = (ndcg_score(true_relevance, scores, k=100))\n",
    "results['ndcg@200'] = (ndcg_score(true_relevance, scores, k=200))\n",
    "results['ndcg@all'] = (ndcg_score(true_relevance, scores, k=None))\n",
    "\n",
    "print('NDCG top50', results['ndcg@50'])\n",
    "print('NDCG top100', results['ndcg@100'])\n",
    "print('NDCG top200', results['ndcg@200'])\n",
    "print('NDCG ALL', results['ndcg@all'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc34eb9b",
   "metadata": {},
   "source": [
    "## train seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2365ef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_GAN_NDCG(w, tfidf_ans, doc_id, verbose=1):\n",
    "#     results = {}\n",
    "    \n",
    "#     scores = w.detach().numpy().reshape(1, -1)\n",
    "#     true_relevance = tfidf_ans[doc_id].reshape(1, -1)\n",
    "\n",
    "    \n",
    "#     results['ndcg@50'] = (ndcg_score(true_relevance, scores, k=50))\n",
    "#     results['ndcg@100'] = (ndcg_score(true_relevance, scores, k=100))\n",
    "#     results['ndcg@200'] = (ndcg_score(true_relevance, scores, k=200))\n",
    "#     results['ndcg@all'] = (ndcg_score(true_relevance, scores, k=None))\n",
    "    \n",
    "#     if verbose == 1:\n",
    "#         print('NDCG top50', results['ndcg@50'])\n",
    "#         print('NDCG top100', results['ndcg@100'])\n",
    "#         print('NDCG top200', results['ndcg@200'])\n",
    "#         print('NDCG ALL', results['ndcg@all'])\n",
    "    \n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9fcf420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = 0.001\n",
    "# n_epoch = 1000\n",
    "\n",
    "# n_critic = 1\n",
    "# clip_value = 1\n",
    "# bs = 1\n",
    "# loss_weight = [0, 1]\n",
    "\n",
    "# num_words = word_vectors.shape[0]\n",
    "# results_all = []\n",
    "\n",
    "# for doc_id in tqdm(range(len(document_vectors_tensor[:10]))):\n",
    "#     doc_emb = document_vectors_tensor[doc_id]\n",
    "#     test_word_weight_tensor = torch.FloatTensor(np.concatenate((tfidf_ans[:doc_id], tfidf_ans[doc_id+1:])))\n",
    "\n",
    "#     w = torch.zeros(num_words).requires_grad_(True)\n",
    "#     D = Discriminator(num_words=num_words, h_dim=64)\n",
    "#     D.train()\n",
    "\n",
    "#     # opt_G = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "#     # opt_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "#     opt_G = torch.optim.SGD([w], lr=lr)\n",
    "#     opt_D = torch.optim.SGD(D.parameters(), lr=lr)\n",
    "\n",
    "#     G_criterion = nn.MSELoss(reduction='mean')\n",
    "#     D_criterion = nn.BCELoss()\n",
    "\n",
    "#     results = []\n",
    "#     for epoch in range(n_epoch):\n",
    "\n",
    "#         D_loss = []\n",
    "#         G_loss_D = []\n",
    "#         G_loss_MSE = []\n",
    "\n",
    "#         perm = torch.randperm(test_word_weight_tensor.size(0))\n",
    "#         true_word_weights_sample = test_word_weight_tensor[perm[:bs]]\n",
    "#         \"\"\" Medium: Use WGAN Loss. \"\"\"\n",
    "#         # Label\n",
    "#         r_label = torch.ones((bs))\n",
    "#         f_label = torch.zeros((bs))\n",
    "\n",
    "#         # Model forwarding\n",
    "#         r_logit = D(true_word_weights_sample.detach())\n",
    "#         f_logit = D(w.detach())\n",
    "\n",
    "#         # Compute the loss for the discriminator.\n",
    "#         # r_loss = D_criterion(r_logit.squeeze(), r_label)\n",
    "#         # f_loss = D_criterion(f_logit.squeeze(), f_label)\n",
    "#         # loss_D = (r_loss + f_loss) / 2\n",
    "\n",
    "#         # WGAN Loss\n",
    "#         loss_D = -torch.mean(r_logit) + torch.mean(f_logit)\n",
    "\n",
    "#         # Model backwarding\n",
    "#         D.zero_grad()\n",
    "#         loss_D.backward()\n",
    "\n",
    "#         # Update the discriminator.\n",
    "#         opt_D.step()\n",
    "\n",
    "#         D_loss.append(loss_D.item())\n",
    "#         \"\"\" Medium: Clip weights of discriminator. \"\"\"\n",
    "#         for p in D.parameters():\n",
    "#             p.data.clamp_(-clip_value, clip_value)\n",
    "\n",
    "#         # ============================================\n",
    "#         #  Train G\n",
    "#         # ============================================\n",
    "#         if epoch % n_critic == 0:\n",
    "#             # Generate some fake images.\n",
    "#             f_logit = D(w)\n",
    "\n",
    "#             \"\"\" Medium: Use WGAN Loss\"\"\"\n",
    "#             # Compute the loss for the generator.\n",
    "#             # loss_G_Dis = D_criterion(f_logit.squeeze(), r_label)\n",
    "#             # WGAN Loss\n",
    "#             loss_G_Dis = -torch.mean(f_logit)\n",
    "\n",
    "#             # MSE loss\n",
    "#             pred_doc_embs = w @ word_vectors_tensor\n",
    "#             loss_G_MSE = G_criterion(pred_doc_embs, doc_emb)\n",
    "\n",
    "#             loss_G = loss_G_Dis * loss_weight[0] + loss_G_MSE * loss_weight[1]\n",
    "\n",
    "#             # Model backwarding\n",
    "#             if w.grad is not None:\n",
    "#                 w.grad.zero_()\n",
    "            \n",
    "#             loss_G.backward()\n",
    "            \n",
    "#             # Update the generator.\n",
    "#             opt_G.step()\n",
    "\n",
    "#             G_loss_D.append(loss_G_Dis.item())\n",
    "#             G_loss_MSE.append(loss_G_MSE.item())\n",
    "        \n",
    "#         if epoch % 20 == 0:\n",
    "#             res = {}\n",
    "#             res['epoch'] = epoch\n",
    "#             res['loss_D'] = np.mean(D_loss)\n",
    "#             res['loss_G_D'] = np.mean(G_loss_D)\n",
    "#             res['loss_G_MSE'] = np.mean(G_loss_MSE)\n",
    "            \n",
    "#             print('epoch', epoch)\n",
    "#             print('loss D', np.mean(D_loss))\n",
    "#             print('loss G D', np.mean(G_loss_D))\n",
    "#             print('loss G MSE', np.mean(G_loss_MSE))\n",
    "            \n",
    "#             res_ndcg = evaluate_GAN_NDCG(w, tfidf_ans, doc_id)\n",
    "#             res.update(res_ndcg)\n",
    "#             results.append(res)\n",
    "\n",
    "#     results_all.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40069345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = []\n",
    "# for r in results_all:\n",
    "#     r = pd.DataFrame(r)\n",
    "#     results_df.append(r)\n",
    "\n",
    "# results_df = pd.concat(results_df)\n",
    "# results_df.groupby(by=['epoch']).mean().plot()\n",
    "# results_df.groupby(by=['epoch']).mean()\n",
    "# # results_df.groupby(by=['epoch']).mean().iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10c1e06",
   "metadata": {},
   "source": [
    "## DNN\n",
    "1. input: doc vec / output: tfidf weight\n",
    "2. eval: ndcg\n",
    "3. model: dnn, rank loss or mse loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4eb3b544",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 doc_vectors,\n",
    "                 tfidf_ans,\n",
    "                 ):\n",
    "        self.doc_vectors = doc_vectors\n",
    "        self.tfidf_ans = tfidf_ans\n",
    "        assert len(doc_vectors) == len(tfidf_ans)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # doc vec, tfidf-ans, doc_id\n",
    "        doc_vec = torch.FloatTensor(self.doc_vectors[idx])\n",
    "        tfidf_ans = torch.FloatTensor(self.tfidf_ans[idx])\n",
    "        \n",
    "        return doc_vec, tfidf_ans\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doc_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ae0e2d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vectors = np.array(dataset.document_vectors)\n",
    "\n",
    "len(document_vectors)\n",
    "train_test_split_ratio = 0.8\n",
    "select_num = int(len(document_vectors) * train_test_split_ratio)\n",
    "\n",
    "train_dataset = DNNDataset(document_vectors[:select_num], tfidf_ans[:select_num])\n",
    "valid_dataset = DNNDataset(document_vectors[select_num:], tfidf_ans[select_num:])\n",
    "\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True, pin_memory=True)\n",
    "test_loader  = torch.utils.data.DataLoader(valid_dataset, batch_size=100, shuffle=True, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb19d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, doc_emb_shape, num_words, h_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(doc_emb_shape, h_dim) \n",
    "        self.fc2 = nn.Linear(h_dim, h_dim)\n",
    "        self.fc3 = nn.Linear(h_dim, num_words)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1fcdba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_DNN(test_loader, model):\n",
    "\n",
    "    results = {}\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = []\n",
    "    scores = []\n",
    "    true_relevance = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            doc_embs, target = data\n",
    "            predicted = model(doc_embs)  \n",
    "            loss = criterion(predicted, target)  # compare the predicted labels with ground-truth labels\n",
    "            total_loss.append(loss.item())\n",
    "            scores.append(predicted.detach().numpy())\n",
    "            true_relevance.append(target.detach().numpy())\n",
    "    \n",
    "    scores = np.concatenate(scores)\n",
    "    true_relevance = np.concatenate(true_relevance)\n",
    "    \n",
    "    results['ndcg@50'] = (ndcg_score(true_relevance, scores, k=50))\n",
    "    results['ndcg@100'] = (ndcg_score(true_relevance, scores, k=100))\n",
    "    results['ndcg@200'] = (ndcg_score(true_relevance, scores, k=200))\n",
    "    results['ndcg@all'] = (ndcg_score(true_relevance, scores, k=None))\n",
    "    results['mse'] = np.mean(total_loss)\n",
    "    \n",
    "    print('Valid loss:',results['mse'])\n",
    "    print('NDCG top50', results['ndcg@50'])\n",
    "    print('NDCG top100', results['ndcg@100'])\n",
    "    print('NDCG top200', results['ndcg@200'])\n",
    "    print('NDCG ALL', results['ndcg@all'])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a02321bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lr = 0.001\n",
    "# h_dim = 64\n",
    "# model = DNN(document_vectors.shape[1], tfidf_ans.shape[1], h_dim)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)  # create a Adam optimizer\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# epochs = 100\n",
    "# results = []\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     total_loss = []\n",
    "#     for data in train_loader:\n",
    "#         doc_embs, target = data\n",
    "#         # training process\n",
    "#         optimizer.zero_grad()    \n",
    "#         predicted = model(doc_embs)  \n",
    "#         loss = criterion(predicted, target)  # compare the predicted labels with ground-truth labels\n",
    "#         loss.backward()      # compute the gradient\n",
    "#         optimizer.step()     # optimize the network\n",
    "#         total_loss.append(loss.item())\n",
    "        \n",
    "#     print(f'epoch:{epoch}')\n",
    "#     print(f'Training loss:{np.mean(total_loss)}')\n",
    "    \n",
    "#     res = evaluate_DNN(test_loader, model)\n",
    "#     results.append(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bab12fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = pd.DataFrame(results)\n",
    "# results.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fb08db",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "936999b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select #answer largest pred\n",
    "def metric4(binary_x, answer, w_idx=None, topk=50, verbose=0):\n",
    "    select_num = topk\n",
    "    answer = list(set(answer))\n",
    "    \n",
    "    if w_idx is not None:\n",
    "        pred = w_idx[np.argsort(binary_x)[-select_num:]]\n",
    "    else:\n",
    "        pred = np.arange(len(binary_x))[np.argsort(binary_x)[-select_num:]]\n",
    "    \n",
    "    hit = np.intersect1d(pred, answer)\n",
    "    hit_num = len(hit)\n",
    "    recall = hit_num / len(answer)\n",
    "    precision = hit_num / len(pred)\n",
    "    if verbose == 1:\n",
    "        print('answer:', word_list[answer])\n",
    "        print('hit:', word_list[hit])\n",
    "    return {\"recall\": recall, \"precision\": precision}\n",
    "\n",
    "def metric_ndcg(binary_x, answer, topk=50, verbose=0):\n",
    "\n",
    "    TFIDF_ans = np.zeros(len(binary_x))\n",
    "    for word_idx in answer:\n",
    "        if word_idx == 0:\n",
    "            continue\n",
    "        word = dataset.vocab.itos[word_idx]\n",
    "        TFIDF_ans[word_idx] += dataset.vocab.IDF[word]\n",
    "    NDCG_score = ndcg_score(TFIDF_ans.reshape(1,-1), binary_x.reshape(1,-1), k=topk)\n",
    "    \n",
    "    if verbose == 1:\n",
    "        print('NDCG_score:', NDCG_score)\n",
    "    return NDCG_score\n",
    "\n",
    "def metric_ndcg2(binary_x, tfidf_ans, doc_id, topk=50, verbose=0):\n",
    "    NDCG_score = ndcg_score(tfidf_ans[doc_id].reshape(1,-1), binary_x.reshape(1,-1), k=topk)\n",
    "    \n",
    "    if verbose == 1:\n",
    "        print('NDCG_score:', NDCG_score)\n",
    "    return NDCG_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d664a812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "class PyTorchLinearRegression:\n",
    "    ''' Class that implemnets Multiple Linear Regression with PyTorch'''\n",
    "    def __init__(self, num_of_features, lr, constraintHigh, constraintLow, total, init_type=0, L1=0, L2=0):\n",
    "        if init_type == 0:\n",
    "            self.w = torch.zeros(num_of_features, requires_grad=True)\n",
    "        elif init_type == 1:\n",
    "            self.w = torch.ones(num_of_features, requires_grad=True)\n",
    "        elif init_type == 2:  \n",
    "            self.w = torch.rand(num_of_features, requires_grad=True)\n",
    "        elif init_type == 3:\n",
    "            self.w = -torch.ones(num_of_features, requires_grad=True)\n",
    "\n",
    "        self.learning_rate = lr\n",
    "        self.high = constraintHigh\n",
    "        self.low = constraintLow\n",
    "        self.total = total\n",
    "        self.rg2 = total / num_of_features\n",
    "        self.L1 = L1\n",
    "        self.L2 = L2\n",
    "        \n",
    "    def _model(self, X):\n",
    "        return X @ self.w.t()# + self.b\n",
    "    \n",
    "    def _mse(self, pred, real):\n",
    "        difference = pred - real\n",
    "        return torch.sum(difference * difference) / difference.numel()\n",
    "    \n",
    "    def _regularization_weightdist(self):\n",
    "        difference = self.w - 1\n",
    "        return -torch.sum(difference * difference) / difference.numel()\n",
    "    \n",
    "    def _regularization_weightsum(self):\n",
    "        difference = torch.sum(self.w) - self.total\n",
    "        return difference * difference / self.w.numel()\n",
    "    \n",
    "    def fit(self, X, y, epochs):\n",
    "        print(loss_weight)\n",
    "        \n",
    "        X = torch.from_numpy(X).float()\n",
    "        y = torch.from_numpy(y).float()\n",
    "\n",
    "        for i in range(epochs):\n",
    "            predictions = self._model(X)\n",
    "            loss1 = self._mse(predictions, y)\n",
    "            loss2 = self._regularization_weightdist()\n",
    "            loss3 = self._regularization_weightsum()\n",
    "            loss = loss1 * loss_weight[0] + loss2 * loss_weight[1] + loss3 * loss_weight[2]\n",
    "\n",
    "            if (i % (epochs//20)) == 0:\n",
    "                print(f'Epoch: {i} - Loss: {loss1}')\n",
    "            \n",
    "            if self.w.grad is not None:\n",
    "                self.w.grad.zero_()\n",
    "            \n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                self.w -= (self.w.grad) * self.learning_rate + torch.sign(self.w)*self.L1 + self.w*self.L2\n",
    "                self.w.data.clamp_(min=self.low, max=self.high)\n",
    "#                 self.w.grad.zero_()\n",
    "#             x = 100\n",
    "#             if i % x == x-1:\n",
    "# #                 self.w=torch.tensor(self.low + (self.high-self.low)*(self.w - torch.min(self.w))/(torch.max(self.w) - torch.min(self.w)), requires_grad=True)\n",
    "#                 self.w.data.clamp_(min=self.low, max=self.high)\n",
    "#                 pass\n",
    "                \n",
    "    def predict(self, X):\n",
    "        X = torch.from_numpy(X).float()\n",
    "        return self._model(X)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        X = torch.from_numpy(X).float()\n",
    "        y_pred = self._model(X).detach().numpy()\n",
    "        return r2_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e0f38702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4127, 100)\n",
      "(9993, 100)\n",
      "9993\n"
     ]
    }
   ],
   "source": [
    "word_embs = np.array(dataset.vocab.word_vectors)\n",
    "doc_embs = np.array(dataset.document_vectors)\n",
    "doc_answers = dataset.document_answers\n",
    "word_list = dataset.vocab.itos\n",
    "\n",
    "print(word_embs.shape)\n",
    "print(doc_embs.shape)\n",
    "print(len(doc_answers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0d58515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pr, re = [[],[],[]], [[],[],[]]\n",
    "# ndcgs = defaultdict(list)\n",
    "\n",
    "# lr = 0.003\n",
    "# epochs = 10000\n",
    "# constraintHigh=1\n",
    "# constraintLow=0\n",
    "# # constraintHigh=float('inf')\n",
    "# # constraintLow=-float('inf')\n",
    "# loss_weight = [1, 0, 1]\n",
    "# L1, L2 = 0, 0\n",
    "# rand_type = 0\n",
    "\n",
    "# total_mul = 1\n",
    "\n",
    "# for uid, uemb in enumerate(tqdm(doc_embs[:50])):\n",
    "#     x = word_embs.T\n",
    "#     y = uemb\n",
    "#     total = len(doc_answers[uid])\n",
    "#     total = 1\n",
    "    \n",
    "#     torch_model = PyTorchLinearRegression(x.shape[1], lr, constraintHigh, constraintLow, int(total*total_mul), rand_type, L1, L2)\n",
    "#     torch_model.fit(x, y, epochs)\n",
    "    \n",
    "#     m1 = metric4(torch_model.w.detach().numpy(), doc_answers[uid], w_idx=None, topk=50, verbose=0)\n",
    "#     m2 = metric4(torch_model.w.detach().numpy(), doc_answers[uid], w_idx=None, topk=100, verbose=0)\n",
    "#     m3 = metric4(torch_model.w.detach().numpy(), doc_answers[uid], w_idx=None, topk=200, verbose=0)\n",
    "#     ndcg1 = metric_ndcg2(torch_model.w.detach().numpy(), tfidf_ans, uid, topk=50, verbose=0)\n",
    "#     ndcg2 = metric_ndcg2(torch_model.w.detach().numpy(), tfidf_ans, uid, topk=100, verbose=0)\n",
    "#     ndcg3 = metric_ndcg2(torch_model.w.detach().numpy(), tfidf_ans, uid, topk=200, verbose=0)\n",
    "#     ndcg4 = metric_ndcg2(torch_model.w.detach().numpy(), tfidf_ans, uid, topk=None, verbose=0)\n",
    "#     pr[0].append(m1[\"precision\"])\n",
    "#     re[0].append(m1[\"recall\"])\n",
    "#     pr[1].append(m2[\"precision\"])\n",
    "#     re[1].append(m2[\"recall\"])\n",
    "#     pr[2].append(m3[\"precision\"])\n",
    "#     re[2].append(m3[\"recall\"])\n",
    "    \n",
    "#     ndcgs[\"50\"].append(ndcg1)\n",
    "#     ndcgs[\"100\"].append(ndcg2)\n",
    "#     ndcgs[\"200\"].append(ndcg3)\n",
    "#     ndcgs[\"-1\"].append(ndcg4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4191c3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Precision:{np.mean(pr[0]):.4f} Recall:{np.mean(re[0]):.4f}\")\n",
    "# print(f\"Precision:{np.mean(pr[1]):.4f} Recall:{np.mean(re[1]):.4f}\")\n",
    "# print(f\"Precision:{np.mean(pr[2]):.4f} Recall:{np.mean(re[2]):.4f}\")\n",
    "# print(f\"NDCG 50:{np.mean(ndcgs['50']):.4f}\")\n",
    "# print(f\"NDCG 100:{np.mean(ndcgs['100']):.4f}\")\n",
    "# print(f\"NDCG 200:{np.mean(ndcgs['200']):.4f}\")\n",
    "# print(f\"NDCG all:{np.mean(ndcgs['-1']):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3827045d",
   "metadata": {},
   "source": [
    "## Nearest Guessing, so bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0bd99ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embs_IDF = word_embs.copy()\n",
    "\n",
    "for word, IDF in dataset.vocab.IDF.items():\n",
    "    word_idx = dataset.vocab.stoi[word]\n",
    "    word_embs_IDF[word_idx] *= IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1db6d370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a84d4835269414ca43197bb2d166bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pr, re = [[],[],[]], [[],[],[]]\n",
    "ndcgs = defaultdict(list)\n",
    "\n",
    "for uid, uemb in enumerate(tqdm(doc_embs[:100])):\n",
    "    y = uemb\n",
    "    word_embs_IDF\n",
    "    word_weight = np.dot(word_embs_IDF, y)\n",
    "    \n",
    "    m1 = metric4(word_weight, doc_answers[uid], w_idx=None, topk=50, verbose=0)\n",
    "    m2 = metric4(word_weight, doc_answers[uid], w_idx=None, topk=100, verbose=0)\n",
    "    m3 = metric4(word_weight, doc_answers[uid], w_idx=None, topk=200, verbose=0)\n",
    "    ndcg1 = metric_ndcg(word_weight, doc_answers[uid], topk=50, verbose=0)\n",
    "    ndcg2 = metric_ndcg(word_weight, doc_answers[uid], topk=100, verbose=0)\n",
    "    ndcg3 = metric_ndcg(word_weight, doc_answers[uid], topk=200, verbose=0)\n",
    "    ndcg4 = metric_ndcg(word_weight, doc_answers[uid], topk=None, verbose=0)\n",
    "    pr[0].append(m1[\"precision\"])\n",
    "    re[0].append(m1[\"recall\"])\n",
    "    pr[1].append(m2[\"precision\"])\n",
    "    re[1].append(m2[\"recall\"])\n",
    "    pr[2].append(m3[\"precision\"])\n",
    "    re[2].append(m3[\"recall\"])\n",
    "    \n",
    "    ndcgs[\"50\"].append(ndcg1)\n",
    "    ndcgs[\"100\"].append(ndcg2)\n",
    "    ndcgs[\"200\"].append(ndcg3)\n",
    "    ndcgs[\"-1\"].append(ndcg4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "71d4d93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.0380 Recall:0.1146\n",
      "Precision:0.0303 Recall:0.1675\n",
      "Precision:0.0250 Recall:0.2413\n",
      "NDCG 50:0.1190\n",
      "NDCG 100:0.1403\n",
      "NDCG 200:0.1673\n",
      "NDCG all:0.3492\n"
     ]
    }
   ],
   "source": [
    "print(f\"Precision:{np.mean(pr[0]):.4f} Recall:{np.mean(re[0]):.4f}\")\n",
    "print(f\"Precision:{np.mean(pr[1]):.4f} Recall:{np.mean(re[1]):.4f}\")\n",
    "print(f\"Precision:{np.mean(pr[2]):.4f} Recall:{np.mean(re[2]):.4f}\")\n",
    "print(f\"NDCG 50:{np.mean(ndcgs['50']):.4f}\")\n",
    "print(f\"NDCG 100:{np.mean(ndcgs['100']):.4f}\")\n",
    "print(f\"NDCG 200:{np.mean(ndcgs['200']):.4f}\")\n",
    "print(f\"NDCG all:{np.mean(ndcgs['-1']):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "56a9e30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## test dcg\n",
    "# from sklearn.metrics import ndcg_score, dcg_score\n",
    "# k=2\n",
    "\n",
    "# true_relevance = np.asarray([[1, 2, 3, 4]])\n",
    "# scores = np.asarray([[1, 2, 3, 2.5]])\n",
    "# print('dcg',dcg_score(true_relevance, scores,k=k))\n",
    "# print('ndcg',ndcg_score(true_relevance, scores,k=k))\n",
    "\n",
    "\n",
    "# w = 1 / (np.log(np.arange(true_relevance.shape[1])[:k] + 2) / np.log(2))\n",
    "# dcg = true_relevance[0][np.argsort(scores)[0][::-1][:k]].dot(w)\n",
    "# print(dcg)\n",
    "\n",
    "# idcg = np.sort(true_relevance[0])[::-1][:k].dot(w)\n",
    "# print(dcg/idcg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
