{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "982acb8f",
   "metadata": {},
   "source": [
    "## Siamese network \n",
    "Steps:\n",
    "1. load word embeding and document embedding\n",
    "2. create pytorch dataset and dataloader\n",
    "3. Try Contrastive loss and triplet loss\n",
    "4. further improve negative sampling (e.g. hard negative or word2vec negative sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67d8234",
   "metadata": {},
   "source": [
    "### raw data\n",
    "* word embedding: glove\n",
    "* doc text: ./data/IMDB.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b007799a",
   "metadata": {},
   "source": [
    "### preprocess\n",
    "1. filter too frequent and less frequent words\n",
    "2. stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6a82ca",
   "metadata": {},
   "source": [
    "### model\n",
    "1. Siamese\n",
    "2. TopK\n",
    "3. DNN\n",
    "4. Lasso\n",
    "5. LassoGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b9967b",
   "metadata": {},
   "source": [
    "### evaluation\n",
    "1. F1\n",
    "2. NDCG\n",
    "    1. MAP\n",
    "    2. MRR\n",
    "    3. ERR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e9beff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np \n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from itertools import cycle\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.metrics import ndcg_score\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2341254",
   "metadata": {},
   "source": [
    "## Preprocess config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dc01df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "config[\"n_document\"] = 10000\n",
    "config[\"min_word_freq_threshold\"] = 20\n",
    "config[\"topk_word_freq_threshold\"] = 500\n",
    "config[\"document_vector_agg\"] = 'TF-IDF'\n",
    "config[\"select_topk_TFIDF\"] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05c1627d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d005689a8f430681c4575423c320b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:400000\n"
     ]
    }
   ],
   "source": [
    "# load word embedding\n",
    "embedding_file = \"../data/glove.6B.100d.txt\"\n",
    "\n",
    "word2embedding = dict()\n",
    "word_dim = int(re.findall(r\".(\\d+)d\",embedding_file)[0])\n",
    "\n",
    "with open(embedding_file,\"r\") as f:\n",
    "    for line in tqdm(f):\n",
    "        line = line.strip().split()\n",
    "        word = line[0]\n",
    "        embedding = list(map(float,line[1:]))\n",
    "        word2embedding[word] = embedding\n",
    "\n",
    "print(\"Number of words:%d\" % len(word2embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1233053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, word2embedding, min_word_freq_threshold=0, topk_word_freq_threshold=0):\n",
    "        # The low frequency words will be assigned as <UNK> token\n",
    "        self.itos = {0: \"<UNK>\"}\n",
    "        self.stoi = {\"<UNK>\": 0}\n",
    "        \n",
    "        self.word2embedding = word2embedding\n",
    "        self.min_word_freq_threshold = min_word_freq_threshold\n",
    "        self.topk_word_freq_threshold = topk_word_freq_threshold\n",
    "        \n",
    "        self.word_freq_in_corpus = defaultdict(int)\n",
    "        self.IDF = {}\n",
    "        self.ps = PorterStemmer()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "#     @staticmethod\n",
    "    def tokenizer_eng(self, text):\n",
    "        text = re.sub(r'[^A-Za-z0-9 ]+', '', text)\n",
    "        text = text.strip().split()\n",
    "        \n",
    "        return [self.ps.stem(w) for w in text]\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        \n",
    "        self.doc_freq = defaultdict(int) # # of document a word appear\n",
    "        self.document_num = len(sentence_list)\n",
    "        self.word_vectors = [[0]*word_dim] # unknown word emb\n",
    "        \n",
    "        for sentence in tqdm(sentence_list, desc=\"Preprocessing documents\"):\n",
    "            # for doc_freq\n",
    "            document_words = set()\n",
    "            \n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                # pass unknown word\n",
    "                if word not in self.word2embedding:\n",
    "                    continue\n",
    "                    \n",
    "                # calculate word freq\n",
    "                self.word_freq_in_corpus[word] += 1\n",
    "                document_words.add(word)\n",
    "                \n",
    "            for word in document_words:\n",
    "                self.doc_freq[word] += 1\n",
    "        \n",
    "        # calculate IDF\n",
    "        print('doc num', self.document_num)\n",
    "        for word, freq in self.doc_freq.items():\n",
    "            self.IDF[word] = math.log(self.document_num / (freq+1))\n",
    "        \n",
    "        # delete less freq words:\n",
    "        delete_words = []\n",
    "        for word, v in self.word_freq_in_corpus.items():\n",
    "            if v < self.min_word_freq_threshold:\n",
    "                delete_words.append(word)     \n",
    "        for word in delete_words:\n",
    "            del self.IDF[word]    \n",
    "            del self.word_freq_in_corpus[word]    \n",
    "        \n",
    "        # delete too freq words\n",
    "        print('eliminate freq words')\n",
    "        IDF = [(word, freq) for word, freq in self.IDF.items()]\n",
    "        IDF.sort(key=lambda x: x[1])\n",
    "\n",
    "        for i in range(self.topk_word_freq_threshold):\n",
    "            print(word)\n",
    "            word = IDF[i][0]\n",
    "            del self.IDF[word]\n",
    "            del self.word_freq_in_corpus[word]\n",
    "        \n",
    "        # construct word_vectors\n",
    "        idx = 1\n",
    "        for word in self.word_freq_in_corpus:\n",
    "            self.word_vectors.append(self.word2embedding[word])\n",
    "            self.stoi[word] = idx\n",
    "            self.itos[idx] = word\n",
    "            idx += 1\n",
    "            \n",
    "    def calculate_document_vector(self, sentence_list, agg, select_topk_TFIDF=None):\n",
    "        document_vectors = []\n",
    "        document_answers = []\n",
    "        \n",
    "        for sentence in tqdm(sentence_list, desc=\"calculate document vectors\"):\n",
    "            document_vector = np.zeros(len(self.word_vectors[0]))\n",
    "            select_words = []\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                # pass unknown word\n",
    "                if word not in self.stoi:\n",
    "                    continue\n",
    "                else:\n",
    "                    select_words.append(word)\n",
    "\n",
    "            # select topk TDIDF\n",
    "            if select_topk_TFIDF is not None:\n",
    "                doc_TFIDF = defaultdict(float)\n",
    "                for word in select_words:    \n",
    "                    doc_TFIDF[word] += self.IDF[word]\n",
    "\n",
    "                doc_TFIDF_l = [(word, TFIDF) for word, TFIDF in doc_TFIDF.items()]\n",
    "                doc_TFIDF_l.sort(key=lambda x:x[1], reverse=True)\n",
    "                \n",
    "                select_topk_words = set(list(map(lambda x:x[0], doc_TFIDF_l[:select_topk_TFIDF])))\n",
    "                select_words = [word for word in select_words if word in select_topk_words]\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            # aggregate to doc vectors\n",
    "            for word in select_words:\n",
    "                if agg == 'mean':\n",
    "                    document_vector += self.word2embedding[word]\n",
    "                elif agg == 'TF-IDF':\n",
    "                    document_vector += np.array(self.word2embedding[word]) * self.IDF[word]\n",
    "\n",
    "            if len(select_words) == 0:\n",
    "                print('error', sentence)\n",
    "                continue\n",
    "            else:\n",
    "                document_vector /= len(select_words)\n",
    "            \n",
    "            document_vectors.append(document_vector)\n",
    "            document_answers.append(select_words)\n",
    "        \n",
    "        # get answers\n",
    "        document_answers_idx = []    \n",
    "        for ans in document_answers:\n",
    "            ans_idx = []\n",
    "            for token in ans:\n",
    "                if token in self.stoi:\n",
    "                    ans_idx.append(self.stoi[token])                    \n",
    "            document_answers_idx.append(ans_idx)\n",
    "            \n",
    "        return document_vectors, document_answers_idx\n",
    "        \n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c5706a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBowDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 raw_data_file_path,\n",
    "                 word2embedding,\n",
    "                 skip_header = False,\n",
    "                 n_document = None, # read first n document\n",
    "                 min_word_freq_threshold = 20, # eliminate less freq words\n",
    "                 topk_word_freq_threshold = 5, # eliminate smallest k IDF words\n",
    "                 select_topk_TFIDF = None, # select topk tf-idf as ground-truth\n",
    "                 document_vector_agg = 'mean',\n",
    "                 ):\n",
    "\n",
    "        assert document_vector_agg in ['mean', 'TF-IDF']\n",
    "        \n",
    "        # raw documents\n",
    "        self.documents = []\n",
    "        # document vectors\n",
    "        self.document_vectors = []\n",
    "        \n",
    "        with open(raw_data_file_path,'r',encoding='utf-8') as f:\n",
    "            if skip_header:\n",
    "                f.readline()\n",
    "            for line in tqdm(f, desc=\"Loading documents\"):\n",
    "                # read firt n document\n",
    "                if n_document is not None and len(self.documents) >= n_document:\n",
    "                    break    \n",
    "                self.documents.append(line.strip(\"\\n\"))\n",
    "\n",
    "        # build vocabulary\n",
    "        self.vocab = Vocabulary(word2embedding, min_word_freq_threshold, topk_word_freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.documents)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "        # calculate document vectors\n",
    "        self.document_vectors, self.document_answers = self.vocab.calculate_document_vector(self.documents, \\\n",
    "                                                                                           document_vector_agg, select_topk_TFIDF)\n",
    "        # train-test split\n",
    "        # training\n",
    "        self.train_split_ratio = 0.8\n",
    "        self.train_length = int(len(self.document_answers) * self.train_split_ratio)\n",
    "        self.train_vectors = self.document_vectors[:self.train_length]\n",
    "        self.train_words = self.document_answers[:self.train_length]\n",
    "        self.document_ids = list(range(self.train_length))\n",
    "        self.generator = cycle(self.context_target_generator())\n",
    "        self.dataset_size = sum([len(s) for s in self.train_words])\n",
    "        \n",
    "        # testing\n",
    "        self.test_vectors = self.document_vectors[self.train_length:]\n",
    "        self.test_words = self.document_answers[self.train_length:]\n",
    "\n",
    "    def context_target_generator(self):\n",
    "        np.random.shuffle(self.document_ids) # inplace shuffle\n",
    "\n",
    "        # randomly select a document and create its training example\n",
    "        for document_id in self.document_ids: \n",
    "            word_list = set(self.train_words[document_id])\n",
    "            negative_sample_space = list(set(range(self.vocab_size)) - word_list)\n",
    "            negative_samples = np.random.choice(negative_sample_space,size=len(word_list),replace = False)\n",
    "            for word_id, negative_wordID in zip(word_list, negative_samples):\n",
    "                yield [document_id, word_id, negative_wordID]\n",
    "                \n",
    "    def __getitem__(self, idx):\n",
    "        doc_id, word_id, negative_wordID = next(self.generator)\n",
    "        doc_id = torch.FloatTensor(self.document_vectors[doc_id])\n",
    "        word_id = torch.FloatTensor(self.vocab.word_vectors[word_id])\n",
    "        negative_word = torch.FloatTensor(self.vocab.word_vectors[negative_wordID])\n",
    "\n",
    "        return doc_id, word_id, negative_word\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1a7186c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d4fd840fc3c42788df1d1e729637a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading documents: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c0f40cd72744ba8a0017dde82ec189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing documents:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc num 10000\n",
      "eliminate freq words\n",
      "boel\n",
      "the\n",
      "and\n",
      "a\n",
      "of\n",
      "to\n",
      "is\n",
      "it\n",
      "thi\n",
      "in\n",
      "that\n",
      "i\n",
      "for\n",
      "with\n",
      "but\n",
      "as\n",
      "on\n",
      "wa\n",
      "be\n",
      "film\n",
      "one\n",
      "not\n",
      "are\n",
      "have\n",
      "all\n",
      "an\n",
      "you\n",
      "at\n",
      "by\n",
      "from\n",
      "who\n",
      "like\n",
      "hi\n",
      "ha\n",
      "so\n",
      "he\n",
      "time\n",
      "about\n",
      "out\n",
      "there\n",
      "if\n",
      "veri\n",
      "see\n",
      "good\n",
      "what\n",
      "more\n",
      "they\n",
      "when\n",
      "just\n",
      "some\n",
      "or\n",
      "make\n",
      "watch\n",
      "great\n",
      "get\n",
      "well\n",
      "my\n",
      "other\n",
      "up\n",
      "can\n",
      "love\n",
      "also\n",
      "which\n",
      "would\n",
      "their\n",
      "will\n",
      "even\n",
      "most\n",
      "her\n",
      "me\n",
      "had\n",
      "much\n",
      "than\n",
      "first\n",
      "do\n",
      "way\n",
      "into\n",
      "play\n",
      "end\n",
      "were\n",
      "no\n",
      "best\n",
      "scene\n",
      "think\n",
      "been\n",
      "how\n",
      "go\n",
      "look\n",
      "show\n",
      "made\n",
      "she\n",
      "after\n",
      "we\n",
      "year\n",
      "mani\n",
      "work\n",
      "know\n",
      "too\n",
      "seen\n",
      "act\n",
      "him\n",
      "them\n",
      "come\n",
      "thing\n",
      "perform\n",
      "two\n",
      "life\n",
      "still\n",
      "never\n",
      "take\n",
      "dont\n",
      "could\n",
      "give\n",
      "say\n",
      "then\n",
      "actor\n",
      "ani\n",
      "doe\n",
      "your\n",
      "where\n",
      "seem\n",
      "find\n",
      "enjoy\n",
      "want\n",
      "ever\n",
      "while\n",
      "man\n",
      "did\n",
      "over\n",
      "cast\n",
      "feel\n",
      "here\n",
      "such\n",
      "back\n",
      "these\n",
      "part\n",
      "those\n",
      "lot\n",
      "live\n",
      "tri\n",
      "role\n",
      "plot\n",
      "wonder\n",
      "interest\n",
      "use\n",
      "though\n",
      "better\n",
      "through\n",
      "now\n",
      "real\n",
      "off\n",
      "new\n",
      "befor\n",
      "world\n",
      "should\n",
      "set\n",
      "both\n",
      "quit\n",
      "again\n",
      "alway\n",
      "day\n",
      "director\n",
      "star\n",
      "young\n",
      "actual\n",
      "few\n",
      "own\n",
      "old\n",
      "same\n",
      "doesnt\n",
      "music\n",
      "direct\n",
      "may\n",
      "excel\n",
      "right\n",
      "fact\n",
      "bit\n",
      "start\n",
      "im\n",
      "turn\n",
      "whi\n",
      "between\n",
      "us\n",
      "saw\n",
      "without\n",
      "thought\n",
      "person\n",
      "long\n",
      "bad\n",
      "point\n",
      "down\n",
      "fan\n",
      "big\n",
      "recommend\n",
      "differ\n",
      "didnt\n",
      "around\n",
      "final\n",
      "must\n",
      "got\n",
      "last\n",
      "effect\n",
      "entertain\n",
      "place\n",
      "done\n",
      "cant\n",
      "need\n",
      "ive\n",
      "tell\n",
      "happen\n",
      "origin\n",
      "almost\n",
      "friend\n",
      "sinc\n",
      "put\n",
      "begin\n",
      "fun\n",
      "help\n",
      "although\n",
      "girl\n",
      "move\n",
      "each\n",
      "action\n",
      "true\n",
      "yet\n",
      "job\n",
      "enough\n",
      "keep\n",
      "sure\n",
      "line\n",
      "cours\n",
      "moment\n",
      "expect\n",
      "screen\n",
      "perfect\n",
      "lead\n",
      "onc\n",
      "classic\n",
      "view\n",
      "away\n",
      "far\n",
      "anyon\n",
      "woman\n",
      "worth\n",
      "kind\n",
      "whole\n",
      "found\n",
      "guy\n",
      "dvd\n",
      "isnt\n",
      "rather\n",
      "name\n",
      "seri\n",
      "later\n",
      "noth\n",
      "am\n",
      "hard\n",
      "follow\n",
      "our\n",
      "laugh\n",
      "mean\n",
      "nice\n",
      "reason\n",
      "goe\n",
      "might\n",
      "appear\n",
      "hope\n",
      "let\n",
      "shot\n",
      "includ\n",
      "run\n",
      "portray\n",
      "least\n",
      "himself\n",
      "tv\n",
      "complet\n",
      "read\n",
      "kill\n",
      "call\n",
      "john\n",
      "american\n",
      "understand\n",
      "viewer\n",
      "human\n",
      "miss\n",
      "face\n",
      "night\n",
      "favorit\n",
      "dure\n",
      "mind\n",
      "script\n",
      "chang\n",
      "power\n",
      "special\n",
      "sens\n",
      "eye\n",
      "second\n",
      "kid\n",
      "bring\n",
      "along\n",
      "three\n",
      "wife\n",
      "10\n",
      "usual\n",
      "open\n",
      "hollywood\n",
      "touch\n",
      "said\n",
      "main\n",
      "book\n",
      "left\n",
      "until\n",
      "meet\n",
      "creat\n",
      "age\n",
      "father\n",
      "support\n",
      "home\n",
      "death\n",
      "idea\n",
      "short\n",
      "came\n",
      "hand\n",
      "often\n",
      "boy\n",
      "top\n",
      "today\n",
      "high\n",
      "small\n",
      "product\n",
      "care\n",
      "horror\n",
      "problem\n",
      "gener\n",
      "heart\n",
      "brilliant\n",
      "full\n",
      "fall\n",
      "men\n",
      "version\n",
      "style\n",
      "murder\n",
      "less\n",
      "fine\n",
      "natur\n",
      "hous\n",
      "sound\n",
      "next\n",
      "base\n",
      "rest\n",
      "impress\n",
      "rate\n",
      "overal\n",
      "absolut\n",
      "given\n",
      "els\n",
      "drama\n",
      "talk\n",
      "sever\n",
      "die\n",
      "close\n",
      "war\n",
      "throughout\n",
      "head\n",
      "present\n",
      "humor\n",
      "wasnt\n",
      "except\n",
      "talent\n",
      "word\n",
      "song\n",
      "case\n",
      "review\n",
      "black\n",
      "cinema\n",
      "fight\n",
      "relationship\n",
      "imagin\n",
      "light\n",
      "dark\n",
      "success\n",
      "write\n",
      "late\n",
      "actress\n",
      "against\n",
      "deal\n",
      "side\n",
      "comment\n",
      "itself\n",
      "anim\n",
      "lost\n",
      "past\n",
      "develop\n",
      "learn\n",
      "art\n",
      "strong\n",
      "hour\n",
      "total\n",
      "stand\n",
      "instead\n",
      "written\n",
      "sort\n",
      "2\n",
      "mother\n",
      "abl\n",
      "theme\n",
      "either\n",
      "wish\n",
      "brother\n",
      "score\n",
      "matter\n",
      "wrong\n",
      "ye\n",
      "mr\n",
      "hit\n",
      "women\n",
      "thank\n",
      "recent\n",
      "soon\n",
      "ask\n",
      "felt\n",
      "camera\n",
      "mention\n",
      "wont\n",
      "import\n",
      "twist\n",
      "under\n",
      "event\n",
      "went\n",
      "citi\n",
      "children\n",
      "happi\n",
      "number\n",
      "type\n",
      "money\n",
      "behind\n",
      "attempt\n",
      "ago\n",
      "wait\n",
      "youll\n",
      "son\n",
      "id\n",
      "white\n",
      "school\n",
      "charm\n",
      "writer\n",
      "video\n",
      "stop\n",
      "question\n",
      "add\n",
      "element\n",
      "disappoint\n",
      "pace\n",
      "dead\n",
      "hold\n",
      "robert\n",
      "jame\n",
      "lack\n",
      "critic\n",
      "told\n",
      "dream\n",
      "evil\n",
      "visual\n",
      "heard\n",
      "daughter\n",
      "superb\n",
      "child\n",
      "order\n",
      "credit\n",
      "somewhat\n",
      "return\n",
      "known\n",
      "comic\n",
      "stay\n",
      "provid\n",
      "major\n",
      "strang\n",
      "career\n",
      "similar\n",
      "basic\n",
      "gave\n",
      "michael\n",
      "took\n",
      "note\n",
      "form\n",
      "opinion\n",
      "rare\n",
      "allow\n",
      "offer\n",
      "remind\n",
      "anyway\n",
      "greatest\n",
      "whose\n",
      "oscar\n",
      "level\n",
      "tale\n",
      "ill\n",
      "alon\n",
      "husband\n",
      "guess\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "897efe32e44c469a906eb6bf6e1b5a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "calculate document vectors:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error i think it's one of the greatest movies which are ever made ,  and i've seen many .  .  .  the book is better ,  but it's still a very good movie ! \n",
      "error all this talk about this being a bad movie is nonsense .  as a matter of fact this is the best movie i've ever seen .  it's an excellent story and the actors in the movie are some of the best .  i would not give criticism to any of the actors .  that movie is the best and it will always stay that way . \n",
      "error smallville episode justice is the best episode of smallville  !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !  it's my favorite episode of smallville !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   ! \n",
      "error smallville episode justice is the best episode of smallville  !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !  it's my favorite episode of smallville !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   ! \n",
      "error this is actually one of my favorite films ,  i would recommend that everyone watches it .  there is some great acting in it and it shows that not all  \" good \"  films are american .  .  .  . \n",
      "error i thought it was an original story ,  very nicely told .  i think all you people are expecting too much .  i mean .  .  . it's just a made for television movie !  what are you expecting ?  some great wonderful dramtic piece ?  i thought it was a really great story for a made for television movie .  .  .  . and that's my opinion . \n",
      "error it's highly stylized ,  but this movie shows that real people appear on these shows and what seems like good fun and a chance to appear on television can have serious consequences .   yes ,  i's mostly comedy ,  but there are some sad moments . \n",
      "Finish building dataset!\n",
      "Number of documents:10000\n",
      "Number of words:4127\n"
     ]
    }
   ],
   "source": [
    "# load and build torch dataset\n",
    "data_file_path = '../data/IMDB.txt'\n",
    "\n",
    "print(\"Building dataset....\")\n",
    "dataset = CBowDataset(\n",
    "                    raw_data_file_path=data_file_path,\n",
    "                    word2embedding=word2embedding,\n",
    "                    skip_header=False,\n",
    "                    n_document = config[\"n_document\"],\n",
    "                    min_word_freq_threshold = config[\"min_word_freq_threshold\"],\n",
    "                    topk_word_freq_threshold = config[\"topk_word_freq_threshold\"],\n",
    "                    document_vector_agg = config[\"document_vector_agg\"],\n",
    "                    select_topk_TFIDF = config[\"select_topk_TFIDF\"]\n",
    "                    )\n",
    "print(\"Finish building dataset!\")\n",
    "print(f\"Number of documents:{len(dataset.documents)}\")\n",
    "print(f\"Number of words:{dataset.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba4758da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# check test doc vectors' correctness\n",
    "word_vectors = np.array(dataset.vocab.word_vectors)\n",
    "word_vectors.shape\n",
    "\n",
    "pred = np.zeros(100)\n",
    "cnt = 0\n",
    "for word_idx in dataset.test_words[0]:\n",
    "    pred += word_vectors[word_idx] * dataset.vocab.IDF[dataset.vocab.itos[word_idx]]\n",
    "    cnt += 1\n",
    "print(dataset.test_vectors[0] - pred/cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf6bf5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 doc_vectors,\n",
    "                 ans_words,\n",
    "                 ):\n",
    "        self.doc_vectors = doc_vectors\n",
    "        self.ans_words = ans_words\n",
    "        assert len(doc_vectors) == len(ans_words)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        doc_vec = torch.FloatTensor(self.doc_vectors[idx])\n",
    "        ans_w = torch.tensor(list(set(self.ans_words[idx])))\n",
    "        return doc_vec, ans_w\n",
    "\n",
    "    def collate_fn(self,batch):\n",
    "        # Batch: List of tuples [(batch1), (batch2)]\n",
    "        \n",
    "        doc_vec = torch.cat([item[0].unsqueeze(0) for item in batch], dim=0)\n",
    "        ans_w = [item[1] for item in batch]\n",
    "        ans_w = pad_sequence(ans_w, batch_first=True, padding_value=-1)\n",
    "        \n",
    "        return doc_vec, ans_w \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doc_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "457c5d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletNet(nn.Module):\n",
    "    def __init__(self, hdim):\n",
    "        super(TripletNet, self).__init__()\n",
    "        self.fc = nn.Sequential(nn.Linear(hdim, 256),\n",
    "                        nn.PReLU(),\n",
    "                        nn.Linear(256, 256),\n",
    "                        nn.PReLU(),\n",
    "                        nn.Linear(256, 2)\n",
    "                        )\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        output1 = self.fc(x1)\n",
    "        output2 = self.fc(x2)\n",
    "        output3 = self.fc(x3)\n",
    "        return output1, output2, output3\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59ec9a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Triplet loss\n",
    "    Takes embeddings of an anchor sample, a positive sample and a negative sample\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative, size_average=True):\n",
    "        distance_positive = (anchor - positive).pow(2).sum(1)  # .pow(.5)\n",
    "        distance_negative = (anchor - negative).pow(2).sum(1)  # .pow(.5)\n",
    "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean() if size_average else losses.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b60ca7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "margin = 1.\n",
    "BATCH_SIZE = 1024\n",
    "EPOCH = 20\n",
    "\n",
    "device = \"cuda:0\"\n",
    "model = TripletNet(word_dim).to(device)\n",
    "loss_fn = TripletLoss(margin).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22a934f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "                        dataset, \n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        num_workers=4,\n",
    "                        shuffle=True,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2aa0290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docvec = dataset.test_vectors\n",
    "test_ans = dataset.test_words\n",
    "test_dataset = TestDataset(test_docvec,test_ans)\n",
    "test_loader = DataLoader(test_dataset,                         \n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         num_workers=4,\n",
    "                         collate_fn=test_dataset.collate_fn)\n",
    "word_embedding_tensor = torch.FloatTensor(dataset.vocab.word_vectors).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05bb89ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_word_emb, loader,Ks = [50,100,150,200]):\n",
    "    avg_precision, avg_recall = [], []\n",
    "    for batch in test_loader:\n",
    "        batch = [item.to(device) for item in batch]\n",
    "        emb, ans = batch\n",
    "        emb = model.get_embedding(emb)\n",
    "        scores = torch.cdist(emb, test_word_emb)\n",
    "        ans_length = torch.sum((~ans.eq(-1)).float(), dim=-1)\n",
    "        mask = ~ans.eq(-1).unsqueeze(-1)\n",
    "        \n",
    "        # calculate precision and recall\n",
    "        tmp_pr, tmp_re = [],[]\n",
    "        for K in Ks:\n",
    "            top_indices = torch.argsort(scores,dim=1)[:,:K]\n",
    "            hit = top_indices.unsqueeze(-2) == ans.unsqueeze(-1)\n",
    "            hit = torch.sum((hit * mask).flatten(1),dim=-1)\n",
    "            precision = hit / K\n",
    "            recall = hit / ans_length\n",
    "            tmp_pr.append(precision)\n",
    "            tmp_re.append(recall)\n",
    "        tmp_pr = torch.stack(tmp_pr).T.detach().cpu().numpy().tolist()\n",
    "        tmp_re = torch.stack(tmp_re).T.detach().cpu().numpy().tolist()\n",
    "        avg_precision.extend(tmp_pr)\n",
    "        avg_recall.extend(tmp_re)\n",
    "        \n",
    "    avg_precision = np.mean(avg_precision,axis=0)\n",
    "    avg_recall = np.mean(avg_recall, axis=0)\n",
    "    for idx, kval in enumerate(Ks):\n",
    "        print(f\"[K={kval}] Precision:{avg_precision[idx]:.4f} Recall:{avg_recall[idx]:.4f}\")\n",
    "    return avg_precision, avg_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c124d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(EPOCH):\n",
    "#     avg_loss = []\n",
    "#     model.train()\n",
    "#     for batch in tqdm(train_loader):\n",
    "#         batch = [item.to(device) for item in batch]\n",
    "#         doc_id,pos_w,neg_w = batch\n",
    "#         optimizer.zero_grad()\n",
    "#         loss = loss_fn(*model(doc_id,pos_w,neg_w))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         avg_loss.append(loss.item())\n",
    "#     avg_loss = np.mean(avg_loss)\n",
    "#     print(f\"Loss:{avg_loss:4f}\")\n",
    "    \n",
    "#     # evaluate\n",
    "#     model.eval()\n",
    "#     test_word_emb = model.get_embedding(word_embedding_tensor)\n",
    "#     res = evaluate(test_word_emb,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bc50afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "def evaluate_NDCG(test_word_emb, loader, topk=None):\n",
    "    NDCGs = defaultdict(list)\n",
    "    \n",
    "    for batch in (test_loader):\n",
    "        batch = [item.to(device) for item in batch]\n",
    "        emb, ans = batch\n",
    "        TFIDF_ans = np.zeros((len(ans), test_word_emb.shape[0]))\n",
    "        for i in range(len(ans)):\n",
    "            ans_row = ans[i]\n",
    "            ans_row = ans_row[~ans_row.eq(-1)]\n",
    "            ans_row = ans_row[~ans_row.eq(0)]\n",
    "            for word_id in ans_row:\n",
    "                word_id = word_id.item()\n",
    "                word = dataset.vocab.itos[word_id]\n",
    "                TFIDF_ans[i][word_id] += dataset.vocab.IDF[word]\n",
    "             \n",
    "        emb = model.get_embedding(emb)\n",
    "        scores = -torch.cdist(emb, test_word_emb).cpu().detach().numpy()\n",
    "        true_relevance = TFIDF_ans\n",
    "\n",
    "        NDCGs['top50'].append(ndcg_score(true_relevance, scores, k=50))\n",
    "        NDCGs['top100'].append(ndcg_score(true_relevance, scores, k=100))\n",
    "        NDCGs['top200'].append(ndcg_score(true_relevance, scores, k=200))\n",
    "        NDCGs['ALL'].append(ndcg_score(true_relevance, scores, k=None))\n",
    "    \n",
    "    print('NDCG top50', np.mean(NDCGs['top50']))\n",
    "    print('NDCG top100', np.mean(NDCGs['top100']))\n",
    "    print('NDCG top200', np.mean(NDCGs['top200']))\n",
    "    print('NDCG ALL', np.mean(NDCGs['ALL']))\n",
    "    return NDCGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8f96c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_history = []\n",
    "\n",
    "# for epoch in range(EPOCH):\n",
    "#     avg_loss = []\n",
    "#     model.train()\n",
    "#     for batch in tqdm(train_loader):\n",
    "#         batch = [item.to(device) for item in batch]\n",
    "#         doc_id,pos_w,neg_w = batch\n",
    "#         optimizer.zero_grad()\n",
    "#         loss = loss_fn(*model(doc_id,pos_w,neg_w))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         avg_loss.append(loss.item())\n",
    "#     avg_loss = np.mean(avg_loss)\n",
    "#     print(f\"Loss:{avg_loss:4f}\")\n",
    "    \n",
    "#     # evaluate\n",
    "#     model.eval()\n",
    "#     test_word_emb = model.get_embedding(word_embedding_tensor)\n",
    "#     ndcg_res = evaluate_NDCG(test_word_emb,test_loader)\n",
    "#     validation_history.append(ndcg_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d31a21",
   "metadata": {},
   "source": [
    "## Top K freq word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1efec49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('game', 783),\n",
       " ('season', 671),\n",
       " ('david', 549),\n",
       " ('ladi', 538),\n",
       " ('jack', 536),\n",
       " ('killer', 527),\n",
       " ('sex', 524),\n",
       " ('town', 524),\n",
       " ('king', 512),\n",
       " ('novel', 496)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = [(word, freq) for word, freq in dataset.vocab.word_freq_in_corpus.items()]\n",
    "word_freq.sort(key=lambda x:x[1], reverse=True)\n",
    "word_freq[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a04793e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc2b8fce49b4560943b4bd52a7f9742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 50 word\n",
      "percision 0.03472736368184093\n",
      "recall 0.06114075689135594\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d76fcf7e7141bbb67af82ea0e30a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 100 word\n",
      "percision 0.03465732866433217\n",
      "recall 0.12218305736433438\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6f407187f545069a6a191deb27f3fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 200 word\n",
      "percision 0.0316983491745873\n",
      "recall 0.22272235160505402\n"
     ]
    }
   ],
   "source": [
    "def topk_word_evaluation(k=50):\n",
    "    topk_word = [word for (word, freq) in word_freq[:k]]\n",
    "\n",
    "    pr, re = [], []\n",
    "    for ans in tqdm(test_ans):\n",
    "        ans = set(ans)\n",
    "        ans = [dataset.vocab.itos[a] for a in ans]\n",
    "\n",
    "        hit = []\n",
    "        for word in ans:\n",
    "            if word in topk_word:\n",
    "                hit.append(word)\n",
    "\n",
    "        precision = len(hit) / k\n",
    "        recall = len(hit) / len(ans)\n",
    "        pr.append(precision)\n",
    "        re.append(recall)\n",
    "\n",
    "    print('top {} word'.format(k))\n",
    "    print('percision', np.mean(pr))\n",
    "    print('recall', np.mean(re))\n",
    "\n",
    "topk_word_evaluation(k=50)\n",
    "topk_word_evaluation(k=100)\n",
    "topk_word_evaluation(k=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26308789",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066e0b29a3a143b2b74dc5b982bff2d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 50 NDCG:0.034637394512390236\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ae96d041fc4438f832851eed4293faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 100 NDCG:0.05239686405893681\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489482287e30443c92fb093994aae58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 200 NDCG:0.07946306791732662\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d390f0fd35e459584c5fb60fc46ce8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top None NDCG:0.28615093379302187\n"
     ]
    }
   ],
   "source": [
    "def topk_word_evaluation_NDCG(k=50):\n",
    "    freq_word =[word for (word, freq) in word_freq]\n",
    "    freq_word_idx = [dataset.vocab.stoi[word] for word in freq_word if word in dataset.vocab.stoi]\n",
    "    \n",
    "    scores = np.zeros(len(dataset.vocab.word_vectors))\n",
    "    for rank, idx in enumerate(freq_word_idx):\n",
    "        scores[idx] = len(dataset.vocab.word_vectors) - rank\n",
    "    \n",
    "    NDCGs = []\n",
    "    \n",
    "    for ans in tqdm(test_ans):\n",
    "        TFIDF_ans = np.zeros(len(dataset.vocab.word_vectors))\n",
    "        \n",
    "        for word_idx in ans:\n",
    "            if word_idx == 0:\n",
    "                continue\n",
    "            word = dataset.vocab.itos[word_idx]\n",
    "            TFIDF_ans[word_idx] += dataset.vocab.IDF[word]\n",
    "\n",
    "        NDCG_score = ndcg_score(TFIDF_ans.reshape(1,-1), scores.reshape(1,-1), k=k)\n",
    "        NDCGs.append(NDCG_score)\n",
    "\n",
    "    print('top {} NDCG:{}'.format(k, np.mean(NDCGs)))\n",
    "\n",
    "topk_word_evaluation_NDCG(k=50)\n",
    "topk_word_evaluation_NDCG(k=100)\n",
    "topk_word_evaluation_NDCG(k=200)\n",
    "topk_word_evaluation_NDCG(k=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec042e25",
   "metadata": {},
   "source": [
    "## DNN\n",
    "1. input: doc vec / output: tfidf weight\n",
    "2. eval: ndcg\n",
    "3. model: dnn, rank loss or mse loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67dd7935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9993, 4127)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a96cf942ea472cb02302e858276356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## create tfidf_ans\n",
    "document_answers = dataset.document_answers\n",
    "\n",
    "onehot_ans = np.zeros((len(document_answers), word_vectors.shape[0]))\n",
    "tfidf_ans = np.zeros((len(document_answers), word_vectors.shape[0]))\n",
    "print(tfidf_ans.shape)\n",
    "\n",
    "for i in tqdm(range(len(document_answers))):\n",
    "    for word_idx in document_answers[i]:\n",
    "        tfidf_ans[i, word_idx] += dataset.vocab.IDF[dataset.vocab.itos[word_idx]]\n",
    "        onehot_ans[i, word_idx] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46179c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 doc_vectors,\n",
    "                 tfidf_ans,\n",
    "                 ):\n",
    "        self.doc_vectors = doc_vectors\n",
    "        self.tfidf_ans = tfidf_ans\n",
    "        assert len(doc_vectors) == len(tfidf_ans)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # doc vec, tfidf-ans, doc_id\n",
    "        doc_vec = torch.FloatTensor(self.doc_vectors[idx])\n",
    "        tfidf_ans = torch.FloatTensor(self.tfidf_ans[idx])\n",
    "        \n",
    "        return doc_vec, tfidf_ans\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doc_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2586eee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vectors = np.array(dataset.document_vectors)\n",
    "\n",
    "len(document_vectors)\n",
    "train_test_split_ratio = 0.8\n",
    "select_num = int(len(document_vectors) * train_test_split_ratio)\n",
    "\n",
    "train_dataset = DNNDataset(document_vectors[:select_num], tfidf_ans[:select_num])\n",
    "valid_dataset = DNNDataset(document_vectors[select_num:], tfidf_ans[select_num:])\n",
    "\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True, pin_memory=True)\n",
    "test_loader  = torch.utils.data.DataLoader(valid_dataset, batch_size=100, shuffle=True, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c68e885",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, doc_emb_shape, num_words, h_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(doc_emb_shape, h_dim) \n",
    "        self.fc2 = nn.Linear(h_dim, h_dim)\n",
    "        self.fc3 = nn.Linear(h_dim, num_words)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "843bc5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_DNN(test_loader, model):\n",
    "\n",
    "    results = {}\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = []\n",
    "    scores = []\n",
    "    true_relevance = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            doc_embs, target = data\n",
    "            predicted = model(doc_embs)  \n",
    "            loss = criterion(predicted, target)  # compare the predicted labels with ground-truth labels\n",
    "            total_loss.append(loss.item())\n",
    "            scores.append(predicted.detach().numpy())\n",
    "            true_relevance.append(target.detach().numpy())\n",
    "    \n",
    "    scores = np.concatenate(scores)\n",
    "    true_relevance = np.concatenate(true_relevance)\n",
    "    \n",
    "    results['ndcg@50'] = (ndcg_score(true_relevance, scores, k=50))\n",
    "    results['ndcg@100'] = (ndcg_score(true_relevance, scores, k=100))\n",
    "    results['ndcg@200'] = (ndcg_score(true_relevance, scores, k=200))\n",
    "    results['ndcg@all'] = (ndcg_score(true_relevance, scores, k=None))\n",
    "    results['mse'] = np.mean(total_loss)\n",
    "    \n",
    "    print('Valid loss:',results['mse'])\n",
    "    print('NDCG top50', results['ndcg@50'])\n",
    "    print('NDCG top100', results['ndcg@100'])\n",
    "    print('NDCG top200', results['ndcg@200'])\n",
    "    print('NDCG ALL', results['ndcg@all'])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9532c784",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0\n",
      "Training loss:0.3832693794742227\n",
      "Valid loss: 0.3469732657074928\n",
      "NDCG top50 0.03353948781374114\n",
      "NDCG top100 0.04858768152474697\n",
      "NDCG top200 0.07426066344471556\n",
      "NDCG ALL 0.2799651432370169\n",
      "epoch:1\n",
      "Training loss:0.38029238376766444\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-709a68e0bafb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Training loss:{np.mean(total_loss)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_DNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-c3552c7be919>\u001b[0m in \u001b[0;36mevaluate_DNN\u001b[0;34m(test_loader, model)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ndcg@100'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mndcg_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_relevance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ndcg@200'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mndcg_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_relevance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ndcg@all'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mndcg_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_relevance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtual_env/py37/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtual_env/py37/lib/python3.7/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mndcg_score\u001b[0;34m(y_true, y_score, k, sample_weight, ignore_ties)\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m     \u001b[0m_check_dcg_target_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1434\u001b[0;31m     \u001b[0mgain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ndcg_sample_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_ties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_ties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1435\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtual_env/py37/lib/python3.7/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_ndcg_sample_scores\u001b[0;34m(y_true, y_score, k, ignore_ties)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m     \"\"\"\n\u001b[0;32m-> 1324\u001b[0;31m     \u001b[0mgain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dcg_sample_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_ties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_ties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1325\u001b[0m     \u001b[0;31m# Here we use the order induced by y_true so we can ignore ties since\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;31m# the gain associated to tied indices is the same (permuting ties doesn't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtual_env/py37/lib/python3.7/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_dcg_sample_scores\u001b[0;34m(y_true, y_score, k, log_base, ignore_ties)\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdiscount_cumsum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         cumulative_gains = [_tie_averaged_dcg(y_t, y_s, discount_cumsum)\n\u001b[0;32m-> 1113\u001b[0;31m                             for y_t, y_s in zip(y_true, y_score)]\n\u001b[0m\u001b[1;32m   1114\u001b[0m         \u001b[0mcumulative_gains\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcumulative_gains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcumulative_gains\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtual_env/py37/lib/python3.7/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdiscount_cumsum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         cumulative_gains = [_tie_averaged_dcg(y_t, y_s, discount_cumsum)\n\u001b[0;32m-> 1113\u001b[0;31m                             for y_t, y_s in zip(y_true, y_score)]\n\u001b[0m\u001b[1;32m   1114\u001b[0m         \u001b[0mcumulative_gains\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcumulative_gains\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcumulative_gains\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtual_env/py37/lib/python3.7/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_tie_averaged_dcg\u001b[0;34m(y_true, y_score, discount_cumsum)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         - y_score, return_inverse=True, return_counts=True)\n\u001b[1;32m   1157\u001b[0m     \u001b[0mranked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m     \u001b[0mranked\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "h_dim = 64\n",
    "model = DNN(document_vectors.shape[1], tfidf_ans.shape[1], h_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # create a Adam optimizer\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "epochs = 100\n",
    "results = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = []\n",
    "    for data in train_loader:\n",
    "        doc_embs, target = data\n",
    "        # training process\n",
    "        optimizer.zero_grad()    \n",
    "        predicted = model(doc_embs)  \n",
    "        loss = criterion(predicted, target)  # compare the predicted labels with ground-truth labels\n",
    "        loss.backward()      # compute the gradient\n",
    "        optimizer.step()     # optimize the network\n",
    "        total_loss.append(loss.item())\n",
    "        \n",
    "    print(f'epoch:{epoch}')\n",
    "    print(f'Training loss:{np.mean(total_loss)}')\n",
    "    \n",
    "    res = evaluate_DNN(test_loader, model)\n",
    "    results.append(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dedb2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results)\n",
    "results.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fffbfc2",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73ba9a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select #answer largest pred\n",
    "def metric4(binary_x, answer, w_idx=None, topk=50, verbose=0):\n",
    "    select_num = topk\n",
    "    answer = list(set(answer))\n",
    "    \n",
    "    if w_idx is not None:\n",
    "        pred = w_idx[np.argsort(binary_x)[-select_num:]]\n",
    "    else:\n",
    "        pred = np.arange(len(binary_x))[np.argsort(binary_x)[-select_num:]]\n",
    "    \n",
    "    hit = np.intersect1d(pred, answer)\n",
    "    hit_num = len(hit)\n",
    "    recall = hit_num / len(answer)\n",
    "    precision = hit_num / len(pred)\n",
    "    if verbose == 1:\n",
    "        print('answer:', word_list[answer])\n",
    "        print('hit:', word_list[hit])\n",
    "    return {\"recall\": recall, \"precision\": precision}\n",
    "\n",
    "def metric_ndcg(binary_x, answer, topk=50, verbose=0):\n",
    "\n",
    "    TFIDF_ans = np.zeros(len(binary_x))\n",
    "    for word_idx in answer:\n",
    "        if word_idx == 0:\n",
    "            continue\n",
    "        word = dataset.vocab.itos[word_idx]\n",
    "        TFIDF_ans[word_idx] += dataset.vocab.IDF[word]\n",
    "    NDCG_score = ndcg_score(TFIDF_ans.reshape(1,-1), binary_x.reshape(1,-1), k=topk)\n",
    "    \n",
    "    if verbose == 1:\n",
    "        print('NDCG_score:', NDCG_score)\n",
    "    return NDCG_score\n",
    "\n",
    "def metric_ndcg2(binary_x, tfidf_ans, doc_id, topk=50, verbose=0):\n",
    "    NDCG_score = ndcg_score(tfidf_ans[doc_id].reshape(1,-1), binary_x.reshape(1,-1), k=topk)\n",
    "    \n",
    "    if verbose == 1:\n",
    "        print('NDCG_score:', NDCG_score)\n",
    "    return NDCG_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2a91719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "class PyTorchLinearRegression:\n",
    "    ''' Class that implemnets Multiple Linear Regression with PyTorch'''\n",
    "    def __init__(self, num_of_features, lr, constraintHigh, constraintLow, total, init_type=0, L1=0, L2=0):\n",
    "        if init_type == 0:\n",
    "            self.w = torch.zeros(num_of_features, requires_grad=True)\n",
    "        elif init_type == 1:\n",
    "            self.w = torch.ones(num_of_features, requires_grad=True)\n",
    "        elif init_type == 2:  \n",
    "            self.w = torch.rand(num_of_features, requires_grad=True)\n",
    "        elif init_type == 3:\n",
    "            self.w = -torch.ones(num_of_features, requires_grad=True)\n",
    "\n",
    "        self.learning_rate = lr\n",
    "        self.high = constraintHigh\n",
    "        self.low = constraintLow\n",
    "        self.total = total\n",
    "        self.rg2 = total / num_of_features\n",
    "        self.L1 = L1\n",
    "        self.L2 = L2\n",
    "        \n",
    "    def _model(self, X):\n",
    "        return X @ self.w.t()# + self.b\n",
    "    \n",
    "    def _mse(self, pred, real):\n",
    "        difference = pred - real\n",
    "        return torch.sum(difference * difference) / difference.numel()\n",
    "    \n",
    "    def _regularization_weightdist(self):\n",
    "        difference = self.w - 1\n",
    "        return -torch.sum(difference * difference) / difference.numel()\n",
    "    \n",
    "    def _regularization_weightsum(self):\n",
    "        difference = torch.sum(self.w) - self.total\n",
    "        return difference * difference / self.w.numel()\n",
    "    \n",
    "    def fit(self, X, y, epochs):\n",
    "        print(loss_weight)\n",
    "        \n",
    "        X = torch.from_numpy(X).float()\n",
    "        y = torch.from_numpy(y).float()\n",
    "\n",
    "        for i in range(epochs):\n",
    "            predictions = self._model(X)\n",
    "            loss1 = self._mse(predictions, y)\n",
    "            loss2 = self._regularization_weightdist()\n",
    "            loss3 = self._regularization_weightsum()\n",
    "            loss = loss1 * loss_weight[0] + loss2 * loss_weight[1] + loss3 * loss_weight[2]\n",
    "\n",
    "            if (i % (epochs//20)) == 0:\n",
    "                print(f'Epoch: {i} - Loss: {loss1}')\n",
    "            \n",
    "            if self.w.grad is not None:\n",
    "                self.w.grad.zero_()\n",
    "            \n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                self.w -= (self.w.grad) * self.learning_rate + torch.sign(self.w)*self.L1 + self.w*self.L2\n",
    "                self.w.data.clamp_(min=self.low, max=self.high)\n",
    "#                 self.w.grad.zero_()\n",
    "#             x = 100\n",
    "#             if i % x == x-1:\n",
    "# #                 self.w=torch.tensor(self.low + (self.high-self.low)*(self.w - torch.min(self.w))/(torch.max(self.w) - torch.min(self.w)), requires_grad=True)\n",
    "#                 self.w.data.clamp_(min=self.low, max=self.high)\n",
    "#                 pass\n",
    "                \n",
    "    def predict(self, X):\n",
    "        X = torch.from_numpy(X).float()\n",
    "        return self._model(X)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        X = torch.from_numpy(X).float()\n",
    "        y_pred = self._model(X).detach().numpy()\n",
    "        return r2_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "90555ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4127, 100)\n",
      "(9993, 100)\n",
      "9993\n"
     ]
    }
   ],
   "source": [
    "word_embs = np.array(dataset.vocab.word_vectors)\n",
    "doc_embs = np.array(dataset.document_vectors)\n",
    "doc_answers = dataset.document_answers\n",
    "word_list = dataset.vocab.itos\n",
    "\n",
    "print(word_embs.shape)\n",
    "print(doc_embs.shape)\n",
    "print(len(doc_answers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "85a3bc33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22809ccbb03345248ca01edf890c0b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.1165862083435059\n",
      "Epoch: 500 - Loss: 0.0028192419558763504\n",
      "Epoch: 1000 - Loss: 0.001100227702409029\n",
      "Epoch: 1500 - Loss: 0.0007361043826676905\n",
      "Epoch: 2000 - Loss: 0.0005721341003663838\n",
      "Epoch: 2500 - Loss: 0.0004927846603095531\n",
      "Epoch: 3000 - Loss: 0.0004479722701944411\n",
      "Epoch: 3500 - Loss: 0.0004205699951853603\n",
      "Epoch: 4000 - Loss: 0.00039610968087799847\n",
      "Epoch: 4500 - Loss: 0.00037219771184027195\n",
      "Epoch: 5000 - Loss: 0.0003498220175970346\n",
      "Epoch: 5500 - Loss: 0.00032899342477321625\n",
      "Epoch: 6000 - Loss: 0.00031210615998134017\n",
      "Epoch: 6500 - Loss: 0.0002968952467199415\n",
      "Epoch: 7000 - Loss: 0.0002857899817172438\n",
      "Epoch: 7500 - Loss: 0.0002743444638326764\n",
      "Epoch: 8000 - Loss: 0.00026430710568092763\n",
      "Epoch: 8500 - Loss: 0.00025625675334595144\n",
      "Epoch: 9000 - Loss: 0.00024993260740302503\n",
      "Epoch: 9500 - Loss: 0.00024273889721371233\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.0777878761291504\n",
      "Epoch: 500 - Loss: 0.00021529271907638758\n",
      "Epoch: 1000 - Loss: 0.0001193823482026346\n",
      "Epoch: 1500 - Loss: 9.067250357475132e-05\n",
      "Epoch: 2000 - Loss: 7.672214269405231e-05\n",
      "Epoch: 2500 - Loss: 6.559042230946943e-05\n",
      "Epoch: 3000 - Loss: 6.0047899751225486e-05\n",
      "Epoch: 3500 - Loss: 5.479542596731335e-05\n",
      "Epoch: 4000 - Loss: 5.1674684073077515e-05\n",
      "Epoch: 4500 - Loss: 4.953174720867537e-05\n",
      "Epoch: 5000 - Loss: 4.664830703404732e-05\n",
      "Epoch: 5500 - Loss: 4.259636625647545e-05\n",
      "Epoch: 6000 - Loss: 3.981661211582832e-05\n",
      "Epoch: 6500 - Loss: 3.808601468335837e-05\n",
      "Epoch: 7000 - Loss: 3.662090966827236e-05\n",
      "Epoch: 7500 - Loss: 3.5051885788561776e-05\n",
      "Epoch: 8000 - Loss: 3.380950874998234e-05\n",
      "Epoch: 8500 - Loss: 3.2779924367787316e-05\n",
      "Epoch: 9000 - Loss: 3.202860170858912e-05\n",
      "Epoch: 9500 - Loss: 3.087215009145439e-05\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.9535503387451172\n",
      "Epoch: 500 - Loss: 0.0015575278084725142\n",
      "Epoch: 1000 - Loss: 0.0006731857429258525\n",
      "Epoch: 1500 - Loss: 0.0004786130739375949\n",
      "Epoch: 2000 - Loss: 0.0004038050537928939\n",
      "Epoch: 2500 - Loss: 0.00035828546970151365\n",
      "Epoch: 3000 - Loss: 0.00032672786619514227\n",
      "Epoch: 3500 - Loss: 0.0003014699323102832\n",
      "Epoch: 4000 - Loss: 0.00028687651501968503\n",
      "Epoch: 4500 - Loss: 0.0002732455322984606\n",
      "Epoch: 5000 - Loss: 0.00026383280055597425\n",
      "Epoch: 5500 - Loss: 0.0002557203406468034\n",
      "Epoch: 6000 - Loss: 0.00024491336080245674\n",
      "Epoch: 6500 - Loss: 0.00023817144392523915\n",
      "Epoch: 7000 - Loss: 0.0002312811993760988\n",
      "Epoch: 7500 - Loss: 0.00022388834622688591\n",
      "Epoch: 8000 - Loss: 0.0002185788907809183\n",
      "Epoch: 8500 - Loss: 0.00021276263578329235\n",
      "Epoch: 9000 - Loss: 0.0002095824311254546\n",
      "Epoch: 9500 - Loss: 0.00020755572768393904\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.8551814556121826\n",
      "Epoch: 500 - Loss: 0.0007609017193317413\n",
      "Epoch: 1000 - Loss: 0.000337411678628996\n",
      "Epoch: 1500 - Loss: 0.00024471714277751744\n",
      "Epoch: 2000 - Loss: 0.0002053941716440022\n",
      "Epoch: 2500 - Loss: 0.00018056340923067182\n",
      "Epoch: 3000 - Loss: 0.00016450283874291927\n",
      "Epoch: 3500 - Loss: 0.0001529440633021295\n",
      "Epoch: 4000 - Loss: 0.00013934819435235113\n",
      "Epoch: 4500 - Loss: 0.00013118538481649011\n",
      "Epoch: 5000 - Loss: 0.00012649912969209254\n",
      "Epoch: 5500 - Loss: 0.00012189925473649055\n",
      "Epoch: 6000 - Loss: 0.00011751464626286179\n",
      "Epoch: 6500 - Loss: 0.00011406283738324419\n",
      "Epoch: 7000 - Loss: 0.00011041165271308273\n",
      "Epoch: 7500 - Loss: 0.00010553604079177603\n",
      "Epoch: 8000 - Loss: 0.00010040572669822723\n",
      "Epoch: 8500 - Loss: 9.73217174760066e-05\n",
      "Epoch: 9000 - Loss: 9.38648809096776e-05\n",
      "Epoch: 9500 - Loss: 9.065808990271762e-05\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.1088433265686035\n",
      "Epoch: 500 - Loss: 0.002627461450174451\n",
      "Epoch: 1000 - Loss: 0.0010318655986338854\n",
      "Epoch: 1500 - Loss: 0.0006956018041819334\n",
      "Epoch: 2000 - Loss: 0.0005570822395384312\n",
      "Epoch: 2500 - Loss: 0.00048245463403873146\n",
      "Epoch: 3000 - Loss: 0.00044024831731803715\n",
      "Epoch: 3500 - Loss: 0.00040976479067467153\n",
      "Epoch: 4000 - Loss: 0.00038444437086582184\n",
      "Epoch: 4500 - Loss: 0.00035905808908864856\n",
      "Epoch: 5000 - Loss: 0.00033850257750600576\n",
      "Epoch: 5500 - Loss: 0.0003209018614143133\n",
      "Epoch: 6000 - Loss: 0.00030564566259272397\n",
      "Epoch: 6500 - Loss: 0.00029223680030554533\n",
      "Epoch: 7000 - Loss: 0.00028165391995571554\n",
      "Epoch: 7500 - Loss: 0.00027215329464524984\n",
      "Epoch: 8000 - Loss: 0.0002665651263669133\n",
      "Epoch: 8500 - Loss: 0.00026100699324160814\n",
      "Epoch: 9000 - Loss: 0.00025627220747992396\n",
      "Epoch: 9500 - Loss: 0.0002518253168091178\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.0367999076843262\n",
      "Epoch: 500 - Loss: 0.0010825167410075665\n",
      "Epoch: 1000 - Loss: 0.00044233695371076465\n",
      "Epoch: 1500 - Loss: 0.0003132871934212744\n",
      "Epoch: 2000 - Loss: 0.00024438140098936856\n",
      "Epoch: 2500 - Loss: 0.00020574990776367486\n",
      "Epoch: 3000 - Loss: 0.00018037755216937512\n",
      "Epoch: 3500 - Loss: 0.00016568880528211594\n",
      "Epoch: 4000 - Loss: 0.00015388760948553681\n",
      "Epoch: 4500 - Loss: 0.00014310058031696826\n",
      "Epoch: 5000 - Loss: 0.00013554334873333573\n",
      "Epoch: 5500 - Loss: 0.00012987364607397467\n",
      "Epoch: 6000 - Loss: 0.00012394993973430246\n",
      "Epoch: 6500 - Loss: 0.00012039086868753657\n",
      "Epoch: 7000 - Loss: 0.0001176991208922118\n",
      "Epoch: 7500 - Loss: 0.00011559596168808639\n",
      "Epoch: 8000 - Loss: 0.00011393114982638508\n",
      "Epoch: 8500 - Loss: 0.00011098747927462682\n",
      "Epoch: 9000 - Loss: 0.00010808933438966051\n",
      "Epoch: 9500 - Loss: 0.00010579424997558817\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.0828191041946411\n",
      "Epoch: 500 - Loss: 0.0012149653630331159\n",
      "Epoch: 1000 - Loss: 0.0004877957981079817\n",
      "Epoch: 1500 - Loss: 0.00033715457539074123\n",
      "Epoch: 2000 - Loss: 0.0002775212051346898\n",
      "Epoch: 2500 - Loss: 0.00024559092707931995\n",
      "Epoch: 3000 - Loss: 0.0002165693585993722\n",
      "Epoch: 3500 - Loss: 0.0001974691986106336\n",
      "Epoch: 4000 - Loss: 0.00018205582455266267\n",
      "Epoch: 4500 - Loss: 0.00017128845502156764\n",
      "Epoch: 5000 - Loss: 0.00016199170204345137\n",
      "Epoch: 5500 - Loss: 0.000153273344039917\n",
      "Epoch: 6000 - Loss: 0.0001456384197808802\n",
      "Epoch: 6500 - Loss: 0.00013912680151406676\n",
      "Epoch: 7000 - Loss: 0.00013394893903750926\n",
      "Epoch: 7500 - Loss: 0.0001297036506002769\n",
      "Epoch: 8000 - Loss: 0.00012446452456060797\n",
      "Epoch: 8500 - Loss: 0.00012091718963347375\n",
      "Epoch: 9000 - Loss: 0.00011767112300731242\n",
      "Epoch: 9500 - Loss: 0.00011390242434572428\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.7859926819801331\n",
      "Epoch: 500 - Loss: 0.0003054293629247695\n",
      "Epoch: 1000 - Loss: 0.00016440189210698009\n",
      "Epoch: 1500 - Loss: 0.00013346533523872495\n",
      "Epoch: 2000 - Loss: 0.00011747483949875459\n",
      "Epoch: 2500 - Loss: 0.0001064362731995061\n",
      "Epoch: 3000 - Loss: 9.659797797212377e-05\n",
      "Epoch: 3500 - Loss: 8.72303280630149e-05\n",
      "Epoch: 4000 - Loss: 7.919547351775691e-05\n",
      "Epoch: 4500 - Loss: 7.30909887352027e-05\n",
      "Epoch: 5000 - Loss: 6.878605927340686e-05\n",
      "Epoch: 5500 - Loss: 6.533505074912682e-05\n",
      "Epoch: 6000 - Loss: 6.247482087928802e-05\n",
      "Epoch: 6500 - Loss: 6.065294292056933e-05\n",
      "Epoch: 7000 - Loss: 5.917763337492943e-05\n",
      "Epoch: 7500 - Loss: 5.7925448345486075e-05\n",
      "Epoch: 8000 - Loss: 5.651617902913131e-05\n",
      "Epoch: 8500 - Loss: 5.521908315131441e-05\n",
      "Epoch: 9000 - Loss: 5.4605592595180497e-05\n",
      "Epoch: 9500 - Loss: 5.3797339205630124e-05\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.7265727519989014\n",
      "Epoch: 500 - Loss: 0.0002500754198990762\n",
      "Epoch: 1000 - Loss: 0.00013658560055773705\n",
      "Epoch: 1500 - Loss: 0.00011093060311395675\n",
      "Epoch: 2000 - Loss: 9.450475772609934e-05\n",
      "Epoch: 2500 - Loss: 8.319938206113875e-05\n",
      "Epoch: 3000 - Loss: 7.692971121286973e-05\n",
      "Epoch: 3500 - Loss: 7.163324335124344e-05\n",
      "Epoch: 4000 - Loss: 6.834659143351018e-05\n",
      "Epoch: 4500 - Loss: 6.45626278128475e-05\n",
      "Epoch: 5000 - Loss: 6.150829722173512e-05\n",
      "Epoch: 5500 - Loss: 5.874589260201901e-05\n",
      "Epoch: 6000 - Loss: 5.596661867457442e-05\n",
      "Epoch: 6500 - Loss: 5.272156704450026e-05\n",
      "Epoch: 7000 - Loss: 5.000826786272228e-05\n",
      "Epoch: 7500 - Loss: 4.858154716202989e-05\n",
      "Epoch: 8000 - Loss: 4.743054523714818e-05\n",
      "Epoch: 8500 - Loss: 4.619047467713244e-05\n",
      "Epoch: 9000 - Loss: 4.4788950617657974e-05\n",
      "Epoch: 9500 - Loss: 4.4037435145583004e-05\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.8648017048835754\n",
      "Epoch: 500 - Loss: 0.00041539649828337133\n",
      "Epoch: 1000 - Loss: 0.00021263948292471468\n",
      "Epoch: 1500 - Loss: 0.00017110008047893643\n",
      "Epoch: 2000 - Loss: 0.00014918246597517282\n",
      "Epoch: 2500 - Loss: 0.00013713720545638353\n",
      "Epoch: 3000 - Loss: 0.00012817553943023086\n",
      "Epoch: 3500 - Loss: 0.0001180507242679596\n",
      "Epoch: 4000 - Loss: 0.00011224992340430617\n",
      "Epoch: 4500 - Loss: 0.00010586010466795415\n",
      "Epoch: 5000 - Loss: 0.00010083405504701659\n",
      "Epoch: 5500 - Loss: 9.709958249004558e-05\n",
      "Epoch: 6000 - Loss: 9.339166717836633e-05\n",
      "Epoch: 6500 - Loss: 8.947105379775167e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7000 - Loss: 8.606599294580519e-05\n",
      "Epoch: 7500 - Loss: 8.346382674062625e-05\n",
      "Epoch: 8000 - Loss: 8.116186654660851e-05\n",
      "Epoch: 8500 - Loss: 7.94860752648674e-05\n",
      "Epoch: 9000 - Loss: 7.725832983851433e-05\n",
      "Epoch: 9500 - Loss: 7.532267773058265e-05\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 2.8218798637390137\n",
      "Epoch: 500 - Loss: 0.02086705155670643\n",
      "Epoch: 1000 - Loss: 0.006972708739340305\n",
      "Epoch: 1500 - Loss: 0.003952528815716505\n",
      "Epoch: 2000 - Loss: 0.0028461795300245285\n",
      "Epoch: 2500 - Loss: 0.002302461303770542\n",
      "Epoch: 3000 - Loss: 0.0019815689884126186\n",
      "Epoch: 3500 - Loss: 0.0017565840389579535\n",
      "Epoch: 4000 - Loss: 0.0015883585438132286\n",
      "Epoch: 4500 - Loss: 0.0014526573941111565\n",
      "Epoch: 5000 - Loss: 0.0013408176600933075\n",
      "Epoch: 5500 - Loss: 0.001251367386430502\n",
      "Epoch: 6000 - Loss: 0.001180357881821692\n",
      "Epoch: 6500 - Loss: 0.0011281404877081513\n",
      "Epoch: 7000 - Loss: 0.0010849989484995604\n",
      "Epoch: 7500 - Loss: 0.0010475903982296586\n",
      "Epoch: 8000 - Loss: 0.0010129238944500685\n",
      "Epoch: 8500 - Loss: 0.000977176707237959\n",
      "Epoch: 9000 - Loss: 0.0009433275554329157\n",
      "Epoch: 9500 - Loss: 0.0009170061093755066\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.9688442945480347\n",
      "Epoch: 500 - Loss: 0.00023584687733091414\n",
      "Epoch: 1000 - Loss: 0.00012240346404723823\n",
      "Epoch: 1500 - Loss: 9.55121940933168e-05\n",
      "Epoch: 2000 - Loss: 8.205007907235995e-05\n",
      "Epoch: 2500 - Loss: 7.535000622738153e-05\n",
      "Epoch: 3000 - Loss: 6.913222750881687e-05\n",
      "Epoch: 3500 - Loss: 6.45044056000188e-05\n",
      "Epoch: 4000 - Loss: 6.179571937536821e-05\n",
      "Epoch: 4500 - Loss: 5.851261448697187e-05\n",
      "Epoch: 5000 - Loss: 5.5131324188550934e-05\n",
      "Epoch: 5500 - Loss: 5.330599014996551e-05\n",
      "Epoch: 6000 - Loss: 5.1346421969356015e-05\n",
      "Epoch: 6500 - Loss: 5.032862463849597e-05\n",
      "Epoch: 7000 - Loss: 4.9934144044527784e-05\n",
      "Epoch: 7500 - Loss: 4.849667675443925e-05\n",
      "Epoch: 8000 - Loss: 4.643892316380516e-05\n",
      "Epoch: 8500 - Loss: 4.47472048108466e-05\n",
      "Epoch: 9000 - Loss: 4.315624028095044e-05\n",
      "Epoch: 9500 - Loss: 4.2445313738426194e-05\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.7911754846572876\n",
      "Epoch: 500 - Loss: 0.00031986343674361706\n",
      "Epoch: 1000 - Loss: 0.00015535630518570542\n",
      "Epoch: 1500 - Loss: 0.00012376972881611437\n",
      "Epoch: 2000 - Loss: 0.00010693016520235687\n",
      "Epoch: 2500 - Loss: 9.621579374652356e-05\n",
      "Epoch: 3000 - Loss: 8.723998325876892e-05\n",
      "Epoch: 3500 - Loss: 7.989723235368729e-05\n",
      "Epoch: 4000 - Loss: 7.650165935046971e-05\n",
      "Epoch: 4500 - Loss: 7.20933749107644e-05\n",
      "Epoch: 5000 - Loss: 6.901330198161304e-05\n",
      "Epoch: 5500 - Loss: 6.681589729851112e-05\n",
      "Epoch: 6000 - Loss: 6.470228254329413e-05\n",
      "Epoch: 6500 - Loss: 6.173725705593824e-05\n",
      "Epoch: 7000 - Loss: 5.926307130721398e-05\n",
      "Epoch: 7500 - Loss: 5.68303294130601e-05\n",
      "Epoch: 8000 - Loss: 5.458877421915531e-05\n",
      "Epoch: 8500 - Loss: 5.270743713481352e-05\n",
      "Epoch: 9000 - Loss: 5.025223799748346e-05\n",
      "Epoch: 9500 - Loss: 4.830598481930792e-05\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.9527069330215454\n",
      "Epoch: 500 - Loss: 0.0005001942627131939\n",
      "Epoch: 1000 - Loss: 0.00027080715517513454\n",
      "Epoch: 1500 - Loss: 0.00021985667990520597\n",
      "Epoch: 2000 - Loss: 0.00019333504315000027\n",
      "Epoch: 2500 - Loss: 0.000173522814293392\n",
      "Epoch: 3000 - Loss: 0.00016089231939986348\n",
      "Epoch: 3500 - Loss: 0.00014631891099270433\n",
      "Epoch: 4000 - Loss: 0.0001342219766229391\n",
      "Epoch: 4500 - Loss: 0.00012682582018896937\n",
      "Epoch: 5000 - Loss: 0.00011997697583865374\n",
      "Epoch: 5500 - Loss: 0.00011309441470075399\n",
      "Epoch: 6000 - Loss: 0.00010944352834485471\n",
      "Epoch: 6500 - Loss: 0.00010646303417161107\n",
      "Epoch: 7000 - Loss: 0.00010289758211001754\n",
      "Epoch: 7500 - Loss: 9.907010826282203e-05\n",
      "Epoch: 8000 - Loss: 9.574700379744172e-05\n",
      "Epoch: 8500 - Loss: 9.285030682804063e-05\n",
      "Epoch: 9000 - Loss: 8.882985275704414e-05\n",
      "Epoch: 9500 - Loss: 8.501850970787928e-05\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.126102089881897\n",
      "Epoch: 500 - Loss: 0.001190100098028779\n",
      "Epoch: 1000 - Loss: 0.0005277263699099422\n",
      "Epoch: 1500 - Loss: 0.0003699114895425737\n",
      "Epoch: 2000 - Loss: 0.00030266481917351484\n",
      "Epoch: 2500 - Loss: 0.0002692460548132658\n",
      "Epoch: 3000 - Loss: 0.00024717862834222615\n",
      "Epoch: 3500 - Loss: 0.00022950574930291623\n",
      "Epoch: 4000 - Loss: 0.0002146138867828995\n",
      "Epoch: 4500 - Loss: 0.00020260554447304457\n",
      "Epoch: 5000 - Loss: 0.00019285488815512508\n",
      "Epoch: 5500 - Loss: 0.00018264823302160949\n",
      "Epoch: 6000 - Loss: 0.0001728299102978781\n",
      "Epoch: 6500 - Loss: 0.00016644663992337883\n",
      "Epoch: 7000 - Loss: 0.00016062358918134123\n",
      "Epoch: 7500 - Loss: 0.0001570537715451792\n",
      "Epoch: 8000 - Loss: 0.00015484969480894506\n",
      "Epoch: 8500 - Loss: 0.00015220785280689597\n",
      "Epoch: 9000 - Loss: 0.00014925102004781365\n",
      "Epoch: 9500 - Loss: 0.00014596527034882456\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.7855663895606995\n",
      "Epoch: 500 - Loss: 0.0010996228083968163\n",
      "Epoch: 1000 - Loss: 0.0004697172262240201\n",
      "Epoch: 1500 - Loss: 0.00033310672733932734\n",
      "Epoch: 2000 - Loss: 0.0002782740630209446\n",
      "Epoch: 2500 - Loss: 0.00024878542171791196\n",
      "Epoch: 3000 - Loss: 0.00023132050409913063\n",
      "Epoch: 3500 - Loss: 0.00021960382582619786\n",
      "Epoch: 4000 - Loss: 0.00020493859483394772\n",
      "Epoch: 4500 - Loss: 0.00019328836060594767\n",
      "Epoch: 5000 - Loss: 0.00018610812549013644\n",
      "Epoch: 5500 - Loss: 0.00018063868628814816\n",
      "Epoch: 6000 - Loss: 0.00017611568910069764\n",
      "Epoch: 6500 - Loss: 0.00017156352987512946\n",
      "Epoch: 7000 - Loss: 0.00016821094322949648\n",
      "Epoch: 7500 - Loss: 0.0001642743736738339\n",
      "Epoch: 8000 - Loss: 0.000160993033205159\n",
      "Epoch: 8500 - Loss: 0.0001582693075761199\n",
      "Epoch: 9000 - Loss: 0.00015569844981655478\n",
      "Epoch: 9500 - Loss: 0.00015332881594076753\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.7206735014915466\n",
      "Epoch: 500 - Loss: 0.0007073319866321981\n",
      "Epoch: 1000 - Loss: 0.00030796232749707997\n",
      "Epoch: 1500 - Loss: 0.00023157085524871945\n",
      "Epoch: 2000 - Loss: 0.0002001474640564993\n",
      "Epoch: 2500 - Loss: 0.00017972633941099048\n",
      "Epoch: 3000 - Loss: 0.0001668724580667913\n",
      "Epoch: 3500 - Loss: 0.0001583318371558562\n",
      "Epoch: 4000 - Loss: 0.0001499073696322739\n",
      "Epoch: 4500 - Loss: 0.0001427496608812362\n",
      "Epoch: 5000 - Loss: 0.00013674594811163843\n",
      "Epoch: 5500 - Loss: 0.00013089521962683648\n",
      "Epoch: 6000 - Loss: 0.0001243431615876034\n",
      "Epoch: 6500 - Loss: 0.0001202679704874754\n",
      "Epoch: 7000 - Loss: 0.00011525619629537687\n",
      "Epoch: 7500 - Loss: 0.000110944478365127\n",
      "Epoch: 8000 - Loss: 0.00010761353769339621\n",
      "Epoch: 8500 - Loss: 0.00010448967077536508\n",
      "Epoch: 9000 - Loss: 0.000100826880952809\n",
      "Epoch: 9500 - Loss: 9.669360588304698e-05\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.7433865070343018\n",
      "Epoch: 500 - Loss: 0.0002594170509837568\n",
      "Epoch: 1000 - Loss: 0.00014194076356943697\n",
      "Epoch: 1500 - Loss: 0.0001074706669896841\n",
      "Epoch: 2000 - Loss: 8.981039718491957e-05\n",
      "Epoch: 2500 - Loss: 7.761534652672708e-05\n",
      "Epoch: 3000 - Loss: 6.881512672407553e-05\n",
      "Epoch: 3500 - Loss: 6.222762021934614e-05\n",
      "Epoch: 4000 - Loss: 5.844985935254954e-05\n",
      "Epoch: 4500 - Loss: 5.48715797776822e-05\n",
      "Epoch: 5000 - Loss: 5.155370672582649e-05\n",
      "Epoch: 5500 - Loss: 4.8469039029441774e-05\n",
      "Epoch: 6000 - Loss: 4.617231388692744e-05\n",
      "Epoch: 6500 - Loss: 4.470935164135881e-05\n",
      "Epoch: 7000 - Loss: 4.3111253035021946e-05\n",
      "Epoch: 7500 - Loss: 4.182574775768444e-05\n",
      "Epoch: 8000 - Loss: 4.101890590391122e-05\n",
      "Epoch: 8500 - Loss: 3.971318801632151e-05\n",
      "Epoch: 9000 - Loss: 3.847396874334663e-05\n",
      "Epoch: 9500 - Loss: 3.7800418795086443e-05\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.021339774131775\n",
      "Epoch: 500 - Loss: 0.0017634867690503597\n",
      "Epoch: 1000 - Loss: 0.0006841670256108046\n",
      "Epoch: 1500 - Loss: 0.00046198166091926396\n",
      "Epoch: 2000 - Loss: 0.000369247019989416\n",
      "Epoch: 2500 - Loss: 0.00032361314515583217\n",
      "Epoch: 3000 - Loss: 0.0002972020593006164\n",
      "Epoch: 3500 - Loss: 0.0002787628909572959\n",
      "Epoch: 4000 - Loss: 0.0002613820834085345\n",
      "Epoch: 4500 - Loss: 0.00024365093850065023\n",
      "Epoch: 5000 - Loss: 0.0002278796600876376\n",
      "Epoch: 5500 - Loss: 0.00021570386888924986\n",
      "Epoch: 6000 - Loss: 0.0002064043073914945\n",
      "Epoch: 6500 - Loss: 0.0001991723256651312\n",
      "Epoch: 7000 - Loss: 0.00019289061310701072\n",
      "Epoch: 7500 - Loss: 0.00018733061733655632\n",
      "Epoch: 8000 - Loss: 0.00018378208915237337\n",
      "Epoch: 8500 - Loss: 0.0001806173095246777\n",
      "Epoch: 9000 - Loss: 0.00017700210446491838\n",
      "Epoch: 9500 - Loss: 0.00017183244926854968\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.0943760871887207\n",
      "Epoch: 500 - Loss: 0.004290468525141478\n",
      "Epoch: 1000 - Loss: 0.0016462591011077166\n",
      "Epoch: 1500 - Loss: 0.0010139925871044397\n",
      "Epoch: 2000 - Loss: 0.0007697315304540098\n",
      "Epoch: 2500 - Loss: 0.0006355300429277122\n",
      "Epoch: 3000 - Loss: 0.000554208701942116\n",
      "Epoch: 3500 - Loss: 0.0005035368958488107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4000 - Loss: 0.0004666099266614765\n",
      "Epoch: 4500 - Loss: 0.00044529660954140127\n",
      "Epoch: 5000 - Loss: 0.00042811420280486345\n",
      "Epoch: 5500 - Loss: 0.00041161320405080914\n",
      "Epoch: 6000 - Loss: 0.0003975808504037559\n",
      "Epoch: 6500 - Loss: 0.0003846089821308851\n",
      "Epoch: 7000 - Loss: 0.0003758023667614907\n",
      "Epoch: 7500 - Loss: 0.00036921779974363744\n",
      "Epoch: 8000 - Loss: 0.0003611668071243912\n",
      "Epoch: 8500 - Loss: 0.00035193946678191423\n",
      "Epoch: 9000 - Loss: 0.0003436727565713227\n",
      "Epoch: 9500 - Loss: 0.00033490368514321744\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.7591738700866699\n",
      "Epoch: 500 - Loss: 0.0003941926406696439\n",
      "Epoch: 1000 - Loss: 0.00018294592155143619\n",
      "Epoch: 1500 - Loss: 0.00013808900257572532\n",
      "Epoch: 2000 - Loss: 0.00011748636461561546\n",
      "Epoch: 2500 - Loss: 0.00010531345469644293\n",
      "Epoch: 3000 - Loss: 9.405449964106083e-05\n",
      "Epoch: 3500 - Loss: 8.687808440299705e-05\n",
      "Epoch: 4000 - Loss: 8.167362830135971e-05\n",
      "Epoch: 4500 - Loss: 7.627667218912393e-05\n",
      "Epoch: 5000 - Loss: 7.340232696151361e-05\n",
      "Epoch: 5500 - Loss: 7.17931761755608e-05\n",
      "Epoch: 6000 - Loss: 7.101068331394345e-05\n",
      "Epoch: 6500 - Loss: 6.978974852245301e-05\n",
      "Epoch: 7000 - Loss: 6.873872189316899e-05\n",
      "Epoch: 7500 - Loss: 6.744189886376262e-05\n",
      "Epoch: 8000 - Loss: 6.583508366020396e-05\n",
      "Epoch: 8500 - Loss: 6.401972495950758e-05\n",
      "Epoch: 9000 - Loss: 6.262037641135976e-05\n",
      "Epoch: 9500 - Loss: 6.0976206441409886e-05\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.0461411476135254\n",
      "Epoch: 500 - Loss: 0.0007579210214316845\n",
      "Epoch: 1000 - Loss: 0.00028752951766364276\n",
      "Epoch: 1500 - Loss: 0.00017789064440876245\n",
      "Epoch: 2000 - Loss: 0.0001350980601273477\n",
      "Epoch: 2500 - Loss: 0.0001161147010861896\n",
      "Epoch: 3000 - Loss: 0.00010558970097918063\n",
      "Epoch: 3500 - Loss: 9.874537499854341e-05\n",
      "Epoch: 4000 - Loss: 9.34336130740121e-05\n",
      "Epoch: 4500 - Loss: 8.678429003339261e-05\n",
      "Epoch: 5000 - Loss: 8.01288042566739e-05\n",
      "Epoch: 5500 - Loss: 7.528603600803763e-05\n",
      "Epoch: 6000 - Loss: 7.242507126647979e-05\n",
      "Epoch: 6500 - Loss: 7.052968430798501e-05\n",
      "Epoch: 7000 - Loss: 6.9103465648368e-05\n",
      "Epoch: 7500 - Loss: 6.778898386983201e-05\n",
      "Epoch: 8000 - Loss: 6.629193376284093e-05\n",
      "Epoch: 8500 - Loss: 6.526890501845628e-05\n",
      "Epoch: 9000 - Loss: 6.386201130226254e-05\n",
      "Epoch: 9500 - Loss: 6.231189036043361e-05\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.0849751234054565\n",
      "Epoch: 500 - Loss: 0.0001545643899589777\n",
      "Epoch: 1000 - Loss: 9.062973549589515e-05\n",
      "Epoch: 1500 - Loss: 6.942132313270122e-05\n",
      "Epoch: 2000 - Loss: 5.7515790103934705e-05\n",
      "Epoch: 2500 - Loss: 4.960340083925985e-05\n",
      "Epoch: 3000 - Loss: 4.524647374637425e-05\n",
      "Epoch: 3500 - Loss: 4.196769077680074e-05\n",
      "Epoch: 4000 - Loss: 3.9820508391130716e-05\n",
      "Epoch: 4500 - Loss: 3.828499757219106e-05\n",
      "Epoch: 5000 - Loss: 3.7739198887720704e-05\n",
      "Epoch: 5500 - Loss: 3.7165820685913786e-05\n",
      "Epoch: 6000 - Loss: 3.62104874511715e-05\n",
      "Epoch: 6500 - Loss: 3.461643063928932e-05\n",
      "Epoch: 7000 - Loss: 3.3603268093429506e-05\n",
      "Epoch: 7500 - Loss: 3.252662645536475e-05\n",
      "Epoch: 8000 - Loss: 3.1444127671420574e-05\n",
      "Epoch: 8500 - Loss: 3.0457273169304244e-05\n",
      "Epoch: 9000 - Loss: 2.9575372536783107e-05\n",
      "Epoch: 9500 - Loss: 2.8911044864798896e-05\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.6508102416992188\n",
      "Epoch: 500 - Loss: 0.0005243973573669791\n",
      "Epoch: 1000 - Loss: 0.0002558346895966679\n",
      "Epoch: 1500 - Loss: 0.00019953897572122514\n",
      "Epoch: 2000 - Loss: 0.00017421352094970644\n",
      "Epoch: 2500 - Loss: 0.00015729182632640004\n",
      "Epoch: 3000 - Loss: 0.00014523004938382655\n",
      "Epoch: 3500 - Loss: 0.00013692658103536814\n",
      "Epoch: 4000 - Loss: 0.0001294630201300606\n",
      "Epoch: 4500 - Loss: 0.00012146797962486744\n",
      "Epoch: 5000 - Loss: 0.00011675042333081365\n",
      "Epoch: 5500 - Loss: 0.00011327087122481316\n",
      "Epoch: 6000 - Loss: 0.00010959325300063938\n",
      "Epoch: 6500 - Loss: 0.00010505298268981278\n",
      "Epoch: 7000 - Loss: 0.00010187884618062526\n",
      "Epoch: 7500 - Loss: 9.87208986771293e-05\n",
      "Epoch: 8000 - Loss: 9.628690168028697e-05\n",
      "Epoch: 8500 - Loss: 9.382925054524094e-05\n",
      "Epoch: 9000 - Loss: 9.117183071793988e-05\n",
      "Epoch: 9500 - Loss: 8.774235902819782e-05\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.9357651472091675\n",
      "Epoch: 500 - Loss: 0.0014913937775418162\n",
      "Epoch: 1000 - Loss: 0.0006460167351178825\n",
      "Epoch: 1500 - Loss: 0.00048691246774978936\n",
      "Epoch: 2000 - Loss: 0.00041831089765764773\n",
      "Epoch: 2500 - Loss: 0.00037470829556696117\n",
      "Epoch: 3000 - Loss: 0.00034557897015474737\n",
      "Epoch: 3500 - Loss: 0.0003231596201658249\n",
      "Epoch: 4000 - Loss: 0.00031012186082080007\n",
      "Epoch: 4500 - Loss: 0.0002979932469315827\n",
      "Epoch: 5000 - Loss: 0.0002891856129281223\n",
      "Epoch: 5500 - Loss: 0.0002808694844134152\n",
      "Epoch: 6000 - Loss: 0.0002752626605797559\n",
      "Epoch: 6500 - Loss: 0.00027114676777273417\n",
      "Epoch: 7000 - Loss: 0.00026504418929107487\n",
      "Epoch: 7500 - Loss: 0.00025841587921604514\n",
      "Epoch: 8000 - Loss: 0.00025469838874414563\n",
      "Epoch: 8500 - Loss: 0.00025170468143187463\n",
      "Epoch: 9000 - Loss: 0.0002477832022123039\n",
      "Epoch: 9500 - Loss: 0.000245303672272712\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.6799862384796143\n",
      "Epoch: 500 - Loss: 0.0018146526999771595\n",
      "Epoch: 1000 - Loss: 0.0007679218542762101\n",
      "Epoch: 1500 - Loss: 0.000545736460480839\n",
      "Epoch: 2000 - Loss: 0.0004545855917967856\n",
      "Epoch: 2500 - Loss: 0.000395279930671677\n",
      "Epoch: 3000 - Loss: 0.0003534054267220199\n",
      "Epoch: 3500 - Loss: 0.00032200399436987936\n",
      "Epoch: 4000 - Loss: 0.0002991556248161942\n",
      "Epoch: 4500 - Loss: 0.0002806660777423531\n",
      "Epoch: 5000 - Loss: 0.0002643126354087144\n",
      "Epoch: 5500 - Loss: 0.00025074221775867045\n",
      "Epoch: 6000 - Loss: 0.0002428169536869973\n",
      "Epoch: 6500 - Loss: 0.00023717012663837522\n",
      "Epoch: 7000 - Loss: 0.00022946503304410726\n",
      "Epoch: 7500 - Loss: 0.0002210718230344355\n",
      "Epoch: 8000 - Loss: 0.00021294242469593883\n",
      "Epoch: 8500 - Loss: 0.00020752135606016964\n",
      "Epoch: 9000 - Loss: 0.00020356534514576197\n",
      "Epoch: 9500 - Loss: 0.000200204667635262\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.1221957206726074\n",
      "Epoch: 500 - Loss: 0.002385379746556282\n",
      "Epoch: 1000 - Loss: 0.0010308946948498487\n",
      "Epoch: 1500 - Loss: 0.0007050571730360389\n",
      "Epoch: 2000 - Loss: 0.0005674946587532759\n",
      "Epoch: 2500 - Loss: 0.00048382405657321215\n",
      "Epoch: 3000 - Loss: 0.0004304549365770072\n",
      "Epoch: 3500 - Loss: 0.0003943718911614269\n",
      "Epoch: 4000 - Loss: 0.00036341164377518\n",
      "Epoch: 4500 - Loss: 0.0003418857522774488\n",
      "Epoch: 5000 - Loss: 0.00032503879629075527\n",
      "Epoch: 5500 - Loss: 0.00030910747591406107\n",
      "Epoch: 6000 - Loss: 0.0002951218339148909\n",
      "Epoch: 6500 - Loss: 0.0002852145116776228\n",
      "Epoch: 7000 - Loss: 0.00027617847081273794\n",
      "Epoch: 7500 - Loss: 0.00026871170848608017\n",
      "Epoch: 8000 - Loss: 0.00026267205248586833\n",
      "Epoch: 8500 - Loss: 0.00025723964790813625\n",
      "Epoch: 9000 - Loss: 0.00025027632364071906\n",
      "Epoch: 9500 - Loss: 0.00024347272119484842\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.2331901788711548\n",
      "Epoch: 500 - Loss: 0.0003395612584426999\n",
      "Epoch: 1000 - Loss: 0.0001683169393800199\n",
      "Epoch: 1500 - Loss: 0.00011733417341019958\n",
      "Epoch: 2000 - Loss: 9.423617302672938e-05\n",
      "Epoch: 2500 - Loss: 8.127973705995828e-05\n",
      "Epoch: 3000 - Loss: 7.147437281673774e-05\n",
      "Epoch: 3500 - Loss: 6.401059363270178e-05\n",
      "Epoch: 4000 - Loss: 5.72167155041825e-05\n",
      "Epoch: 4500 - Loss: 5.299138865666464e-05\n",
      "Epoch: 5000 - Loss: 4.922854350297712e-05\n",
      "Epoch: 5500 - Loss: 4.6083358029136434e-05\n",
      "Epoch: 6000 - Loss: 4.363900006865151e-05\n",
      "Epoch: 6500 - Loss: 4.175466892775148e-05\n",
      "Epoch: 7000 - Loss: 3.943377669202164e-05\n",
      "Epoch: 7500 - Loss: 3.665544500108808e-05\n",
      "Epoch: 8000 - Loss: 3.4680244425544515e-05\n",
      "Epoch: 8500 - Loss: 3.3109976357081905e-05\n",
      "Epoch: 9000 - Loss: 3.180449493811466e-05\n",
      "Epoch: 9500 - Loss: 3.081489194300957e-05\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.3011581897735596\n",
      "Epoch: 500 - Loss: 0.004343447275459766\n",
      "Epoch: 1000 - Loss: 0.0019008792005479336\n",
      "Epoch: 1500 - Loss: 0.001279131742194295\n",
      "Epoch: 2000 - Loss: 0.0010095362085849047\n",
      "Epoch: 2500 - Loss: 0.0008497919770888984\n",
      "Epoch: 3000 - Loss: 0.0007427938398905098\n",
      "Epoch: 3500 - Loss: 0.0006664778338745236\n",
      "Epoch: 4000 - Loss: 0.0006011778023093939\n",
      "Epoch: 4500 - Loss: 0.0005473957862704992\n",
      "Epoch: 5000 - Loss: 0.0005074701039120555\n",
      "Epoch: 5500 - Loss: 0.0004733471432700753\n",
      "Epoch: 6000 - Loss: 0.00044251736835576594\n",
      "Epoch: 6500 - Loss: 0.0004191006300970912\n",
      "Epoch: 7000 - Loss: 0.0003982006455771625\n",
      "Epoch: 7500 - Loss: 0.0003795950033236295\n",
      "Epoch: 8000 - Loss: 0.00036438144161365926\n",
      "Epoch: 8500 - Loss: 0.00035133882192894816\n",
      "Epoch: 9000 - Loss: 0.00033727652044035494\n",
      "Epoch: 9500 - Loss: 0.0003262957907281816\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.9767674803733826\n",
      "Epoch: 500 - Loss: 0.0020711871329694986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 - Loss: 0.0008350228308700025\n",
      "Epoch: 1500 - Loss: 0.0005508192116394639\n",
      "Epoch: 2000 - Loss: 0.00044171782792545855\n",
      "Epoch: 2500 - Loss: 0.0003794905205722898\n",
      "Epoch: 3000 - Loss: 0.0003410362114664167\n",
      "Epoch: 3500 - Loss: 0.0003101150505244732\n",
      "Epoch: 4000 - Loss: 0.0002916003577411175\n",
      "Epoch: 4500 - Loss: 0.0002777316840365529\n",
      "Epoch: 5000 - Loss: 0.0002663183549884707\n",
      "Epoch: 5500 - Loss: 0.0002569221833255142\n",
      "Epoch: 6000 - Loss: 0.000249540142249316\n",
      "Epoch: 6500 - Loss: 0.00024483216111548245\n",
      "Epoch: 7000 - Loss: 0.00023893579782452434\n",
      "Epoch: 7500 - Loss: 0.00023500794486608356\n",
      "Epoch: 8000 - Loss: 0.00022872482077218592\n",
      "Epoch: 8500 - Loss: 0.00022119104687590152\n",
      "Epoch: 9000 - Loss: 0.00021576376457232982\n",
      "Epoch: 9500 - Loss: 0.00021225438104011118\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.1682430505752563\n",
      "Epoch: 500 - Loss: 0.0016594919143244624\n",
      "Epoch: 1000 - Loss: 0.000724718498531729\n",
      "Epoch: 1500 - Loss: 0.0005355305038392544\n",
      "Epoch: 2000 - Loss: 0.00045775328180752695\n",
      "Epoch: 2500 - Loss: 0.0004103614774066955\n",
      "Epoch: 3000 - Loss: 0.0003681293746922165\n",
      "Epoch: 3500 - Loss: 0.0003367372846696526\n",
      "Epoch: 4000 - Loss: 0.0003171560529153794\n",
      "Epoch: 4500 - Loss: 0.00029766338411718607\n",
      "Epoch: 5000 - Loss: 0.0002832775062415749\n",
      "Epoch: 5500 - Loss: 0.0002709069231059402\n",
      "Epoch: 6000 - Loss: 0.00026094785425812006\n",
      "Epoch: 6500 - Loss: 0.00025124603416770697\n",
      "Epoch: 7000 - Loss: 0.00024341745302081108\n",
      "Epoch: 7500 - Loss: 0.00023538421373814344\n",
      "Epoch: 8000 - Loss: 0.00022802689636591822\n",
      "Epoch: 8500 - Loss: 0.00022436046856455505\n",
      "Epoch: 9000 - Loss: 0.00022016727598384023\n",
      "Epoch: 9500 - Loss: 0.00021517864661291242\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.5298782587051392\n",
      "Epoch: 500 - Loss: 0.0009183674119412899\n",
      "Epoch: 1000 - Loss: 0.00038697876152582467\n",
      "Epoch: 1500 - Loss: 0.0002647631918080151\n",
      "Epoch: 2000 - Loss: 0.00021761741663794965\n",
      "Epoch: 2500 - Loss: 0.00019056662858929485\n",
      "Epoch: 3000 - Loss: 0.0001682042347965762\n",
      "Epoch: 3500 - Loss: 0.0001540042576380074\n",
      "Epoch: 4000 - Loss: 0.0001440611231373623\n",
      "Epoch: 4500 - Loss: 0.00013346118794288486\n",
      "Epoch: 5000 - Loss: 0.0001235729141626507\n",
      "Epoch: 5500 - Loss: 0.00011797519982792437\n",
      "Epoch: 6000 - Loss: 0.0001131634198827669\n",
      "Epoch: 6500 - Loss: 0.00010688943439163268\n",
      "Epoch: 7000 - Loss: 0.00010199184180237353\n",
      "Epoch: 7500 - Loss: 9.70498876995407e-05\n",
      "Epoch: 8000 - Loss: 9.335247887065634e-05\n",
      "Epoch: 8500 - Loss: 8.955092926044017e-05\n",
      "Epoch: 9000 - Loss: 8.58717248775065e-05\n",
      "Epoch: 9500 - Loss: 8.324487134814262e-05\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.27974534034729\n",
      "Epoch: 500 - Loss: 0.0035630378406494856\n",
      "Epoch: 1000 - Loss: 0.001525997999124229\n",
      "Epoch: 1500 - Loss: 0.0010320500005036592\n",
      "Epoch: 2000 - Loss: 0.0008079680847004056\n",
      "Epoch: 2500 - Loss: 0.0006862492300570011\n",
      "Epoch: 3000 - Loss: 0.0006128168315626681\n",
      "Epoch: 3500 - Loss: 0.000568953575566411\n",
      "Epoch: 4000 - Loss: 0.0005374619504436851\n",
      "Epoch: 4500 - Loss: 0.0005120636778883636\n",
      "Epoch: 5000 - Loss: 0.000487934157717973\n",
      "Epoch: 5500 - Loss: 0.00047246876056306064\n",
      "Epoch: 6000 - Loss: 0.0004536542692221701\n",
      "Epoch: 6500 - Loss: 0.00044250194332562387\n",
      "Epoch: 7000 - Loss: 0.00043062196345999837\n",
      "Epoch: 7500 - Loss: 0.00042176491115242243\n",
      "Epoch: 8000 - Loss: 0.00041368594975210726\n",
      "Epoch: 8500 - Loss: 0.0004053183947689831\n",
      "Epoch: 9000 - Loss: 0.00039780509541742504\n",
      "Epoch: 9500 - Loss: 0.00038985637365840375\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.9033657312393188\n",
      "Epoch: 500 - Loss: 0.001045222976244986\n",
      "Epoch: 1000 - Loss: 0.0004601554246619344\n",
      "Epoch: 1500 - Loss: 0.0003329412720631808\n",
      "Epoch: 2000 - Loss: 0.00027669963310472667\n",
      "Epoch: 2500 - Loss: 0.0002448006998747587\n",
      "Epoch: 3000 - Loss: 0.00022254898794926703\n",
      "Epoch: 3500 - Loss: 0.00020388353732414544\n",
      "Epoch: 4000 - Loss: 0.00019134374451823533\n",
      "Epoch: 4500 - Loss: 0.00018269458087161183\n",
      "Epoch: 5000 - Loss: 0.00017201896116603166\n",
      "Epoch: 5500 - Loss: 0.00016478919133078307\n",
      "Epoch: 6000 - Loss: 0.0001609255705261603\n",
      "Epoch: 6500 - Loss: 0.00015895627439022064\n",
      "Epoch: 7000 - Loss: 0.0001558063377160579\n",
      "Epoch: 7500 - Loss: 0.00015249736316036433\n",
      "Epoch: 8000 - Loss: 0.00014659520820714533\n",
      "Epoch: 8500 - Loss: 0.000141869779326953\n",
      "Epoch: 9000 - Loss: 0.00013829499948769808\n",
      "Epoch: 9500 - Loss: 0.00013530136493500322\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.9898731708526611\n",
      "Epoch: 500 - Loss: 0.0003791616763919592\n",
      "Epoch: 1000 - Loss: 0.0001838921452872455\n",
      "Epoch: 1500 - Loss: 0.00013742799637839198\n",
      "Epoch: 2000 - Loss: 0.00011348652333253995\n",
      "Epoch: 2500 - Loss: 9.7276883025188e-05\n",
      "Epoch: 3000 - Loss: 8.689316746313125e-05\n",
      "Epoch: 3500 - Loss: 7.912488945294172e-05\n",
      "Epoch: 4000 - Loss: 7.309181819437072e-05\n",
      "Epoch: 4500 - Loss: 6.812512583564967e-05\n",
      "Epoch: 5000 - Loss: 6.424475577659905e-05\n",
      "Epoch: 5500 - Loss: 6.181064964039251e-05\n",
      "Epoch: 6000 - Loss: 5.944008080405183e-05\n",
      "Epoch: 6500 - Loss: 5.7053268392337486e-05\n",
      "Epoch: 7000 - Loss: 5.4578151321038604e-05\n",
      "Epoch: 7500 - Loss: 5.2807237807428464e-05\n",
      "Epoch: 8000 - Loss: 5.1300739869475365e-05\n",
      "Epoch: 8500 - Loss: 5.006725405110046e-05\n",
      "Epoch: 9000 - Loss: 4.9096950533566996e-05\n",
      "Epoch: 9500 - Loss: 4.869239273830317e-05\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.9852170348167419\n",
      "Epoch: 500 - Loss: 0.0019842321053147316\n",
      "Epoch: 1000 - Loss: 0.0009393757791258395\n",
      "Epoch: 1500 - Loss: 0.000649251218419522\n",
      "Epoch: 2000 - Loss: 0.0005190047086216509\n",
      "Epoch: 2500 - Loss: 0.00044111424358561635\n",
      "Epoch: 3000 - Loss: 0.00039213578565977514\n",
      "Epoch: 3500 - Loss: 0.0003550140536390245\n",
      "Epoch: 4000 - Loss: 0.0003282158577349037\n",
      "Epoch: 4500 - Loss: 0.0003069500089623034\n",
      "Epoch: 5000 - Loss: 0.0002919813559856266\n",
      "Epoch: 5500 - Loss: 0.0002795954351313412\n",
      "Epoch: 6000 - Loss: 0.00026542184059508145\n",
      "Epoch: 6500 - Loss: 0.0002546198957134038\n",
      "Epoch: 7000 - Loss: 0.0002478651003912091\n",
      "Epoch: 7500 - Loss: 0.00024384459538850933\n",
      "Epoch: 8000 - Loss: 0.00023768909159116447\n",
      "Epoch: 8500 - Loss: 0.0002315037854714319\n",
      "Epoch: 9000 - Loss: 0.00022699117835145444\n",
      "Epoch: 9500 - Loss: 0.00022176152560859919\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.9116342067718506\n",
      "Epoch: 500 - Loss: 0.001059750677086413\n",
      "Epoch: 1000 - Loss: 0.0004548851284198463\n",
      "Epoch: 1500 - Loss: 0.00031169038265943527\n",
      "Epoch: 2000 - Loss: 0.00024740153457969427\n",
      "Epoch: 2500 - Loss: 0.0002100606943713501\n",
      "Epoch: 3000 - Loss: 0.00018536299467086792\n",
      "Epoch: 3500 - Loss: 0.00016711591160856187\n",
      "Epoch: 4000 - Loss: 0.00015333673218265176\n",
      "Epoch: 4500 - Loss: 0.00014220166485756636\n",
      "Epoch: 5000 - Loss: 0.00013386702630668879\n",
      "Epoch: 5500 - Loss: 0.0001272634108318016\n",
      "Epoch: 6000 - Loss: 0.00011851893941638991\n",
      "Epoch: 6500 - Loss: 0.00011110157356597483\n",
      "Epoch: 7000 - Loss: 0.00010677757381927222\n",
      "Epoch: 7500 - Loss: 0.00010200832184636965\n",
      "Epoch: 8000 - Loss: 9.767759911483154e-05\n",
      "Epoch: 8500 - Loss: 9.31746035348624e-05\n",
      "Epoch: 9000 - Loss: 8.876439096638933e-05\n",
      "Epoch: 9500 - Loss: 8.461918332614005e-05\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 4.259442329406738\n",
      "Epoch: 500 - Loss: 0.011823180131614208\n",
      "Epoch: 1000 - Loss: 0.005351151805371046\n",
      "Epoch: 1500 - Loss: 0.0036714235320687294\n",
      "Epoch: 2000 - Loss: 0.002907062880694866\n",
      "Epoch: 2500 - Loss: 0.0024648921098560095\n",
      "Epoch: 3000 - Loss: 0.002196675632148981\n",
      "Epoch: 3500 - Loss: 0.002011279109865427\n",
      "Epoch: 4000 - Loss: 0.0018671960569918156\n",
      "Epoch: 4500 - Loss: 0.001761743682436645\n",
      "Epoch: 5000 - Loss: 0.0016797678545117378\n",
      "Epoch: 5500 - Loss: 0.0016056912718340755\n",
      "Epoch: 6000 - Loss: 0.0015311874449253082\n",
      "Epoch: 6500 - Loss: 0.0014702480984851718\n",
      "Epoch: 7000 - Loss: 0.0014108840841799974\n",
      "Epoch: 7500 - Loss: 0.0013592432951554656\n",
      "Epoch: 8000 - Loss: 0.0013124160468578339\n",
      "Epoch: 8500 - Loss: 0.0012724435655400157\n",
      "Epoch: 9000 - Loss: 0.001234722905792296\n",
      "Epoch: 9500 - Loss: 0.0011984332231804729\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.026536226272583\n",
      "Epoch: 500 - Loss: 0.0015319745289161801\n",
      "Epoch: 1000 - Loss: 0.0006346782902255654\n",
      "Epoch: 1500 - Loss: 0.00043399978312663734\n",
      "Epoch: 2000 - Loss: 0.0003526027430780232\n",
      "Epoch: 2500 - Loss: 0.00030294741736724973\n",
      "Epoch: 3000 - Loss: 0.0002717589377425611\n",
      "Epoch: 3500 - Loss: 0.00025096346507780254\n",
      "Epoch: 4000 - Loss: 0.0002350400754949078\n",
      "Epoch: 4500 - Loss: 0.00021947639470454305\n",
      "Epoch: 5000 - Loss: 0.00020609225612133741\n",
      "Epoch: 5500 - Loss: 0.00019704796432051808\n",
      "Epoch: 6000 - Loss: 0.0001898395421449095\n",
      "Epoch: 6500 - Loss: 0.00018357843509875238\n",
      "Epoch: 7000 - Loss: 0.00017775617016013712\n",
      "Epoch: 7500 - Loss: 0.00017242913600057364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8000 - Loss: 0.0001671220234129578\n",
      "Epoch: 8500 - Loss: 0.00016317982226610184\n",
      "Epoch: 9000 - Loss: 0.00015554109995719045\n",
      "Epoch: 9500 - Loss: 0.00015125959180295467\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.8531501889228821\n",
      "Epoch: 500 - Loss: 0.00059879309264943\n",
      "Epoch: 1000 - Loss: 0.0002901901607401669\n",
      "Epoch: 1500 - Loss: 0.00022010054090060294\n",
      "Epoch: 2000 - Loss: 0.00018356699729338288\n",
      "Epoch: 2500 - Loss: 0.00015853589866310358\n",
      "Epoch: 3000 - Loss: 0.00013910354755353183\n",
      "Epoch: 3500 - Loss: 0.00012484478065744042\n",
      "Epoch: 4000 - Loss: 0.0001144433772424236\n",
      "Epoch: 4500 - Loss: 0.00010759544238680974\n",
      "Epoch: 5000 - Loss: 0.00010252924403175712\n",
      "Epoch: 5500 - Loss: 9.678510832600296e-05\n",
      "Epoch: 6000 - Loss: 9.296691132476553e-05\n",
      "Epoch: 6500 - Loss: 8.947925380198285e-05\n",
      "Epoch: 7000 - Loss: 8.33880330901593e-05\n",
      "Epoch: 7500 - Loss: 7.891356653999537e-05\n",
      "Epoch: 8000 - Loss: 7.499411003664136e-05\n",
      "Epoch: 8500 - Loss: 7.106512202881277e-05\n",
      "Epoch: 9000 - Loss: 6.763033889001235e-05\n",
      "Epoch: 9500 - Loss: 6.533015402965248e-05\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.9373425841331482\n",
      "Epoch: 500 - Loss: 0.00016725834575481713\n",
      "Epoch: 1000 - Loss: 9.483956091571599e-05\n",
      "Epoch: 1500 - Loss: 7.364716293523088e-05\n",
      "Epoch: 2000 - Loss: 6.171032146085054e-05\n",
      "Epoch: 2500 - Loss: 5.31609111931175e-05\n",
      "Epoch: 3000 - Loss: 4.8048910684883595e-05\n",
      "Epoch: 3500 - Loss: 4.415890361997299e-05\n",
      "Epoch: 4000 - Loss: 4.056631223647855e-05\n",
      "Epoch: 4500 - Loss: 3.809727422776632e-05\n",
      "Epoch: 5000 - Loss: 3.6458099202718586e-05\n",
      "Epoch: 5500 - Loss: 3.560456752893515e-05\n",
      "Epoch: 6000 - Loss: 3.482628017081879e-05\n",
      "Epoch: 6500 - Loss: 3.421623114263639e-05\n",
      "Epoch: 7000 - Loss: 3.3957298001041636e-05\n",
      "Epoch: 7500 - Loss: 3.345434015500359e-05\n",
      "Epoch: 8000 - Loss: 3.292479959782213e-05\n",
      "Epoch: 8500 - Loss: 3.222798113711178e-05\n",
      "Epoch: 9000 - Loss: 3.174867742927745e-05\n",
      "Epoch: 9500 - Loss: 3.119722168776207e-05\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.6216921806335449\n",
      "Epoch: 500 - Loss: 0.0008028663578443229\n",
      "Epoch: 1000 - Loss: 0.0003555815783329308\n",
      "Epoch: 1500 - Loss: 0.0002488904574420303\n",
      "Epoch: 2000 - Loss: 0.00020049426530022174\n",
      "Epoch: 2500 - Loss: 0.0001765878841979429\n",
      "Epoch: 3000 - Loss: 0.00016091851284727454\n",
      "Epoch: 3500 - Loss: 0.00014964908768888563\n",
      "Epoch: 4000 - Loss: 0.00014177607954479754\n",
      "Epoch: 4500 - Loss: 0.00013392406981438398\n",
      "Epoch: 5000 - Loss: 0.0001260380377061665\n",
      "Epoch: 5500 - Loss: 0.00011969194747507572\n",
      "Epoch: 6000 - Loss: 0.00011456110223662108\n",
      "Epoch: 6500 - Loss: 0.00010908602416748181\n",
      "Epoch: 7000 - Loss: 0.00010374952398706228\n",
      "Epoch: 7500 - Loss: 0.00010086200927617028\n",
      "Epoch: 8000 - Loss: 9.87722523859702e-05\n",
      "Epoch: 8500 - Loss: 9.709437290439382e-05\n",
      "Epoch: 9000 - Loss: 9.55259456532076e-05\n",
      "Epoch: 9500 - Loss: 9.42340338951908e-05\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.3897671699523926\n",
      "Epoch: 500 - Loss: 0.002470207866281271\n",
      "Epoch: 1000 - Loss: 0.0009574804571457207\n",
      "Epoch: 1500 - Loss: 0.0006238971836864948\n",
      "Epoch: 2000 - Loss: 0.0004831060650758445\n",
      "Epoch: 2500 - Loss: 0.00040687675937078893\n",
      "Epoch: 3000 - Loss: 0.00036339915823191404\n",
      "Epoch: 3500 - Loss: 0.00033870863262563944\n",
      "Epoch: 4000 - Loss: 0.0003216107143089175\n",
      "Epoch: 4500 - Loss: 0.00030348976724781096\n",
      "Epoch: 5000 - Loss: 0.0002882202388718724\n",
      "Epoch: 5500 - Loss: 0.0002763376687653363\n",
      "Epoch: 6000 - Loss: 0.00026641497970558703\n",
      "Epoch: 6500 - Loss: 0.0002555779938120395\n",
      "Epoch: 7000 - Loss: 0.00024633886641822755\n",
      "Epoch: 7500 - Loss: 0.00023958939709700644\n",
      "Epoch: 8000 - Loss: 0.00023356739256996661\n",
      "Epoch: 8500 - Loss: 0.0002286411472596228\n",
      "Epoch: 9000 - Loss: 0.00022171359159983695\n",
      "Epoch: 9500 - Loss: 0.00021559040760621428\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.8425377011299133\n",
      "Epoch: 500 - Loss: 0.00042254748404957354\n",
      "Epoch: 1000 - Loss: 0.0002306476962985471\n",
      "Epoch: 1500 - Loss: 0.00018570426618680358\n",
      "Epoch: 2000 - Loss: 0.00016127566050272435\n",
      "Epoch: 2500 - Loss: 0.00014433733304031193\n",
      "Epoch: 3000 - Loss: 0.00013244443107396364\n",
      "Epoch: 3500 - Loss: 0.00012746310676448047\n",
      "Epoch: 4000 - Loss: 0.00012156216689618304\n",
      "Epoch: 4500 - Loss: 0.00011445719428593293\n",
      "Epoch: 5000 - Loss: 0.00011077718227170408\n",
      "Epoch: 5500 - Loss: 0.00010669398761820048\n",
      "Epoch: 6000 - Loss: 0.00010214932990493253\n",
      "Epoch: 6500 - Loss: 9.896294068312272e-05\n",
      "Epoch: 7000 - Loss: 9.602960926713422e-05\n",
      "Epoch: 7500 - Loss: 9.28527006180957e-05\n",
      "Epoch: 8000 - Loss: 8.978116966318339e-05\n",
      "Epoch: 8500 - Loss: 8.663502376293764e-05\n",
      "Epoch: 9000 - Loss: 8.359043567907065e-05\n",
      "Epoch: 9500 - Loss: 8.13315564300865e-05\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.9187240600585938\n",
      "Epoch: 500 - Loss: 0.00038418720941990614\n",
      "Epoch: 1000 - Loss: 0.00018983089830726385\n",
      "Epoch: 1500 - Loss: 0.0001332058454863727\n",
      "Epoch: 2000 - Loss: 0.0001035435197991319\n",
      "Epoch: 2500 - Loss: 8.710313704796135e-05\n",
      "Epoch: 3000 - Loss: 7.722104783169925e-05\n",
      "Epoch: 3500 - Loss: 6.836577085778117e-05\n",
      "Epoch: 4000 - Loss: 6.024622416589409e-05\n",
      "Epoch: 4500 - Loss: 5.539904304896481e-05\n",
      "Epoch: 5000 - Loss: 5.178738501854241e-05\n",
      "Epoch: 5500 - Loss: 4.814851126866415e-05\n",
      "Epoch: 6000 - Loss: 4.524814721662551e-05\n",
      "Epoch: 6500 - Loss: 4.2948111513396725e-05\n",
      "Epoch: 7000 - Loss: 4.094223913853057e-05\n",
      "Epoch: 7500 - Loss: 3.9424910937668756e-05\n",
      "Epoch: 8000 - Loss: 3.831227877526544e-05\n",
      "Epoch: 8500 - Loss: 3.713215119205415e-05\n",
      "Epoch: 9000 - Loss: 3.6108562198933214e-05\n",
      "Epoch: 9500 - Loss: 3.539066528901458e-05\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.9063105583190918\n",
      "Epoch: 500 - Loss: 0.0005224101478233933\n",
      "Epoch: 1000 - Loss: 0.00027244407101534307\n",
      "Epoch: 1500 - Loss: 0.0002161470038117841\n",
      "Epoch: 2000 - Loss: 0.00018832454225048423\n",
      "Epoch: 2500 - Loss: 0.00016874917491804808\n",
      "Epoch: 3000 - Loss: 0.00015640410128980875\n",
      "Epoch: 3500 - Loss: 0.0001464945380575955\n",
      "Epoch: 4000 - Loss: 0.00014044780982658267\n",
      "Epoch: 4500 - Loss: 0.00013428319653030485\n",
      "Epoch: 5000 - Loss: 0.0001309528888668865\n",
      "Epoch: 5500 - Loss: 0.0001274303940590471\n",
      "Epoch: 6000 - Loss: 0.0001234604133060202\n",
      "Epoch: 6500 - Loss: 0.00011947994062211365\n",
      "Epoch: 7000 - Loss: 0.00011322050704620779\n",
      "Epoch: 7500 - Loss: 0.0001085586627596058\n",
      "Epoch: 8000 - Loss: 0.0001052192019415088\n",
      "Epoch: 8500 - Loss: 0.00010227754683000967\n",
      "Epoch: 9000 - Loss: 0.0001000875054160133\n",
      "Epoch: 9500 - Loss: 9.825605957303196e-05\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.2471717596054077\n",
      "Epoch: 500 - Loss: 0.0002953045186586678\n",
      "Epoch: 1000 - Loss: 0.0001402974739903584\n",
      "Epoch: 1500 - Loss: 0.00010325575567549095\n",
      "Epoch: 2000 - Loss: 8.217884169425815e-05\n",
      "Epoch: 2500 - Loss: 6.999844481470063e-05\n",
      "Epoch: 3000 - Loss: 5.9966856497339904e-05\n",
      "Epoch: 3500 - Loss: 5.337564289220609e-05\n",
      "Epoch: 4000 - Loss: 4.7249584895325825e-05\n",
      "Epoch: 4500 - Loss: 4.2953892261721194e-05\n",
      "Epoch: 5000 - Loss: 3.9889433537609875e-05\n",
      "Epoch: 5500 - Loss: 3.7802245060447603e-05\n",
      "Epoch: 6000 - Loss: 3.584384103305638e-05\n",
      "Epoch: 6500 - Loss: 3.41907943948172e-05\n",
      "Epoch: 7000 - Loss: 3.2551812182646245e-05\n",
      "Epoch: 7500 - Loss: 3.119834582321346e-05\n",
      "Epoch: 8000 - Loss: 3.0395898647839203e-05\n",
      "Epoch: 8500 - Loss: 2.9874148822273128e-05\n",
      "Epoch: 9000 - Loss: 2.9307009754120372e-05\n",
      "Epoch: 9500 - Loss: 2.891753138101194e-05\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.8749516010284424\n",
      "Epoch: 500 - Loss: 0.00012689702271018177\n",
      "Epoch: 1000 - Loss: 8.093629003269598e-05\n",
      "Epoch: 1500 - Loss: 6.110747199272737e-05\n",
      "Epoch: 2000 - Loss: 5.320613126968965e-05\n",
      "Epoch: 2500 - Loss: 4.789210288436152e-05\n",
      "Epoch: 3000 - Loss: 4.3537598685361445e-05\n",
      "Epoch: 3500 - Loss: 4.034834273625165e-05\n",
      "Epoch: 4000 - Loss: 3.8518071960425004e-05\n",
      "Epoch: 4500 - Loss: 3.690675657708198e-05\n",
      "Epoch: 5000 - Loss: 3.582951467251405e-05\n",
      "Epoch: 5500 - Loss: 3.4694279747782275e-05\n",
      "Epoch: 6000 - Loss: 3.353974534547888e-05\n",
      "Epoch: 6500 - Loss: 3.2558389648329467e-05\n",
      "Epoch: 7000 - Loss: 3.1768973713042215e-05\n",
      "Epoch: 7500 - Loss: 3.1122719519771636e-05\n",
      "Epoch: 8000 - Loss: 3.0777897336520255e-05\n",
      "Epoch: 8500 - Loss: 3.059993832721375e-05\n",
      "Epoch: 9000 - Loss: 3.0326256819535047e-05\n",
      "Epoch: 9500 - Loss: 2.990997199958656e-05\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.5509413480758667\n",
      "Epoch: 500 - Loss: 0.013501920737326145\n",
      "Epoch: 1000 - Loss: 0.005133660975843668\n",
      "Epoch: 1500 - Loss: 0.0030790800228714943\n",
      "Epoch: 2000 - Loss: 0.002230624668300152\n",
      "Epoch: 2500 - Loss: 0.0017943711718544364\n",
      "Epoch: 3000 - Loss: 0.0015166286611929536\n",
      "Epoch: 3500 - Loss: 0.0013366473140195012\n",
      "Epoch: 4000 - Loss: 0.0012047593481838703\n",
      "Epoch: 4500 - Loss: 0.0011130948550999165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000 - Loss: 0.001041990239173174\n",
      "Epoch: 5500 - Loss: 0.0009878257988020778\n",
      "Epoch: 6000 - Loss: 0.0009395949309691787\n",
      "Epoch: 6500 - Loss: 0.0008944746223278344\n",
      "Epoch: 7000 - Loss: 0.0008586410549469292\n",
      "Epoch: 7500 - Loss: 0.0008256711298599839\n",
      "Epoch: 8000 - Loss: 0.0007983606192283332\n",
      "Epoch: 8500 - Loss: 0.0007792897522449493\n",
      "Epoch: 9000 - Loss: 0.0007624689606018364\n",
      "Epoch: 9500 - Loss: 0.0007424359791912138\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.1870461702346802\n",
      "Epoch: 500 - Loss: 0.00029590370832011104\n",
      "Epoch: 1000 - Loss: 0.00013609585585072637\n",
      "Epoch: 1500 - Loss: 9.72212728811428e-05\n",
      "Epoch: 2000 - Loss: 8.037229417823255e-05\n",
      "Epoch: 2500 - Loss: 6.777436647098511e-05\n",
      "Epoch: 3000 - Loss: 5.841271558892913e-05\n",
      "Epoch: 3500 - Loss: 4.936654295306653e-05\n",
      "Epoch: 4000 - Loss: 4.47673664893955e-05\n",
      "Epoch: 4500 - Loss: 4.1625607991591096e-05\n",
      "Epoch: 5000 - Loss: 3.9558170101372525e-05\n",
      "Epoch: 5500 - Loss: 3.827494219876826e-05\n",
      "Epoch: 6000 - Loss: 3.707283394760452e-05\n",
      "Epoch: 6500 - Loss: 3.605832898756489e-05\n",
      "Epoch: 7000 - Loss: 3.511778049869463e-05\n",
      "Epoch: 7500 - Loss: 3.355370063218288e-05\n",
      "Epoch: 8000 - Loss: 3.175923120579682e-05\n",
      "Epoch: 8500 - Loss: 3.084056152147241e-05\n",
      "Epoch: 9000 - Loss: 3.0159708330756985e-05\n",
      "Epoch: 9500 - Loss: 2.9433449526550248e-05\n"
     ]
    }
   ],
   "source": [
    "pr, re = [[],[],[]], [[],[],[]]\n",
    "ndcgs = defaultdict(list)\n",
    "\n",
    "lr = 0.003\n",
    "epochs = 10000\n",
    "constraintHigh=1\n",
    "constraintLow=0\n",
    "# constraintHigh=float('inf')\n",
    "# constraintLow=-float('inf')\n",
    "loss_weight = [1, 0, 1]\n",
    "L1, L2 = 0, 0\n",
    "rand_type = 0\n",
    "\n",
    "total_mul = 1\n",
    "\n",
    "for uid, uemb in enumerate(tqdm(doc_embs[:50])):\n",
    "    x = word_embs.T\n",
    "    y = uemb\n",
    "    total = len(doc_answers[uid])\n",
    "    total = 1\n",
    "    \n",
    "    torch_model = PyTorchLinearRegression(x.shape[1], lr, constraintHigh, constraintLow, int(total*total_mul), rand_type, L1, L2)\n",
    "    torch_model.fit(x, y, epochs)\n",
    "    \n",
    "    m1 = metric4(torch_model.w.detach().numpy(), doc_answers[uid], w_idx=None, topk=50, verbose=0)\n",
    "    m2 = metric4(torch_model.w.detach().numpy(), doc_answers[uid], w_idx=None, topk=100, verbose=0)\n",
    "    m3 = metric4(torch_model.w.detach().numpy(), doc_answers[uid], w_idx=None, topk=200, verbose=0)\n",
    "    ndcg1 = metric_ndcg2(torch_model.w.detach().numpy(), tfidf_ans, uid, topk=50, verbose=0)\n",
    "    ndcg2 = metric_ndcg2(torch_model.w.detach().numpy(), tfidf_ans, uid, topk=100, verbose=0)\n",
    "    ndcg3 = metric_ndcg2(torch_model.w.detach().numpy(), tfidf_ans, uid, topk=200, verbose=0)\n",
    "    ndcg4 = metric_ndcg2(torch_model.w.detach().numpy(), tfidf_ans, uid, topk=None, verbose=0)\n",
    "    pr[0].append(m1[\"precision\"])\n",
    "    re[0].append(m1[\"recall\"])\n",
    "    pr[1].append(m2[\"precision\"])\n",
    "    re[1].append(m2[\"recall\"])\n",
    "    pr[2].append(m3[\"precision\"])\n",
    "    re[2].append(m3[\"recall\"])\n",
    "    \n",
    "    ndcgs[\"50\"].append(ndcg1)\n",
    "    ndcgs[\"100\"].append(ndcg2)\n",
    "    ndcgs[\"200\"].append(ndcg3)\n",
    "    ndcgs[\"-1\"].append(ndcg4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4518c2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.2004 Recall:0.4375\n",
      "Precision:0.1300 Recall:0.5277\n",
      "Precision:0.0844 Recall:0.6365\n",
      "NDCG 50:0.6045\n",
      "NDCG 100:0.6312\n",
      "NDCG 200:0.6653\n",
      "NDCG all:0.7505\n"
     ]
    }
   ],
   "source": [
    "print(f\"Precision:{np.mean(pr[0]):.4f} Recall:{np.mean(re[0]):.4f}\")\n",
    "print(f\"Precision:{np.mean(pr[1]):.4f} Recall:{np.mean(re[1]):.4f}\")\n",
    "print(f\"Precision:{np.mean(pr[2]):.4f} Recall:{np.mean(re[2]):.4f}\")\n",
    "print(f\"NDCG 50:{np.mean(ndcgs['50']):.4f}\")\n",
    "print(f\"NDCG 100:{np.mean(ndcgs['100']):.4f}\")\n",
    "print(f\"NDCG 200:{np.mean(ndcgs['200']):.4f}\")\n",
    "print(f\"NDCG all:{np.mean(ndcgs['-1']):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "026e5545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.1165862083435059\n",
      "Epoch: 50 - Loss: 0.1827160269021988\n",
      "Epoch: 100 - Loss: 0.10726621747016907\n",
      "Epoch: 150 - Loss: 0.07198724895715714\n",
      "Epoch: 200 - Loss: 0.052256956696510315\n",
      "Epoch: 250 - Loss: 0.03986570984125137\n",
      "Epoch: 300 - Loss: 0.031511615961790085\n",
      "Epoch: 350 - Loss: 0.025626081973314285\n",
      "Epoch: 400 - Loss: 0.021288728341460228\n",
      "Epoch: 450 - Loss: 0.017993472516536713\n",
      "Epoch: 500 - Loss: 0.015447573736310005\n",
      "Epoch: 550 - Loss: 0.013419730588793755\n",
      "Epoch: 600 - Loss: 0.01178539264947176\n",
      "Epoch: 650 - Loss: 0.010446067899465561\n",
      "Epoch: 700 - Loss: 0.009328894317150116\n",
      "Epoch: 750 - Loss: 0.008385752327740192\n",
      "Epoch: 800 - Loss: 0.007579994387924671\n",
      "Epoch: 850 - Loss: 0.006889311596751213\n",
      "Epoch: 900 - Loss: 0.006292302627116442\n",
      "Epoch: 950 - Loss: 0.005773392505943775\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.0777878761291504\n",
      "Epoch: 50 - Loss: 0.03163642808794975\n",
      "Epoch: 100 - Loss: 0.015163120813667774\n",
      "Epoch: 150 - Loss: 0.009008600376546383\n",
      "Epoch: 200 - Loss: 0.005963977426290512\n",
      "Epoch: 250 - Loss: 0.004215116612613201\n",
      "Epoch: 300 - Loss: 0.003117712214589119\n",
      "Epoch: 350 - Loss: 0.002384400460869074\n",
      "Epoch: 400 - Loss: 0.0018739744555205107\n",
      "Epoch: 450 - Loss: 0.001512390561401844\n",
      "Epoch: 500 - Loss: 0.0012481700396165252\n",
      "Epoch: 550 - Loss: 0.0010500098578631878\n",
      "Epoch: 600 - Loss: 0.0008974197553470731\n",
      "Epoch: 650 - Loss: 0.0007776707643643022\n",
      "Epoch: 700 - Loss: 0.0006827791221439838\n",
      "Epoch: 750 - Loss: 0.0006064720219001174\n",
      "Epoch: 800 - Loss: 0.00054322206415236\n",
      "Epoch: 850 - Loss: 0.000490339589305222\n",
      "Epoch: 900 - Loss: 0.0004465550882741809\n",
      "Epoch: 950 - Loss: 0.0004096156044397503\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.9535503387451172\n",
      "Epoch: 50 - Loss: 0.09071342647075653\n",
      "Epoch: 100 - Loss: 0.05220615491271019\n",
      "Epoch: 150 - Loss: 0.03506401553750038\n",
      "Epoch: 200 - Loss: 0.02546084113419056\n",
      "Epoch: 250 - Loss: 0.01949138380587101\n",
      "Epoch: 300 - Loss: 0.015469861216843128\n",
      "Epoch: 350 - Loss: 0.01262041088193655\n",
      "Epoch: 400 - Loss: 0.010549765080213547\n",
      "Epoch: 450 - Loss: 0.008972536772489548\n",
      "Epoch: 500 - Loss: 0.007741653826087713\n",
      "Epoch: 550 - Loss: 0.006765436381101608\n",
      "Epoch: 600 - Loss: 0.005977565422654152\n",
      "Epoch: 650 - Loss: 0.005326948128640652\n",
      "Epoch: 700 - Loss: 0.00478124525398016\n",
      "Epoch: 750 - Loss: 0.0043192533776164055\n",
      "Epoch: 800 - Loss: 0.0039251758717000484\n",
      "Epoch: 850 - Loss: 0.0035872056614607573\n",
      "Epoch: 900 - Loss: 0.0032950076274573803\n",
      "Epoch: 950 - Loss: 0.0030397740192711353\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.8551814556121826\n",
      "Epoch: 50 - Loss: 0.05479184165596962\n",
      "Epoch: 100 - Loss: 0.029770618304610252\n",
      "Epoch: 150 - Loss: 0.019505782052874565\n",
      "Epoch: 200 - Loss: 0.013913963921368122\n",
      "Epoch: 250 - Loss: 0.010489548556506634\n",
      "Epoch: 300 - Loss: 0.008244004100561142\n",
      "Epoch: 350 - Loss: 0.006690690293908119\n",
      "Epoch: 400 - Loss: 0.005554400850087404\n",
      "Epoch: 450 - Loss: 0.004694350995123386\n",
      "Epoch: 500 - Loss: 0.004026743117719889\n",
      "Epoch: 550 - Loss: 0.003496396355330944\n",
      "Epoch: 600 - Loss: 0.003069356782361865\n",
      "Epoch: 650 - Loss: 0.002717108465731144\n",
      "Epoch: 700 - Loss: 0.002424562582746148\n",
      "Epoch: 750 - Loss: 0.0021793765481561422\n",
      "Epoch: 800 - Loss: 0.0019738436676561832\n",
      "Epoch: 850 - Loss: 0.0017980380216613412\n",
      "Epoch: 900 - Loss: 0.001645329175516963\n",
      "Epoch: 950 - Loss: 0.0015121635515242815\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.1088433265686035\n",
      "Epoch: 50 - Loss: 0.10312871634960175\n",
      "Epoch: 100 - Loss: 0.06354867666959763\n",
      "Epoch: 150 - Loss: 0.045348867774009705\n",
      "Epoch: 200 - Loss: 0.03471419960260391\n",
      "Epoch: 250 - Loss: 0.02777467668056488\n",
      "Epoch: 300 - Loss: 0.02286238595843315\n",
      "Epoch: 350 - Loss: 0.019247980788350105\n",
      "Epoch: 400 - Loss: 0.016493968665599823\n",
      "Epoch: 450 - Loss: 0.014305684715509415\n",
      "Epoch: 500 - Loss: 0.012540510855615139\n",
      "Epoch: 550 - Loss: 0.011089836247265339\n",
      "Epoch: 600 - Loss: 0.009884728118777275\n",
      "Epoch: 650 - Loss: 0.008863297291100025\n",
      "Epoch: 700 - Loss: 0.007998942397534847\n",
      "Epoch: 750 - Loss: 0.007261563558131456\n",
      "Epoch: 800 - Loss: 0.006626129150390625\n",
      "Epoch: 850 - Loss: 0.006075324025005102\n",
      "Epoch: 900 - Loss: 0.005596480797976255\n",
      "Epoch: 950 - Loss: 0.005176797043532133\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.0367999076843262\n",
      "Epoch: 50 - Loss: 0.06593222916126251\n",
      "Epoch: 100 - Loss: 0.038673561066389084\n",
      "Epoch: 150 - Loss: 0.026578610762953758\n",
      "Epoch: 200 - Loss: 0.019702870398759842\n",
      "Epoch: 250 - Loss: 0.015321621671319008\n",
      "Epoch: 300 - Loss: 0.01228079479187727\n",
      "Epoch: 350 - Loss: 0.010101147927343845\n",
      "Epoch: 400 - Loss: 0.008453636430203915\n",
      "Epoch: 450 - Loss: 0.007173700723797083\n",
      "Epoch: 500 - Loss: 0.006154580041766167\n",
      "Epoch: 550 - Loss: 0.0053304568864405155\n",
      "Epoch: 600 - Loss: 0.004666347522288561\n",
      "Epoch: 650 - Loss: 0.004123760387301445\n",
      "Epoch: 700 - Loss: 0.003672390477731824\n",
      "Epoch: 750 - Loss: 0.0032926679123193026\n",
      "Epoch: 800 - Loss: 0.0029713124968111515\n",
      "Epoch: 850 - Loss: 0.0026952847838401794\n",
      "Epoch: 900 - Loss: 0.0024563639890402555\n",
      "Epoch: 950 - Loss: 0.0022488157264888287\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.0828191041946411\n",
      "Epoch: 50 - Loss: 0.06298636645078659\n",
      "Epoch: 100 - Loss: 0.03787338361144066\n",
      "Epoch: 150 - Loss: 0.026547875255346298\n",
      "Epoch: 200 - Loss: 0.020019443705677986\n",
      "Epoch: 250 - Loss: 0.01573677361011505\n",
      "Epoch: 300 - Loss: 0.012709032744169235\n",
      "Epoch: 350 - Loss: 0.01047742459923029\n",
      "Epoch: 400 - Loss: 0.008794649504125118\n",
      "Epoch: 450 - Loss: 0.007497915532439947\n",
      "Epoch: 500 - Loss: 0.006472117733210325\n",
      "Epoch: 550 - Loss: 0.005649143364280462\n",
      "Epoch: 600 - Loss: 0.004973076283931732\n",
      "Epoch: 650 - Loss: 0.004412690177559853\n",
      "Epoch: 700 - Loss: 0.0039443206042051315\n",
      "Epoch: 750 - Loss: 0.003549296176061034\n",
      "Epoch: 800 - Loss: 0.0032127229496836662\n",
      "Epoch: 850 - Loss: 0.0029258655849844217\n",
      "Epoch: 900 - Loss: 0.002679465338587761\n",
      "Epoch: 950 - Loss: 0.0024654085282236338\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.7859926819801331\n",
      "Epoch: 50 - Loss: 0.02542371302843094\n",
      "Epoch: 100 - Loss: 0.012973002158105373\n",
      "Epoch: 150 - Loss: 0.00810442678630352\n",
      "Epoch: 200 - Loss: 0.005588490515947342\n",
      "Epoch: 250 - Loss: 0.004112137481570244\n",
      "Epoch: 300 - Loss: 0.0031655842904001474\n",
      "Epoch: 350 - Loss: 0.0025245333090424538\n",
      "Epoch: 400 - Loss: 0.0020634024403989315\n",
      "Epoch: 450 - Loss: 0.001722045592032373\n",
      "Epoch: 500 - Loss: 0.0014619333669543266\n",
      "Epoch: 550 - Loss: 0.0012609129771590233\n",
      "Epoch: 600 - Loss: 0.0011021844111382961\n",
      "Epoch: 650 - Loss: 0.0009750769822858274\n",
      "Epoch: 700 - Loss: 0.0008714551222510636\n",
      "Epoch: 750 - Loss: 0.0007861528429202735\n",
      "Epoch: 800 - Loss: 0.0007146482821553946\n",
      "Epoch: 850 - Loss: 0.0006541348993778229\n",
      "Epoch: 900 - Loss: 0.000602071697358042\n",
      "Epoch: 950 - Loss: 0.0005568505730479956\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.7265727519989014\n",
      "Epoch: 50 - Loss: 0.024069182574748993\n",
      "Epoch: 100 - Loss: 0.012923039495944977\n",
      "Epoch: 150 - Loss: 0.008278902620077133\n",
      "Epoch: 200 - Loss: 0.005744661670178175\n",
      "Epoch: 250 - Loss: 0.00421726843342185\n",
      "Epoch: 300 - Loss: 0.003216295735910535\n",
      "Epoch: 350 - Loss: 0.0025349475909024477\n",
      "Epoch: 400 - Loss: 0.002042364561930299\n",
      "Epoch: 450 - Loss: 0.0016777084674686193\n",
      "Epoch: 500 - Loss: 0.0014038685476407409\n",
      "Epoch: 550 - Loss: 0.0011935861548408866\n",
      "Epoch: 600 - Loss: 0.0010284717427566648\n",
      "Epoch: 650 - Loss: 0.0008967038011178374\n",
      "Epoch: 700 - Loss: 0.0007903855293989182\n",
      "Epoch: 750 - Loss: 0.0007032863213680685\n",
      "Epoch: 800 - Loss: 0.0006310796597972512\n",
      "Epoch: 850 - Loss: 0.0005709867691621184\n",
      "Epoch: 900 - Loss: 0.0005200240993872285\n",
      "Epoch: 950 - Loss: 0.000476482673548162\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.8648017048835754\n",
      "Epoch: 50 - Loss: 0.027167601510882378\n",
      "Epoch: 100 - Loss: 0.015006273053586483\n",
      "Epoch: 150 - Loss: 0.010059798136353493\n",
      "Epoch: 200 - Loss: 0.007325715385377407\n",
      "Epoch: 250 - Loss: 0.005609874147921801\n",
      "Epoch: 300 - Loss: 0.004460071213543415\n",
      "Epoch: 350 - Loss: 0.0036369427107274532\n",
      "Epoch: 400 - Loss: 0.0030190718825906515\n",
      "Epoch: 450 - Loss: 0.002550675766542554\n",
      "Epoch: 500 - Loss: 0.0021872930228710175\n",
      "Epoch: 550 - Loss: 0.0018969192169606686\n",
      "Epoch: 600 - Loss: 0.0016611696919426322\n",
      "Epoch: 650 - Loss: 0.001468220492824912\n",
      "Epoch: 700 - Loss: 0.0013083918020129204\n",
      "Epoch: 750 - Loss: 0.0011754411971196532\n",
      "Epoch: 800 - Loss: 0.0010633408091962337\n",
      "Epoch: 850 - Loss: 0.0009671177831478417\n",
      "Epoch: 900 - Loss: 0.000884135952219367\n",
      "Epoch: 950 - Loss: 0.0008126539760269225\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 2.8218798637390137\n",
      "Epoch: 50 - Loss: 0.5275692343711853\n",
      "Epoch: 100 - Loss: 0.36178237199783325\n",
      "Epoch: 150 - Loss: 0.27510392665863037\n",
      "Epoch: 200 - Loss: 0.2196209877729416\n",
      "Epoch: 250 - Loss: 0.18080662190914154\n",
      "Epoch: 300 - Loss: 0.15232829749584198\n",
      "Epoch: 350 - Loss: 0.1306358277797699\n",
      "Epoch: 400 - Loss: 0.1139487773180008\n",
      "Epoch: 450 - Loss: 0.1006222665309906\n",
      "Epoch: 500 - Loss: 0.08967652171850204\n",
      "Epoch: 550 - Loss: 0.08060920983552933\n",
      "Epoch: 600 - Loss: 0.07293760776519775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 650 - Loss: 0.06637369096279144\n",
      "Epoch: 700 - Loss: 0.0606866255402565\n",
      "Epoch: 750 - Loss: 0.05571786314249039\n",
      "Epoch: 800 - Loss: 0.05135881528258324\n",
      "Epoch: 850 - Loss: 0.047489527612924576\n",
      "Epoch: 900 - Loss: 0.04401727765798569\n",
      "Epoch: 950 - Loss: 0.04090593755245209\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.9688442945480347\n",
      "Epoch: 50 - Loss: 0.030629010871052742\n",
      "Epoch: 100 - Loss: 0.014871841296553612\n",
      "Epoch: 150 - Loss: 0.009177191182971\n",
      "Epoch: 200 - Loss: 0.006267895922064781\n",
      "Epoch: 250 - Loss: 0.004544368013739586\n",
      "Epoch: 300 - Loss: 0.003437310690060258\n",
      "Epoch: 350 - Loss: 0.0026890579611063004\n",
      "Epoch: 400 - Loss: 0.0021603151690214872\n",
      "Epoch: 450 - Loss: 0.0017732952255755663\n",
      "Epoch: 500 - Loss: 0.001479952479712665\n",
      "Epoch: 550 - Loss: 0.0012549790553748608\n",
      "Epoch: 600 - Loss: 0.001077379216440022\n",
      "Epoch: 650 - Loss: 0.000934306182898581\n",
      "Epoch: 700 - Loss: 0.0008183493628166616\n",
      "Epoch: 750 - Loss: 0.0007239367696456611\n",
      "Epoch: 800 - Loss: 0.0006461957818828523\n",
      "Epoch: 850 - Loss: 0.0005812850431539118\n",
      "Epoch: 900 - Loss: 0.0005267653032205999\n",
      "Epoch: 950 - Loss: 0.00048018715460784733\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.7911754846572876\n",
      "Epoch: 50 - Loss: 0.02926374226808548\n",
      "Epoch: 100 - Loss: 0.015418695285916328\n",
      "Epoch: 150 - Loss: 0.009859353303909302\n",
      "Epoch: 200 - Loss: 0.006935039535164833\n",
      "Epoch: 250 - Loss: 0.00515866931527853\n",
      "Epoch: 300 - Loss: 0.003995245788246393\n",
      "Epoch: 350 - Loss: 0.0031895763240754604\n",
      "Epoch: 400 - Loss: 0.002610174473375082\n",
      "Epoch: 450 - Loss: 0.002177793998271227\n",
      "Epoch: 500 - Loss: 0.0018459048587828875\n",
      "Epoch: 550 - Loss: 0.001587968203239143\n",
      "Epoch: 600 - Loss: 0.0013813471887260675\n",
      "Epoch: 650 - Loss: 0.0012140802573412657\n",
      "Epoch: 700 - Loss: 0.0010767638450488448\n",
      "Epoch: 750 - Loss: 0.0009625531965866685\n",
      "Epoch: 800 - Loss: 0.0008668498485349119\n",
      "Epoch: 850 - Loss: 0.0007853282731957734\n",
      "Epoch: 900 - Loss: 0.0007154832710511982\n",
      "Epoch: 950 - Loss: 0.0006555060972459614\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.9527069330215454\n",
      "Epoch: 50 - Loss: 0.04333300516009331\n",
      "Epoch: 100 - Loss: 0.023263106122612953\n",
      "Epoch: 150 - Loss: 0.015084812417626381\n",
      "Epoch: 200 - Loss: 0.01070866733789444\n",
      "Epoch: 250 - Loss: 0.008019600063562393\n",
      "Epoch: 300 - Loss: 0.0062278569675982\n",
      "Epoch: 350 - Loss: 0.004974894225597382\n",
      "Epoch: 400 - Loss: 0.004069000016897917\n",
      "Epoch: 450 - Loss: 0.003388101700693369\n",
      "Epoch: 500 - Loss: 0.002855810569599271\n",
      "Epoch: 550 - Loss: 0.002436874434351921\n",
      "Epoch: 600 - Loss: 0.002105199033394456\n",
      "Epoch: 650 - Loss: 0.0018399120308458805\n",
      "Epoch: 700 - Loss: 0.001626088167540729\n",
      "Epoch: 750 - Loss: 0.0014508537715300918\n",
      "Epoch: 800 - Loss: 0.001304214121773839\n",
      "Epoch: 850 - Loss: 0.0011799861676990986\n",
      "Epoch: 900 - Loss: 0.001074446365237236\n",
      "Epoch: 950 - Loss: 0.0009842102881520987\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.126102089881897\n",
      "Epoch: 50 - Loss: 0.06620252877473831\n",
      "Epoch: 100 - Loss: 0.0386245958507061\n",
      "Epoch: 150 - Loss: 0.026802504435181618\n",
      "Epoch: 200 - Loss: 0.020012345165014267\n",
      "Epoch: 250 - Loss: 0.015643205493688583\n",
      "Epoch: 300 - Loss: 0.012584426440298557\n",
      "Epoch: 350 - Loss: 0.010360101237893105\n",
      "Epoch: 400 - Loss: 0.008673643693327904\n",
      "Epoch: 450 - Loss: 0.007369109429419041\n",
      "Epoch: 500 - Loss: 0.006341623142361641\n",
      "Epoch: 550 - Loss: 0.005523927044123411\n",
      "Epoch: 600 - Loss: 0.00485695106908679\n",
      "Epoch: 650 - Loss: 0.004304209724068642\n",
      "Epoch: 700 - Loss: 0.003838060423731804\n",
      "Epoch: 750 - Loss: 0.003447368973866105\n",
      "Epoch: 800 - Loss: 0.0031174817122519016\n",
      "Epoch: 850 - Loss: 0.002836422063410282\n",
      "Epoch: 900 - Loss: 0.0025939561892300844\n",
      "Epoch: 950 - Loss: 0.0023835524916648865\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.7855663895606995\n",
      "Epoch: 50 - Loss: 0.05339531600475311\n",
      "Epoch: 100 - Loss: 0.032147955149412155\n",
      "Epoch: 150 - Loss: 0.022625846788287163\n",
      "Epoch: 200 - Loss: 0.01711267977952957\n",
      "Epoch: 250 - Loss: 0.013491664081811905\n",
      "Epoch: 300 - Loss: 0.01093312632292509\n",
      "Epoch: 350 - Loss: 0.00907602533698082\n",
      "Epoch: 400 - Loss: 0.007666466291993856\n",
      "Epoch: 450 - Loss: 0.006567774806171656\n",
      "Epoch: 500 - Loss: 0.005697265267372131\n",
      "Epoch: 550 - Loss: 0.004995929542928934\n",
      "Epoch: 600 - Loss: 0.00441998103633523\n",
      "Epoch: 650 - Loss: 0.0039412640035152435\n",
      "Epoch: 700 - Loss: 0.0035376055166125298\n",
      "Epoch: 750 - Loss: 0.003193669253960252\n",
      "Epoch: 800 - Loss: 0.0028996379114687443\n",
      "Epoch: 850 - Loss: 0.0026448636781424284\n",
      "Epoch: 900 - Loss: 0.0024227919057011604\n",
      "Epoch: 950 - Loss: 0.0022278819233179092\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.7206735014915466\n",
      "Epoch: 50 - Loss: 0.04392179846763611\n",
      "Epoch: 100 - Loss: 0.02574482187628746\n",
      "Epoch: 150 - Loss: 0.017841476947069168\n",
      "Epoch: 200 - Loss: 0.013321361504495144\n",
      "Epoch: 250 - Loss: 0.010408281348645687\n",
      "Epoch: 300 - Loss: 0.008379709906876087\n",
      "Epoch: 350 - Loss: 0.006895420141518116\n",
      "Epoch: 400 - Loss: 0.00576274748891592\n",
      "Epoch: 450 - Loss: 0.004882932174950838\n",
      "Epoch: 500 - Loss: 0.0041871629655361176\n",
      "Epoch: 550 - Loss: 0.003629491664469242\n",
      "Epoch: 600 - Loss: 0.0031729922629892826\n",
      "Epoch: 650 - Loss: 0.002793109742924571\n",
      "Epoch: 700 - Loss: 0.002477018628269434\n",
      "Epoch: 750 - Loss: 0.002210981911048293\n",
      "Epoch: 800 - Loss: 0.0019862025510519743\n",
      "Epoch: 850 - Loss: 0.0017950581386685371\n",
      "Epoch: 900 - Loss: 0.0016316621331498027\n",
      "Epoch: 950 - Loss: 0.0014908132143318653\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.7433865070343018\n",
      "Epoch: 50 - Loss: 0.025435615330934525\n",
      "Epoch: 100 - Loss: 0.013093804940581322\n",
      "Epoch: 150 - Loss: 0.008236240595579147\n",
      "Epoch: 200 - Loss: 0.0056665861047804356\n",
      "Epoch: 250 - Loss: 0.004156137816607952\n",
      "Epoch: 300 - Loss: 0.0031740108970552683\n",
      "Epoch: 350 - Loss: 0.0025030067190527916\n",
      "Epoch: 400 - Loss: 0.0020244731567800045\n",
      "Epoch: 450 - Loss: 0.0016731016803532839\n",
      "Epoch: 500 - Loss: 0.0014068257296457887\n",
      "Epoch: 550 - Loss: 0.0012007977347820997\n",
      "Epoch: 600 - Loss: 0.0010393359698355198\n",
      "Epoch: 650 - Loss: 0.0009100323659367859\n",
      "Epoch: 700 - Loss: 0.0008044980932027102\n",
      "Epoch: 750 - Loss: 0.0007173690828494728\n",
      "Epoch: 800 - Loss: 0.0006449662032537162\n",
      "Epoch: 850 - Loss: 0.0005840835510753095\n",
      "Epoch: 900 - Loss: 0.0005326637765392661\n",
      "Epoch: 950 - Loss: 0.0004893220029771328\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.021339774131775\n",
      "Epoch: 50 - Loss: 0.07062774151563644\n",
      "Epoch: 100 - Loss: 0.043244294822216034\n",
      "Epoch: 150 - Loss: 0.031123261898756027\n",
      "Epoch: 200 - Loss: 0.023973189294338226\n",
      "Epoch: 250 - Loss: 0.019201815128326416\n",
      "Epoch: 300 - Loss: 0.015814252197742462\n",
      "Epoch: 350 - Loss: 0.013308924622833729\n",
      "Epoch: 400 - Loss: 0.011407822370529175\n",
      "Epoch: 450 - Loss: 0.009911629371345043\n",
      "Epoch: 500 - Loss: 0.00869729369878769\n",
      "Epoch: 550 - Loss: 0.007694766391068697\n",
      "Epoch: 600 - Loss: 0.006862931419163942\n",
      "Epoch: 650 - Loss: 0.006159271113574505\n",
      "Epoch: 700 - Loss: 0.005560252815485001\n",
      "Epoch: 750 - Loss: 0.005045298486948013\n",
      "Epoch: 800 - Loss: 0.004600584041327238\n",
      "Epoch: 850 - Loss: 0.004213128238916397\n",
      "Epoch: 900 - Loss: 0.00387133308686316\n",
      "Epoch: 950 - Loss: 0.003569620894268155\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.0943760871887207\n",
      "Epoch: 50 - Loss: 0.12368614971637726\n",
      "Epoch: 100 - Loss: 0.07834980636835098\n",
      "Epoch: 150 - Loss: 0.05760353431105614\n",
      "Epoch: 200 - Loss: 0.045233748853206635\n",
      "Epoch: 250 - Loss: 0.03696879744529724\n",
      "Epoch: 300 - Loss: 0.03100983425974846\n",
      "Epoch: 350 - Loss: 0.026539606973528862\n",
      "Epoch: 400 - Loss: 0.023031586781144142\n",
      "Epoch: 450 - Loss: 0.020263323560357094\n",
      "Epoch: 500 - Loss: 0.01800217479467392\n",
      "Epoch: 550 - Loss: 0.01610657386481762\n",
      "Epoch: 600 - Loss: 0.014518830925226212\n",
      "Epoch: 650 - Loss: 0.01316811516880989\n",
      "Epoch: 700 - Loss: 0.012005423195660114\n",
      "Epoch: 750 - Loss: 0.011000573635101318\n",
      "Epoch: 800 - Loss: 0.010126996785402298\n",
      "Epoch: 850 - Loss: 0.009363589808344841\n",
      "Epoch: 900 - Loss: 0.008690561167895794\n",
      "Epoch: 950 - Loss: 0.008088740520179272\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.7591738700866699\n",
      "Epoch: 50 - Loss: 0.03516002744436264\n",
      "Epoch: 100 - Loss: 0.01937081292271614\n",
      "Epoch: 150 - Loss: 0.012691829353570938\n",
      "Epoch: 200 - Loss: 0.009035781025886536\n",
      "Epoch: 250 - Loss: 0.006786382291465998\n",
      "Epoch: 300 - Loss: 0.005294359754770994\n",
      "Epoch: 350 - Loss: 0.004236304201185703\n",
      "Epoch: 400 - Loss: 0.0034580493811517954\n",
      "Epoch: 450 - Loss: 0.002870013704523444\n",
      "Epoch: 500 - Loss: 0.002417294541373849\n",
      "Epoch: 550 - Loss: 0.0020631756633520126\n",
      "Epoch: 600 - Loss: 0.0017827881965786219\n",
      "Epoch: 650 - Loss: 0.0015571394469588995\n",
      "Epoch: 700 - Loss: 0.0013734542299062014\n",
      "Epoch: 750 - Loss: 0.0012211318826302886\n",
      "Epoch: 800 - Loss: 0.0010932437144219875\n",
      "Epoch: 850 - Loss: 0.000984878744930029\n",
      "Epoch: 900 - Loss: 0.0008927094750106335\n",
      "Epoch: 950 - Loss: 0.0008139417041093111\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.0461411476135254\n",
      "Epoch: 50 - Loss: 0.050347376614809036\n",
      "Epoch: 100 - Loss: 0.02755223773419857\n",
      "Epoch: 150 - Loss: 0.018457140773534775\n",
      "Epoch: 200 - Loss: 0.013481800444424152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 250 - Loss: 0.010404424741864204\n",
      "Epoch: 300 - Loss: 0.008330289274454117\n",
      "Epoch: 350 - Loss: 0.006847130134701729\n",
      "Epoch: 400 - Loss: 0.005737720523029566\n",
      "Epoch: 450 - Loss: 0.004869015421718359\n",
      "Epoch: 500 - Loss: 0.00418343860656023\n",
      "Epoch: 550 - Loss: 0.003640058683231473\n",
      "Epoch: 600 - Loss: 0.0031989484559744596\n",
      "Epoch: 650 - Loss: 0.00283500156365335\n",
      "Epoch: 700 - Loss: 0.0025321990251541138\n",
      "Epoch: 750 - Loss: 0.0022747027687728405\n",
      "Epoch: 800 - Loss: 0.0020563602447509766\n",
      "Epoch: 850 - Loss: 0.0018688795389607549\n",
      "Epoch: 900 - Loss: 0.0017071333713829517\n",
      "Epoch: 950 - Loss: 0.0015664357924833894\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.0849751234054565\n",
      "Epoch: 50 - Loss: 0.01902751624584198\n",
      "Epoch: 100 - Loss: 0.00917135737836361\n",
      "Epoch: 150 - Loss: 0.005516348406672478\n",
      "Epoch: 200 - Loss: 0.003689889330416918\n",
      "Epoch: 250 - Loss: 0.002622753381729126\n",
      "Epoch: 300 - Loss: 0.0019506625831127167\n",
      "Epoch: 350 - Loss: 0.0015075516421347857\n",
      "Epoch: 400 - Loss: 0.0012018424458801746\n",
      "Epoch: 450 - Loss: 0.000978809199295938\n",
      "Epoch: 500 - Loss: 0.0008127837791107595\n",
      "Epoch: 550 - Loss: 0.00068709219340235\n",
      "Epoch: 600 - Loss: 0.0005898906965740025\n",
      "Epoch: 650 - Loss: 0.0005135249812155962\n",
      "Epoch: 700 - Loss: 0.000452508800663054\n",
      "Epoch: 750 - Loss: 0.000403005862608552\n",
      "Epoch: 800 - Loss: 0.0003623561642598361\n",
      "Epoch: 850 - Loss: 0.0003288557636551559\n",
      "Epoch: 900 - Loss: 0.00030092967790551484\n",
      "Epoch: 950 - Loss: 0.00027752717141993344\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.6508102416992188\n",
      "Epoch: 50 - Loss: 0.04880313575267792\n",
      "Epoch: 100 - Loss: 0.026018939912319183\n",
      "Epoch: 150 - Loss: 0.01692224107682705\n",
      "Epoch: 200 - Loss: 0.012028095312416553\n",
      "Epoch: 250 - Loss: 0.009017547592520714\n",
      "Epoch: 300 - Loss: 0.007026355247944593\n",
      "Epoch: 350 - Loss: 0.005629267077893019\n",
      "Epoch: 400 - Loss: 0.00460108183324337\n",
      "Epoch: 450 - Loss: 0.003827559296041727\n",
      "Epoch: 500 - Loss: 0.0032302404288202524\n",
      "Epoch: 550 - Loss: 0.0027596948202699423\n",
      "Epoch: 600 - Loss: 0.002382870065048337\n",
      "Epoch: 650 - Loss: 0.0020818107295781374\n",
      "Epoch: 700 - Loss: 0.0018357383087277412\n",
      "Epoch: 750 - Loss: 0.0016316329129040241\n",
      "Epoch: 800 - Loss: 0.0014604090247303247\n",
      "Epoch: 850 - Loss: 0.0013163394760340452\n",
      "Epoch: 900 - Loss: 0.0011937962844967842\n",
      "Epoch: 950 - Loss: 0.0010888815158978105\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.9357651472091675\n",
      "Epoch: 50 - Loss: 0.09799576550722122\n",
      "Epoch: 100 - Loss: 0.05767284706234932\n",
      "Epoch: 150 - Loss: 0.03969208896160126\n",
      "Epoch: 200 - Loss: 0.029337234795093536\n",
      "Epoch: 250 - Loss: 0.022706618532538414\n",
      "Epoch: 300 - Loss: 0.018141672015190125\n",
      "Epoch: 350 - Loss: 0.014823228120803833\n",
      "Epoch: 400 - Loss: 0.012351959012448788\n",
      "Epoch: 450 - Loss: 0.010447314009070396\n",
      "Epoch: 500 - Loss: 0.008952006697654724\n",
      "Epoch: 550 - Loss: 0.007757518440485001\n",
      "Epoch: 600 - Loss: 0.006786775775253773\n",
      "Epoch: 650 - Loss: 0.00598431471735239\n",
      "Epoch: 700 - Loss: 0.005313902627676725\n",
      "Epoch: 750 - Loss: 0.004749399144202471\n",
      "Epoch: 800 - Loss: 0.004269914701581001\n",
      "Epoch: 850 - Loss: 0.0038590184412896633\n",
      "Epoch: 900 - Loss: 0.003508062567561865\n",
      "Epoch: 950 - Loss: 0.003203571541234851\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.6799862384796143\n",
      "Epoch: 50 - Loss: 0.11900076270103455\n",
      "Epoch: 100 - Loss: 0.06274033337831497\n",
      "Epoch: 150 - Loss: 0.040636930614709854\n",
      "Epoch: 200 - Loss: 0.02917870506644249\n",
      "Epoch: 250 - Loss: 0.022236628457903862\n",
      "Epoch: 300 - Loss: 0.017667079344391823\n",
      "Epoch: 350 - Loss: 0.014432316645979881\n",
      "Epoch: 400 - Loss: 0.012032009661197662\n",
      "Epoch: 450 - Loss: 0.01021275669336319\n",
      "Epoch: 500 - Loss: 0.008803349919617176\n",
      "Epoch: 550 - Loss: 0.00769464485347271\n",
      "Epoch: 600 - Loss: 0.006807980593293905\n",
      "Epoch: 650 - Loss: 0.006081278435885906\n",
      "Epoch: 700 - Loss: 0.005474605597555637\n",
      "Epoch: 750 - Loss: 0.004964716732501984\n",
      "Epoch: 800 - Loss: 0.004526718985289335\n",
      "Epoch: 850 - Loss: 0.004148317966610193\n",
      "Epoch: 900 - Loss: 0.003820224432274699\n",
      "Epoch: 950 - Loss: 0.0035320960450917482\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.1221957206726074\n",
      "Epoch: 50 - Loss: 0.09354348480701447\n",
      "Epoch: 100 - Loss: 0.058304499834775925\n",
      "Epoch: 150 - Loss: 0.04169831797480583\n",
      "Epoch: 200 - Loss: 0.031759824603796005\n",
      "Epoch: 250 - Loss: 0.025252986699342728\n",
      "Epoch: 300 - Loss: 0.02068129926919937\n",
      "Epoch: 350 - Loss: 0.017331378534436226\n",
      "Epoch: 400 - Loss: 0.014771057292819023\n",
      "Epoch: 450 - Loss: 0.012779920361936092\n",
      "Epoch: 500 - Loss: 0.011179829016327858\n",
      "Epoch: 550 - Loss: 0.009876281023025513\n",
      "Epoch: 600 - Loss: 0.008803789503872395\n",
      "Epoch: 650 - Loss: 0.007905484177172184\n",
      "Epoch: 700 - Loss: 0.007143635302782059\n",
      "Epoch: 750 - Loss: 0.006491992622613907\n",
      "Epoch: 800 - Loss: 0.005930068902671337\n",
      "Epoch: 850 - Loss: 0.005444159731268883\n",
      "Epoch: 900 - Loss: 0.005020222160965204\n",
      "Epoch: 950 - Loss: 0.004645651206374168\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.2331901788711548\n",
      "Epoch: 50 - Loss: 0.02995762601494789\n",
      "Epoch: 100 - Loss: 0.014322109520435333\n",
      "Epoch: 150 - Loss: 0.008762276731431484\n",
      "Epoch: 200 - Loss: 0.006022776942700148\n",
      "Epoch: 250 - Loss: 0.004441071301698685\n",
      "Epoch: 300 - Loss: 0.003438241546973586\n",
      "Epoch: 350 - Loss: 0.0027619919274002314\n",
      "Epoch: 400 - Loss: 0.0022780345752835274\n",
      "Epoch: 450 - Loss: 0.0019142829114571214\n",
      "Epoch: 500 - Loss: 0.0016382519388571382\n",
      "Epoch: 550 - Loss: 0.001420273445546627\n",
      "Epoch: 600 - Loss: 0.0012446525506675243\n",
      "Epoch: 650 - Loss: 0.0011015870841220021\n",
      "Epoch: 700 - Loss: 0.000982756493613124\n",
      "Epoch: 750 - Loss: 0.0008836269262246788\n",
      "Epoch: 800 - Loss: 0.0008018153021112084\n",
      "Epoch: 850 - Loss: 0.0007333479006774724\n",
      "Epoch: 900 - Loss: 0.0006751015316694975\n",
      "Epoch: 950 - Loss: 0.0006252315361052752\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.3011581897735596\n",
      "Epoch: 50 - Loss: 0.14060258865356445\n",
      "Epoch: 100 - Loss: 0.08541637659072876\n",
      "Epoch: 150 - Loss: 0.06104067713022232\n",
      "Epoch: 200 - Loss: 0.04697715863585472\n",
      "Epoch: 250 - Loss: 0.03783543035387993\n",
      "Epoch: 300 - Loss: 0.03147067874670029\n",
      "Epoch: 350 - Loss: 0.026775753125548363\n",
      "Epoch: 400 - Loss: 0.0231665950268507\n",
      "Epoch: 450 - Loss: 0.02029349096119404\n",
      "Epoch: 500 - Loss: 0.017959661781787872\n",
      "Epoch: 550 - Loss: 0.016028666868805885\n",
      "Epoch: 600 - Loss: 0.014414150267839432\n",
      "Epoch: 650 - Loss: 0.013053661212325096\n",
      "Epoch: 700 - Loss: 0.011888043954968452\n",
      "Epoch: 750 - Loss: 0.010880081914365292\n",
      "Epoch: 800 - Loss: 0.010008057579398155\n",
      "Epoch: 850 - Loss: 0.009246676228940487\n",
      "Epoch: 900 - Loss: 0.008581256493926048\n",
      "Epoch: 950 - Loss: 0.007992061786353588\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.9767674803733826\n",
      "Epoch: 50 - Loss: 0.10899379849433899\n",
      "Epoch: 100 - Loss: 0.06307536363601685\n",
      "Epoch: 150 - Loss: 0.04313086345791817\n",
      "Epoch: 200 - Loss: 0.031986452639102936\n",
      "Epoch: 250 - Loss: 0.02489430457353592\n",
      "Epoch: 300 - Loss: 0.020111083984375\n",
      "Epoch: 350 - Loss: 0.01667199656367302\n",
      "Epoch: 400 - Loss: 0.014100058004260063\n",
      "Epoch: 450 - Loss: 0.01210095640271902\n",
      "Epoch: 500 - Loss: 0.010508142411708832\n",
      "Epoch: 550 - Loss: 0.009216606616973877\n",
      "Epoch: 600 - Loss: 0.00815555825829506\n",
      "Epoch: 650 - Loss: 0.0072727263905107975\n",
      "Epoch: 700 - Loss: 0.006531643681228161\n",
      "Epoch: 750 - Loss: 0.0058984579518437386\n",
      "Epoch: 800 - Loss: 0.005356152076274157\n",
      "Epoch: 850 - Loss: 0.004889058880507946\n",
      "Epoch: 900 - Loss: 0.004484627395868301\n",
      "Epoch: 950 - Loss: 0.004130367189645767\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.1682430505752563\n",
      "Epoch: 50 - Loss: 0.0911167562007904\n",
      "Epoch: 100 - Loss: 0.051090069115161896\n",
      "Epoch: 150 - Loss: 0.03433811292052269\n",
      "Epoch: 200 - Loss: 0.025190429762005806\n",
      "Epoch: 250 - Loss: 0.019546374678611755\n",
      "Epoch: 300 - Loss: 0.01576865464448929\n",
      "Epoch: 350 - Loss: 0.013084964826703072\n",
      "Epoch: 400 - Loss: 0.011054439470171928\n",
      "Epoch: 450 - Loss: 0.009481419809162617\n",
      "Epoch: 500 - Loss: 0.008241205476224422\n",
      "Epoch: 550 - Loss: 0.007239120081067085\n",
      "Epoch: 600 - Loss: 0.006414524745196104\n",
      "Epoch: 650 - Loss: 0.005725751630961895\n",
      "Epoch: 700 - Loss: 0.005145689472556114\n",
      "Epoch: 750 - Loss: 0.0046538193710148335\n",
      "Epoch: 800 - Loss: 0.004232184030115604\n",
      "Epoch: 850 - Loss: 0.0038661016151309013\n",
      "Epoch: 900 - Loss: 0.003548158099874854\n",
      "Epoch: 950 - Loss: 0.003270082175731659\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.5298782587051392\n",
      "Epoch: 50 - Loss: 0.053816936910152435\n",
      "Epoch: 100 - Loss: 0.028285162523388863\n",
      "Epoch: 150 - Loss: 0.01883808895945549\n",
      "Epoch: 200 - Loss: 0.01388726755976677\n",
      "Epoch: 250 - Loss: 0.01079487707465887\n",
      "Epoch: 300 - Loss: 0.008692077361047268\n",
      "Epoch: 350 - Loss: 0.0071838428266346455\n",
      "Epoch: 400 - Loss: 0.006062459200620651\n",
      "Epoch: 450 - Loss: 0.005196176003664732\n",
      "Epoch: 500 - Loss: 0.004513494670391083\n",
      "Epoch: 550 - Loss: 0.0039605386555194855\n",
      "Epoch: 600 - Loss: 0.003506098175421357\n",
      "Epoch: 650 - Loss: 0.0031294014770537615\n",
      "Epoch: 700 - Loss: 0.0028130351565778255\n",
      "Epoch: 750 - Loss: 0.0025464303325861692\n",
      "Epoch: 800 - Loss: 0.0023192574735730886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 850 - Loss: 0.002124612219631672\n",
      "Epoch: 900 - Loss: 0.0019560293294489384\n",
      "Epoch: 950 - Loss: 0.001808028551749885\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.27974534034729\n",
      "Epoch: 50 - Loss: 0.1494886577129364\n",
      "Epoch: 100 - Loss: 0.08851858973503113\n",
      "Epoch: 150 - Loss: 0.06170250102877617\n",
      "Epoch: 200 - Loss: 0.04654977470636368\n",
      "Epoch: 250 - Loss: 0.036874961107969284\n",
      "Epoch: 300 - Loss: 0.03008950501680374\n",
      "Epoch: 350 - Loss: 0.025084685534238815\n",
      "Epoch: 400 - Loss: 0.021295787766575813\n",
      "Epoch: 450 - Loss: 0.018360964953899384\n",
      "Epoch: 500 - Loss: 0.01602783240377903\n",
      "Epoch: 550 - Loss: 0.014130030758678913\n",
      "Epoch: 600 - Loss: 0.012567558325827122\n",
      "Epoch: 650 - Loss: 0.011264767497777939\n",
      "Epoch: 700 - Loss: 0.010164594277739525\n",
      "Epoch: 750 - Loss: 0.009238744154572487\n",
      "Epoch: 800 - Loss: 0.008455488830804825\n",
      "Epoch: 850 - Loss: 0.0077773756347596645\n",
      "Epoch: 900 - Loss: 0.007184536661952734\n",
      "Epoch: 950 - Loss: 0.006666114553809166\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.9033657312393188\n",
      "Epoch: 50 - Loss: 0.06067061796784401\n",
      "Epoch: 100 - Loss: 0.034047242254018784\n",
      "Epoch: 150 - Loss: 0.023205483332276344\n",
      "Epoch: 200 - Loss: 0.017171168699860573\n",
      "Epoch: 250 - Loss: 0.013366002589464188\n",
      "Epoch: 300 - Loss: 0.010731358081102371\n",
      "Epoch: 350 - Loss: 0.008827256970107555\n",
      "Epoch: 400 - Loss: 0.007402262184768915\n",
      "Epoch: 450 - Loss: 0.006295604631304741\n",
      "Epoch: 500 - Loss: 0.005421006120741367\n",
      "Epoch: 550 - Loss: 0.0047266604378819466\n",
      "Epoch: 600 - Loss: 0.0041575925424695015\n",
      "Epoch: 650 - Loss: 0.0036872881464660168\n",
      "Epoch: 700 - Loss: 0.003294598776847124\n",
      "Epoch: 750 - Loss: 0.002962522441521287\n",
      "Epoch: 800 - Loss: 0.002681657439097762\n",
      "Epoch: 850 - Loss: 0.0024415687657892704\n",
      "Epoch: 900 - Loss: 0.002233329229056835\n",
      "Epoch: 950 - Loss: 0.0020541194826364517\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.9898731708526611\n",
      "Epoch: 50 - Loss: 0.037316083908081055\n",
      "Epoch: 100 - Loss: 0.017840562388300896\n",
      "Epoch: 150 - Loss: 0.011142261326313019\n",
      "Epoch: 200 - Loss: 0.007761325687170029\n",
      "Epoch: 250 - Loss: 0.00577019015327096\n",
      "Epoch: 300 - Loss: 0.004476866219192743\n",
      "Epoch: 350 - Loss: 0.0035839113406836987\n",
      "Epoch: 400 - Loss: 0.002941079204902053\n",
      "Epoch: 450 - Loss: 0.0024564710911363363\n",
      "Epoch: 500 - Loss: 0.0020815683528780937\n",
      "Epoch: 550 - Loss: 0.001787436893209815\n",
      "Epoch: 600 - Loss: 0.0015530938981100917\n",
      "Epoch: 650 - Loss: 0.00136410107370466\n",
      "Epoch: 700 - Loss: 0.0012094140984117985\n",
      "Epoch: 750 - Loss: 0.0010810483945533633\n",
      "Epoch: 800 - Loss: 0.0009739078814163804\n",
      "Epoch: 850 - Loss: 0.0008837085915729403\n",
      "Epoch: 900 - Loss: 0.0008071443298831582\n",
      "Epoch: 950 - Loss: 0.0007409017998725176\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.9852170348167419\n",
      "Epoch: 50 - Loss: 0.10753209888935089\n",
      "Epoch: 100 - Loss: 0.06195756047964096\n",
      "Epoch: 150 - Loss: 0.04138200730085373\n",
      "Epoch: 200 - Loss: 0.029918788000941277\n",
      "Epoch: 250 - Loss: 0.022826237604022026\n",
      "Epoch: 300 - Loss: 0.01806522160768509\n",
      "Epoch: 350 - Loss: 0.014709698036313057\n",
      "Epoch: 400 - Loss: 0.012256460264325142\n",
      "Epoch: 450 - Loss: 0.010403793305158615\n",
      "Epoch: 500 - Loss: 0.008957755751907825\n",
      "Epoch: 550 - Loss: 0.007812193129211664\n",
      "Epoch: 600 - Loss: 0.006896873004734516\n",
      "Epoch: 650 - Loss: 0.00615459680557251\n",
      "Epoch: 700 - Loss: 0.005541007965803146\n",
      "Epoch: 750 - Loss: 0.005027578677982092\n",
      "Epoch: 800 - Loss: 0.004592955578118563\n",
      "Epoch: 850 - Loss: 0.004222134128212929\n",
      "Epoch: 900 - Loss: 0.003902978962287307\n",
      "Epoch: 950 - Loss: 0.003623839234933257\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.9116342067718506\n",
      "Epoch: 50 - Loss: 0.10641235113143921\n",
      "Epoch: 100 - Loss: 0.05027324706315994\n",
      "Epoch: 150 - Loss: 0.03101900778710842\n",
      "Epoch: 200 - Loss: 0.021511096507310867\n",
      "Epoch: 250 - Loss: 0.015984686091542244\n",
      "Epoch: 300 - Loss: 0.012380920350551605\n",
      "Epoch: 350 - Loss: 0.009917721152305603\n",
      "Epoch: 400 - Loss: 0.00813133455812931\n",
      "Epoch: 450 - Loss: 0.006804279051721096\n",
      "Epoch: 500 - Loss: 0.0057840789668262005\n",
      "Epoch: 550 - Loss: 0.004990215413272381\n",
      "Epoch: 600 - Loss: 0.0043554832227528095\n",
      "Epoch: 650 - Loss: 0.003844108199700713\n",
      "Epoch: 700 - Loss: 0.003422680776566267\n",
      "Epoch: 750 - Loss: 0.003074454376474023\n",
      "Epoch: 800 - Loss: 0.002780698472633958\n",
      "Epoch: 850 - Loss: 0.00252760318107903\n",
      "Epoch: 900 - Loss: 0.0023092636838555336\n",
      "Epoch: 950 - Loss: 0.002120217541232705\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 4.259442329406738\n",
      "Epoch: 50 - Loss: 0.5642253160476685\n",
      "Epoch: 100 - Loss: 0.30139386653900146\n",
      "Epoch: 150 - Loss: 0.19613243639469147\n",
      "Epoch: 200 - Loss: 0.14250978827476501\n",
      "Epoch: 250 - Loss: 0.11086329817771912\n",
      "Epoch: 300 - Loss: 0.09000157564878464\n",
      "Epoch: 350 - Loss: 0.07508077472448349\n",
      "Epoch: 400 - Loss: 0.06381931900978088\n",
      "Epoch: 450 - Loss: 0.05512012913823128\n",
      "Epoch: 500 - Loss: 0.048260096460580826\n",
      "Epoch: 550 - Loss: 0.04270040988922119\n",
      "Epoch: 600 - Loss: 0.0381438247859478\n",
      "Epoch: 650 - Loss: 0.03437255322933197\n",
      "Epoch: 700 - Loss: 0.03118983469903469\n",
      "Epoch: 750 - Loss: 0.02854134514927864\n",
      "Epoch: 800 - Loss: 0.026275107637047768\n",
      "Epoch: 850 - Loss: 0.024300014600157738\n",
      "Epoch: 900 - Loss: 0.02257213182747364\n",
      "Epoch: 950 - Loss: 0.021050209179520607\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.026536226272583\n",
      "Epoch: 50 - Loss: 0.07965350896120071\n",
      "Epoch: 100 - Loss: 0.04664170742034912\n",
      "Epoch: 150 - Loss: 0.03199606388807297\n",
      "Epoch: 200 - Loss: 0.023766659200191498\n",
      "Epoch: 250 - Loss: 0.01855260320007801\n",
      "Epoch: 300 - Loss: 0.01496125664561987\n",
      "Epoch: 350 - Loss: 0.012344861403107643\n",
      "Epoch: 400 - Loss: 0.010373439639806747\n",
      "Epoch: 450 - Loss: 0.008847212418913841\n",
      "Epoch: 500 - Loss: 0.0076433890499174595\n",
      "Epoch: 550 - Loss: 0.006676422897726297\n",
      "Epoch: 600 - Loss: 0.00588804529979825\n",
      "Epoch: 650 - Loss: 0.005242018029093742\n",
      "Epoch: 700 - Loss: 0.0047018639743328094\n",
      "Epoch: 750 - Loss: 0.004249091260135174\n",
      "Epoch: 800 - Loss: 0.003864834550768137\n",
      "Epoch: 850 - Loss: 0.0035345065407454967\n",
      "Epoch: 900 - Loss: 0.0032476719934493303\n",
      "Epoch: 950 - Loss: 0.0029978840611875057\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.8531501889228821\n",
      "Epoch: 50 - Loss: 0.0451483279466629\n",
      "Epoch: 100 - Loss: 0.02394508197903633\n",
      "Epoch: 150 - Loss: 0.015543030574917793\n",
      "Epoch: 200 - Loss: 0.011045518331229687\n",
      "Epoch: 250 - Loss: 0.008308499120175838\n",
      "Epoch: 300 - Loss: 0.0065021030604839325\n",
      "Epoch: 350 - Loss: 0.005232574418187141\n",
      "Epoch: 400 - Loss: 0.004304844886064529\n",
      "Epoch: 450 - Loss: 0.003605364356189966\n",
      "Epoch: 500 - Loss: 0.0030670221894979477\n",
      "Epoch: 550 - Loss: 0.002643115818500519\n",
      "Epoch: 600 - Loss: 0.0023065167479217052\n",
      "Epoch: 650 - Loss: 0.0020334511063992977\n",
      "Epoch: 700 - Loss: 0.0018089605728164315\n",
      "Epoch: 750 - Loss: 0.0016234242357313633\n",
      "Epoch: 800 - Loss: 0.001467989874072373\n",
      "Epoch: 850 - Loss: 0.0013367427745833993\n",
      "Epoch: 900 - Loss: 0.0012255003675818443\n",
      "Epoch: 950 - Loss: 0.0011302699567750096\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.9373425841331482\n",
      "Epoch: 50 - Loss: 0.02374851144850254\n",
      "Epoch: 100 - Loss: 0.010942202992737293\n",
      "Epoch: 150 - Loss: 0.006360581144690514\n",
      "Epoch: 200 - Loss: 0.004139935597777367\n",
      "Epoch: 250 - Loss: 0.0028874343261122704\n",
      "Epoch: 300 - Loss: 0.0021217213943600655\n",
      "Epoch: 350 - Loss: 0.001619967631995678\n",
      "Epoch: 400 - Loss: 0.001278775162063539\n",
      "Epoch: 450 - Loss: 0.0010359551524743438\n",
      "Epoch: 500 - Loss: 0.0008575250394642353\n",
      "Epoch: 550 - Loss: 0.0007226158049888909\n",
      "Epoch: 600 - Loss: 0.000619186379481107\n",
      "Epoch: 650 - Loss: 0.000538769003469497\n",
      "Epoch: 700 - Loss: 0.0004748525097966194\n",
      "Epoch: 750 - Loss: 0.0004228890174999833\n",
      "Epoch: 800 - Loss: 0.00038042262895032763\n",
      "Epoch: 850 - Loss: 0.00034562349901534617\n",
      "Epoch: 900 - Loss: 0.0003165941743645817\n",
      "Epoch: 950 - Loss: 0.00029190655914135277\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.6216921806335449\n",
      "Epoch: 50 - Loss: 0.062414634972810745\n",
      "Epoch: 100 - Loss: 0.03423763066530228\n",
      "Epoch: 150 - Loss: 0.02247445657849312\n",
      "Epoch: 200 - Loss: 0.01604972966015339\n",
      "Epoch: 250 - Loss: 0.012090501375496387\n",
      "Epoch: 300 - Loss: 0.009455477818846703\n",
      "Epoch: 350 - Loss: 0.007603438105434179\n",
      "Epoch: 400 - Loss: 0.006248555611819029\n",
      "Epoch: 450 - Loss: 0.005226475186645985\n",
      "Epoch: 500 - Loss: 0.004433486610651016\n",
      "Epoch: 550 - Loss: 0.0038068846333771944\n",
      "Epoch: 600 - Loss: 0.003305143676698208\n",
      "Epoch: 650 - Loss: 0.002900185063481331\n",
      "Epoch: 700 - Loss: 0.0025704288855195045\n",
      "Epoch: 750 - Loss: 0.0022977280896157026\n",
      "Epoch: 800 - Loss: 0.0020704835187643766\n",
      "Epoch: 850 - Loss: 0.0018789103487506509\n",
      "Epoch: 900 - Loss: 0.0017159524140879512\n",
      "Epoch: 950 - Loss: 0.0015766508877277374\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.3897671699523926\n",
      "Epoch: 50 - Loss: 0.08911313861608505\n",
      "Epoch: 100 - Loss: 0.05445737764239311\n",
      "Epoch: 150 - Loss: 0.03918902575969696\n",
      "Epoch: 200 - Loss: 0.030262667685747147\n",
      "Epoch: 250 - Loss: 0.024410497397184372\n",
      "Epoch: 300 - Loss: 0.020216288045048714\n",
      "Epoch: 350 - Loss: 0.017085673287510872\n",
      "Epoch: 400 - Loss: 0.014665281400084496\n",
      "Epoch: 450 - Loss: 0.012767347507178783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 500 - Loss: 0.011233700439333916\n",
      "Epoch: 550 - Loss: 0.009977533482015133\n",
      "Epoch: 600 - Loss: 0.008938005194067955\n",
      "Epoch: 650 - Loss: 0.008064805530011654\n",
      "Epoch: 700 - Loss: 0.007318519987165928\n",
      "Epoch: 750 - Loss: 0.006675945594906807\n",
      "Epoch: 800 - Loss: 0.0061158789321780205\n",
      "Epoch: 850 - Loss: 0.005626716185361147\n",
      "Epoch: 900 - Loss: 0.005197673570364714\n",
      "Epoch: 950 - Loss: 0.004816304426640272\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.8425377011299133\n",
      "Epoch: 50 - Loss: 0.04102066159248352\n",
      "Epoch: 100 - Loss: 0.021722732111811638\n",
      "Epoch: 150 - Loss: 0.013927562162280083\n",
      "Epoch: 200 - Loss: 0.009822634980082512\n",
      "Epoch: 250 - Loss: 0.007345687597990036\n",
      "Epoch: 300 - Loss: 0.005706587340682745\n",
      "Epoch: 350 - Loss: 0.004552874714136124\n",
      "Epoch: 400 - Loss: 0.003705492941662669\n",
      "Epoch: 450 - Loss: 0.003065686207264662\n",
      "Epoch: 500 - Loss: 0.0025769954081624746\n",
      "Epoch: 550 - Loss: 0.002193664200603962\n",
      "Epoch: 600 - Loss: 0.00188799190800637\n",
      "Epoch: 650 - Loss: 0.0016406227368861437\n",
      "Epoch: 700 - Loss: 0.0014401780208572745\n",
      "Epoch: 750 - Loss: 0.0012772938935086131\n",
      "Epoch: 800 - Loss: 0.0011433068430051208\n",
      "Epoch: 850 - Loss: 0.0010304157622158527\n",
      "Epoch: 900 - Loss: 0.0009350047912448645\n",
      "Epoch: 950 - Loss: 0.000854033452924341\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.9187240600585938\n",
      "Epoch: 50 - Loss: 0.0322003997862339\n",
      "Epoch: 100 - Loss: 0.01655704900622368\n",
      "Epoch: 150 - Loss: 0.010517111979424953\n",
      "Epoch: 200 - Loss: 0.007329563610255718\n",
      "Epoch: 250 - Loss: 0.005391679238528013\n",
      "Epoch: 300 - Loss: 0.0041303266771137714\n",
      "Epoch: 350 - Loss: 0.0032724342308938503\n",
      "Epoch: 400 - Loss: 0.0026606956962496042\n",
      "Epoch: 450 - Loss: 0.002209327882155776\n",
      "Epoch: 500 - Loss: 0.0018702452071011066\n",
      "Epoch: 550 - Loss: 0.0016103797825053334\n",
      "Epoch: 600 - Loss: 0.00140471663326025\n",
      "Epoch: 650 - Loss: 0.0012397997779771686\n",
      "Epoch: 700 - Loss: 0.0011060558026656508\n",
      "Epoch: 750 - Loss: 0.0009961858158931136\n",
      "Epoch: 800 - Loss: 0.0009047522908076644\n",
      "Epoch: 850 - Loss: 0.0008277089800685644\n",
      "Epoch: 900 - Loss: 0.0007618825184181333\n",
      "Epoch: 950 - Loss: 0.0007055134046822786\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.9063105583190918\n",
      "Epoch: 50 - Loss: 0.04666866362094879\n",
      "Epoch: 100 - Loss: 0.02601408027112484\n",
      "Epoch: 150 - Loss: 0.017154039815068245\n",
      "Epoch: 200 - Loss: 0.012260141782462597\n",
      "Epoch: 250 - Loss: 0.009209772571921349\n",
      "Epoch: 300 - Loss: 0.007151435595005751\n",
      "Epoch: 350 - Loss: 0.005695864092558622\n",
      "Epoch: 400 - Loss: 0.004627097863703966\n",
      "Epoch: 450 - Loss: 0.003818945959210396\n",
      "Epoch: 500 - Loss: 0.0032028036657720804\n",
      "Epoch: 550 - Loss: 0.002720859833061695\n",
      "Epoch: 600 - Loss: 0.002342549618333578\n",
      "Epoch: 650 - Loss: 0.002039034850895405\n",
      "Epoch: 700 - Loss: 0.0017932349583134055\n",
      "Epoch: 750 - Loss: 0.0015906633343547583\n",
      "Epoch: 800 - Loss: 0.0014218329451978207\n",
      "Epoch: 850 - Loss: 0.0012799930991604924\n",
      "Epoch: 900 - Loss: 0.0011603848543018103\n",
      "Epoch: 950 - Loss: 0.0010587326250970364\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.2471717596054077\n",
      "Epoch: 50 - Loss: 0.04104243218898773\n",
      "Epoch: 100 - Loss: 0.020179383456707\n",
      "Epoch: 150 - Loss: 0.012574467808008194\n",
      "Epoch: 200 - Loss: 0.008671944960951805\n",
      "Epoch: 250 - Loss: 0.006352671422064304\n",
      "Epoch: 300 - Loss: 0.004843226633965969\n",
      "Epoch: 350 - Loss: 0.0037934936117380857\n",
      "Epoch: 400 - Loss: 0.003035939997062087\n",
      "Epoch: 450 - Loss: 0.0024773881305009127\n",
      "Epoch: 500 - Loss: 0.0020565323065966368\n",
      "Epoch: 550 - Loss: 0.0017313819844275713\n",
      "Epoch: 600 - Loss: 0.0014760870253667235\n",
      "Epoch: 650 - Loss: 0.0012740361271426082\n",
      "Epoch: 700 - Loss: 0.0011121168499812484\n",
      "Epoch: 750 - Loss: 0.0009798441315069795\n",
      "Epoch: 800 - Loss: 0.0008699379395693541\n",
      "Epoch: 850 - Loss: 0.0007783386972732842\n",
      "Epoch: 900 - Loss: 0.0007011082489043474\n",
      "Epoch: 950 - Loss: 0.0006354507640935481\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 0.8749516010284424\n",
      "Epoch: 50 - Loss: 0.02685985527932644\n",
      "Epoch: 100 - Loss: 0.012073631398379803\n",
      "Epoch: 150 - Loss: 0.006885279435664415\n",
      "Epoch: 200 - Loss: 0.0043764542788267136\n",
      "Epoch: 250 - Loss: 0.0029735045973211527\n",
      "Epoch: 300 - Loss: 0.0021178117021918297\n",
      "Epoch: 350 - Loss: 0.0015657112235203385\n",
      "Epoch: 400 - Loss: 0.0011970545165240765\n",
      "Epoch: 450 - Loss: 0.0009409449994564056\n",
      "Epoch: 500 - Loss: 0.0007588125299662352\n",
      "Epoch: 550 - Loss: 0.0006245281547307968\n",
      "Epoch: 600 - Loss: 0.0005231620743870735\n",
      "Epoch: 650 - Loss: 0.0004452361026778817\n",
      "Epoch: 700 - Loss: 0.00038451619911938906\n",
      "Epoch: 750 - Loss: 0.0003371934872120619\n",
      "Epoch: 800 - Loss: 0.0002993025700561702\n",
      "Epoch: 850 - Loss: 0.00026906930725090206\n",
      "Epoch: 900 - Loss: 0.00024443582515232265\n",
      "Epoch: 950 - Loss: 0.00022398926375899464\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.5509413480758667\n",
      "Epoch: 50 - Loss: 0.3233165740966797\n",
      "Epoch: 100 - Loss: 0.21526767313480377\n",
      "Epoch: 150 - Loss: 0.16233764588832855\n",
      "Epoch: 200 - Loss: 0.12951168417930603\n",
      "Epoch: 250 - Loss: 0.10696469992399216\n",
      "Epoch: 300 - Loss: 0.09061060845851898\n",
      "Epoch: 350 - Loss: 0.07824354618787766\n",
      "Epoch: 400 - Loss: 0.06845903396606445\n",
      "Epoch: 450 - Loss: 0.06057165190577507\n",
      "Epoch: 500 - Loss: 0.05408076196908951\n",
      "Epoch: 550 - Loss: 0.048681870102882385\n",
      "Epoch: 600 - Loss: 0.04412304237484932\n",
      "Epoch: 650 - Loss: 0.040216993540525436\n",
      "Epoch: 700 - Loss: 0.03682877495884895\n",
      "Epoch: 750 - Loss: 0.03385603427886963\n",
      "Epoch: 800 - Loss: 0.031242532655596733\n",
      "Epoch: 850 - Loss: 0.02892986126244068\n",
      "Epoch: 900 - Loss: 0.026888875290751457\n",
      "Epoch: 950 - Loss: 0.02507258951663971\n",
      "[1, 0, 1]\n",
      "Epoch: 0 - Loss: 1.1870461702346802\n",
      "Epoch: 50 - Loss: 0.04442141577601433\n",
      "Epoch: 100 - Loss: 0.02100839652121067\n",
      "Epoch: 150 - Loss: 0.012512380257248878\n",
      "Epoch: 200 - Loss: 0.008352000266313553\n",
      "Epoch: 250 - Loss: 0.0059783244505524635\n",
      "Epoch: 300 - Loss: 0.004485504701733589\n",
      "Epoch: 350 - Loss: 0.0034796586260199547\n",
      "Epoch: 400 - Loss: 0.002773698652163148\n",
      "Epoch: 450 - Loss: 0.0022615620400756598\n",
      "Epoch: 500 - Loss: 0.0018766981083899736\n",
      "Epoch: 550 - Loss: 0.001581842196173966\n",
      "Epoch: 600 - Loss: 0.0013534694444388151\n",
      "Epoch: 650 - Loss: 0.0011740208137780428\n",
      "Epoch: 700 - Loss: 0.0010295978281646967\n",
      "Epoch: 750 - Loss: 0.0009113125270232558\n",
      "Epoch: 800 - Loss: 0.0008135073003359139\n",
      "Epoch: 850 - Loss: 0.0007319359574466944\n",
      "Epoch: 900 - Loss: 0.0006628248374909163\n",
      "Epoch: 950 - Loss: 0.000603975378908217\n"
     ]
    }
   ],
   "source": [
    "pr, re = [[],[],[]], [[],[],[]]\n",
    "ndcgs = defaultdict(list)\n",
    "\n",
    "lr = 0.001\n",
    "epochs = 1000\n",
    "constraintHigh=1\n",
    "constraintLow=0\n",
    "# constraintHigh=float('inf')\n",
    "# constraintLow=-float('inf')\n",
    "loss_weight = [1, 0, 1]\n",
    "L1, L2 = 0, 0\n",
    "rand_type = 0\n",
    "\n",
    "total_mul = 1\n",
    "\n",
    "for uid, uemb in enumerate(tqdm(doc_embs[:50])):\n",
    "    x = word_embs.T\n",
    "    y = uemb\n",
    "    total = len(doc_answers[uid])\n",
    "    total = 1\n",
    "    \n",
    "    torch_model = PyTorchLinearRegression(x.shape[1], lr, constraintHigh, constraintLow, int(total*total_mul), rand_type, L1, L2)\n",
    "    torch_model.fit(x, y, epochs)\n",
    "    \n",
    "    m1 = metric4(torch_model.w.detach().numpy(), doc_answers[uid], w_idx=None, topk=50, verbose=0)\n",
    "    m2 = metric4(torch_model.w.detach().numpy(), doc_answers[uid], w_idx=None, topk=100, verbose=0)\n",
    "    m3 = metric4(torch_model.w.detach().numpy(), doc_answers[uid], w_idx=None, topk=200, verbose=0)\n",
    "    ndcg1 = metric_ndcg2(torch_model.w.detach().numpy(), tfidf_ans, uid, topk=50, verbose=0)\n",
    "    ndcg2 = metric_ndcg2(torch_model.w.detach().numpy(), tfidf_ans, uid, topk=100, verbose=0)\n",
    "    ndcg3 = metric_ndcg2(torch_model.w.detach().numpy(), tfidf_ans, uid, topk=200, verbose=0)\n",
    "    ndcg4 = metric_ndcg2(torch_model.w.detach().numpy(), tfidf_ans, uid, topk=None, verbose=0)\n",
    "    pr[0].append(m1[\"precision\"])\n",
    "    re[0].append(m1[\"recall\"])\n",
    "    pr[1].append(m2[\"precision\"])\n",
    "    re[1].append(m2[\"recall\"])\n",
    "    pr[2].append(m3[\"precision\"])\n",
    "    re[2].append(m3[\"recall\"])\n",
    "    \n",
    "    ndcgs[\"50\"].append(ndcg1)\n",
    "    ndcgs[\"100\"].append(ndcg2)\n",
    "    ndcgs[\"200\"].append(ndcg3)\n",
    "    ndcgs[\"-1\"].append(ndcg4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1aae95d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.1668 Recall:0.3633\n",
      "Precision:0.1130 Recall:0.4528\n",
      "Precision:0.0738 Recall:0.5616\n",
      "NDCG 50:0.5402\n",
      "NDCG 100:0.5692\n",
      "NDCG 200:0.6025\n",
      "NDCG all:0.7088\n"
     ]
    }
   ],
   "source": [
    "print(f\"Precision:{np.mean(pr[0]):.4f} Recall:{np.mean(re[0]):.4f}\")\n",
    "print(f\"Precision:{np.mean(pr[1]):.4f} Recall:{np.mean(re[1]):.4f}\")\n",
    "print(f\"Precision:{np.mean(pr[2]):.4f} Recall:{np.mean(re[2]):.4f}\")\n",
    "print(f\"NDCG 50:{np.mean(ndcgs['50']):.4f}\")\n",
    "print(f\"NDCG 100:{np.mean(ndcgs['100']):.4f}\")\n",
    "print(f\"NDCG 200:{np.mean(ndcgs['200']):.4f}\")\n",
    "print(f\"NDCG all:{np.mean(ndcgs['-1']):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41e49473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d341822bf34485b8b8e59fa8984523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.1165862083435059\n",
      "Epoch: 50 - Loss: 0.1816755086183548\n",
      "Epoch: 100 - Loss: 0.10563840717077255\n",
      "Epoch: 150 - Loss: 0.07007451355457306\n",
      "Epoch: 200 - Loss: 0.05024390295147896\n",
      "Epoch: 250 - Loss: 0.03785489872097969\n",
      "Epoch: 300 - Loss: 0.029545774683356285\n",
      "Epoch: 350 - Loss: 0.023728150874376297\n",
      "Epoch: 400 - Loss: 0.01943042315542698\n",
      "Epoch: 450 - Loss: 0.016181474551558495\n",
      "Epoch: 500 - Loss: 0.013679803349077702\n",
      "Epoch: 550 - Loss: 0.011709356680512428\n",
      "Epoch: 600 - Loss: 0.010133947245776653\n",
      "Epoch: 650 - Loss: 0.00884774886071682\n",
      "Epoch: 700 - Loss: 0.0077849519439041615\n",
      "Epoch: 750 - Loss: 0.006893964018672705\n",
      "Epoch: 800 - Loss: 0.00613404018804431\n",
      "Epoch: 850 - Loss: 0.005482356529682875\n",
      "Epoch: 900 - Loss: 0.00492490828037262\n",
      "Epoch: 950 - Loss: 0.004443036392331123\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.0777878761291504\n",
      "Epoch: 50 - Loss: 0.031145384535193443\n",
      "Epoch: 100 - Loss: 0.014589441940188408\n",
      "Epoch: 150 - Loss: 0.008423123508691788\n",
      "Epoch: 200 - Loss: 0.005396455526351929\n",
      "Epoch: 250 - Loss: 0.0036758792120963335\n",
      "Epoch: 300 - Loss: 0.00261151185259223\n",
      "Epoch: 350 - Loss: 0.0019139237701892853\n",
      "Epoch: 400 - Loss: 0.0014377972111105919\n",
      "Epoch: 450 - Loss: 0.0011048309970647097\n",
      "Epoch: 500 - Loss: 0.0008651316165924072\n",
      "Epoch: 550 - Loss: 0.0006883472087793052\n",
      "Epoch: 600 - Loss: 0.0005563437007367611\n",
      "Epoch: 650 - Loss: 0.00045581618905998766\n",
      "Epoch: 700 - Loss: 0.00037701838300563395\n",
      "Epoch: 750 - Loss: 0.00031490265973843634\n",
      "Epoch: 800 - Loss: 0.00026519980747252703\n",
      "Epoch: 850 - Loss: 0.00022519295453093946\n",
      "Epoch: 900 - Loss: 0.00019251300545874983\n",
      "Epoch: 950 - Loss: 0.00016538667841814458\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.9535503387451172\n",
      "Epoch: 50 - Loss: 0.0897340327501297\n",
      "Epoch: 100 - Loss: 0.05087755620479584\n",
      "Epoch: 150 - Loss: 0.033586032688617706\n",
      "Epoch: 200 - Loss: 0.02392159216105938\n",
      "Epoch: 250 - Loss: 0.01795768365263939\n",
      "Epoch: 300 - Loss: 0.013960436917841434\n",
      "Epoch: 350 - Loss: 0.011142661795020103\n",
      "Epoch: 400 - Loss: 0.009108480997383595\n",
      "Epoch: 450 - Loss: 0.007569433655589819\n",
      "Epoch: 500 - Loss: 0.006371540017426014\n",
      "Epoch: 550 - Loss: 0.005424260627478361\n",
      "Epoch: 600 - Loss: 0.004663277883082628\n",
      "Epoch: 650 - Loss: 0.0040379674173891544\n",
      "Epoch: 700 - Loss: 0.0035166707821190357\n",
      "Epoch: 750 - Loss: 0.003079422749578953\n",
      "Epoch: 800 - Loss: 0.0027100006118416786\n",
      "Epoch: 850 - Loss: 0.0023948324378579855\n",
      "Epoch: 900 - Loss: 0.0021244436502456665\n",
      "Epoch: 950 - Loss: 0.001890472136437893\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.8551814556121826\n",
      "Epoch: 50 - Loss: 0.05419726297259331\n",
      "Epoch: 100 - Loss: 0.028987528756260872\n",
      "Epoch: 150 - Loss: 0.018637893721461296\n",
      "Epoch: 200 - Loss: 0.013030774891376495\n",
      "Epoch: 250 - Loss: 0.009621425531804562\n",
      "Epoch: 300 - Loss: 0.007390997372567654\n",
      "Epoch: 350 - Loss: 0.005861826706677675\n",
      "Epoch: 400 - Loss: 0.004753198008984327\n",
      "Epoch: 450 - Loss: 0.003915786277502775\n",
      "Epoch: 500 - Loss: 0.00326896901242435\n",
      "Epoch: 550 - Loss: 0.002760672476142645\n",
      "Epoch: 600 - Loss: 0.002353210933506489\n",
      "Epoch: 650 - Loss: 0.002021975116804242\n",
      "Epoch: 700 - Loss: 0.0017516539664939046\n",
      "Epoch: 750 - Loss: 0.001526312087662518\n",
      "Epoch: 800 - Loss: 0.0013379703741520643\n",
      "Epoch: 850 - Loss: 0.0011799379717558622\n",
      "Epoch: 900 - Loss: 0.0010461792116984725\n",
      "Epoch: 950 - Loss: 0.0009319824166595936\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.1088433265686035\n",
      "Epoch: 50 - Loss: 0.10227731615304947\n",
      "Epoch: 100 - Loss: 0.06234977766871452\n",
      "Epoch: 150 - Loss: 0.04395866021513939\n",
      "Epoch: 200 - Loss: 0.033221982419490814\n",
      "Epoch: 250 - Loss: 0.026216847822070122\n",
      "Epoch: 300 - Loss: 0.02128179743885994\n",
      "Epoch: 350 - Loss: 0.01764487475156784\n",
      "Epoch: 400 - Loss: 0.014876200817525387\n",
      "Epoch: 450 - Loss: 0.012693921104073524\n",
      "Epoch: 500 - Loss: 0.010946886613965034\n",
      "Epoch: 550 - Loss: 0.009520009160041809\n",
      "Epoch: 600 - Loss: 0.008328545838594437\n",
      "Epoch: 650 - Loss: 0.007324707694351673\n",
      "Epoch: 700 - Loss: 0.006478442344814539\n",
      "Epoch: 750 - Loss: 0.005761065986007452\n",
      "Epoch: 800 - Loss: 0.005148187279701233\n",
      "Epoch: 850 - Loss: 0.004618665669113398\n",
      "Epoch: 900 - Loss: 0.004159600008279085\n",
      "Epoch: 950 - Loss: 0.0037579433992505074\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.0367999076843262\n",
      "Epoch: 50 - Loss: 0.065402090549469\n",
      "Epoch: 100 - Loss: 0.03793670982122421\n",
      "Epoch: 150 - Loss: 0.025737565010786057\n",
      "Epoch: 200 - Loss: 0.018796248361468315\n",
      "Epoch: 250 - Loss: 0.014367028139531612\n",
      "Epoch: 300 - Loss: 0.011298473924398422\n",
      "Epoch: 350 - Loss: 0.009103693068027496\n",
      "Epoch: 400 - Loss: 0.007460040971636772\n",
      "Epoch: 450 - Loss: 0.00619165226817131\n",
      "Epoch: 500 - Loss: 0.005190597847104073\n",
      "Epoch: 550 - Loss: 0.004389290232211351\n",
      "Epoch: 600 - Loss: 0.003740559332072735\n",
      "Epoch: 650 - Loss: 0.0032145362347364426\n",
      "Epoch: 700 - Loss: 0.0027824544813483953\n",
      "Epoch: 750 - Loss: 0.002423639874905348\n",
      "Epoch: 800 - Loss: 0.0021209176629781723\n",
      "Epoch: 850 - Loss: 0.0018630134873092175\n",
      "Epoch: 900 - Loss: 0.0016425722278654575\n",
      "Epoch: 950 - Loss: 0.0014541870914399624\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.0828191041946411\n",
      "Epoch: 50 - Loss: 0.06244263797998428\n",
      "Epoch: 100 - Loss: 0.03708764165639877\n",
      "Epoch: 150 - Loss: 0.025631332769989967\n",
      "Epoch: 200 - Loss: 0.019018489867448807\n",
      "Epoch: 250 - Loss: 0.014704705215990543\n",
      "Epoch: 300 - Loss: 0.011660736985504627\n",
      "Epoch: 350 - Loss: 0.009433366358280182\n",
      "Epoch: 400 - Loss: 0.007765407674014568\n",
      "Epoch: 450 - Loss: 0.0064838118851184845\n",
      "Epoch: 500 - Loss: 0.0054768449626863\n",
      "Epoch: 550 - Loss: 0.004677588120102882\n",
      "Epoch: 600 - Loss: 0.004028609488159418\n",
      "Epoch: 650 - Loss: 0.0034935057628899813\n",
      "Epoch: 700 - Loss: 0.0030525282490998507\n",
      "Epoch: 750 - Loss: 0.002682737074792385\n",
      "Epoch: 800 - Loss: 0.0023704548366367817\n",
      "Epoch: 850 - Loss: 0.002105951076373458\n",
      "Epoch: 900 - Loss: 0.0018805824220180511\n",
      "Epoch: 950 - Loss: 0.0016854449640959501\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.7859926819801331\n",
      "Epoch: 50 - Loss: 0.02502225898206234\n",
      "Epoch: 100 - Loss: 0.0125057902187109\n",
      "Epoch: 150 - Loss: 0.007617316674441099\n",
      "Epoch: 200 - Loss: 0.005086660850793123\n",
      "Epoch: 250 - Loss: 0.003615104593336582\n",
      "Epoch: 300 - Loss: 0.0026743996422737837\n",
      "Epoch: 350 - Loss: 0.0020453124307096004\n",
      "Epoch: 400 - Loss: 0.0015967977233231068\n",
      "Epoch: 450 - Loss: 0.001271708169952035\n",
      "Epoch: 500 - Loss: 0.0010256946552544832\n",
      "Epoch: 550 - Loss: 0.0008363380911760032\n",
      "Epoch: 600 - Loss: 0.0006885713082738221\n",
      "Epoch: 650 - Loss: 0.0005714684375561774\n",
      "Epoch: 700 - Loss: 0.0004779018054250628\n",
      "Epoch: 750 - Loss: 0.00040256805368699133\n",
      "Epoch: 800 - Loss: 0.00034144558594562113\n",
      "Epoch: 850 - Loss: 0.0002909389731939882\n",
      "Epoch: 900 - Loss: 0.00024875960662029684\n",
      "Epoch: 950 - Loss: 0.00021347144502215087\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.7265727519989014\n",
      "Epoch: 50 - Loss: 0.02377687767148018\n",
      "Epoch: 100 - Loss: 0.012534544803202152\n",
      "Epoch: 150 - Loss: 0.007851635105907917\n",
      "Epoch: 200 - Loss: 0.005300818011164665\n",
      "Epoch: 250 - Loss: 0.0037678920198231936\n",
      "Epoch: 300 - Loss: 0.0027754574548453093\n",
      "Epoch: 350 - Loss: 0.0021024092566221952\n",
      "Epoch: 400 - Loss: 0.0016242304118350148\n",
      "Epoch: 450 - Loss: 0.0012763050617650151\n",
      "Epoch: 500 - Loss: 0.0010173132177442312\n",
      "Epoch: 550 - Loss: 0.0008204600308090448\n",
      "Epoch: 600 - Loss: 0.0006686704000458121\n",
      "Epoch: 650 - Loss: 0.0005505862645804882\n",
      "Epoch: 700 - Loss: 0.00045666605001315475\n",
      "Epoch: 750 - Loss: 0.00038157388917170465\n",
      "Epoch: 800 - Loss: 0.00032086807186715305\n",
      "Epoch: 850 - Loss: 0.0002714555594138801\n",
      "Epoch: 900 - Loss: 0.0002309291739948094\n",
      "Epoch: 950 - Loss: 0.0001974759652512148\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.8648017048835754\n",
      "Epoch: 50 - Loss: 0.026804611086845398\n",
      "Epoch: 100 - Loss: 0.014529452659189701\n",
      "Epoch: 150 - Loss: 0.009540652856230736\n",
      "Epoch: 200 - Loss: 0.006792786065489054\n",
      "Epoch: 250 - Loss: 0.00506470026448369\n",
      "Epoch: 300 - Loss: 0.003914410248398781\n",
      "Epoch: 350 - Loss: 0.003097287379205227\n",
      "Epoch: 400 - Loss: 0.0024842622224241495\n",
      "Epoch: 450 - Loss: 0.0020198740530759096\n",
      "Epoch: 500 - Loss: 0.0016600884264335036\n",
      "Epoch: 550 - Loss: 0.0013779415749013424\n",
      "Epoch: 600 - Loss: 0.0011529277544468641\n",
      "Epoch: 650 - Loss: 0.0009702130919322371\n",
      "Epoch: 700 - Loss: 0.0008208244107663631\n",
      "Epoch: 750 - Loss: 0.0006980132311582565\n",
      "Epoch: 800 - Loss: 0.0005961161805316806\n",
      "Epoch: 850 - Loss: 0.0005111886421218514\n",
      "Epoch: 900 - Loss: 0.0004396383883431554\n",
      "Epoch: 950 - Loss: 0.00037940728361718357\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 2.8218798637390137\n",
      "Epoch: 50 - Loss: 0.525658369064331\n",
      "Epoch: 100 - Loss: 0.3584999442100525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 150 - Loss: 0.2710718810558319\n",
      "Epoch: 200 - Loss: 0.2151811718940735\n",
      "Epoch: 250 - Loss: 0.17607662081718445\n",
      "Epoch: 300 - Loss: 0.14746537804603577\n",
      "Epoch: 350 - Loss: 0.12569749355316162\n",
      "Epoch: 400 - Loss: 0.10886117070913315\n",
      "Epoch: 450 - Loss: 0.0954778864979744\n",
      "Epoch: 500 - Loss: 0.08449497073888779\n",
      "Epoch: 550 - Loss: 0.07536499202251434\n",
      "Epoch: 600 - Loss: 0.06769471615552902\n",
      "Epoch: 650 - Loss: 0.06115030124783516\n",
      "Epoch: 700 - Loss: 0.05543007701635361\n",
      "Epoch: 750 - Loss: 0.050411347299814224\n",
      "Epoch: 800 - Loss: 0.04602452367544174\n",
      "Epoch: 850 - Loss: 0.04214639216661453\n",
      "Epoch: 900 - Loss: 0.0386776365339756\n",
      "Epoch: 950 - Loss: 0.03556389734148979\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.9688442945480347\n",
      "Epoch: 50 - Loss: 0.03014732338488102\n",
      "Epoch: 100 - Loss: 0.014356052502989769\n",
      "Epoch: 150 - Loss: 0.00866059772670269\n",
      "Epoch: 200 - Loss: 0.005771436262875795\n",
      "Epoch: 250 - Loss: 0.004069525748491287\n",
      "Epoch: 300 - Loss: 0.0029855899047106504\n",
      "Epoch: 350 - Loss: 0.002260894514620304\n",
      "Epoch: 400 - Loss: 0.0017533793579787016\n",
      "Epoch: 450 - Loss: 0.001385563868097961\n",
      "Epoch: 500 - Loss: 0.0011094488436356187\n",
      "Epoch: 550 - Loss: 0.0008992927614599466\n",
      "Epoch: 600 - Loss: 0.000736951653379947\n",
      "Epoch: 650 - Loss: 0.0006099307793192565\n",
      "Epoch: 700 - Loss: 0.0005089478217996657\n",
      "Epoch: 750 - Loss: 0.00042756370385177433\n",
      "Epoch: 800 - Loss: 0.00036146226921118796\n",
      "Epoch: 850 - Loss: 0.0003075593849644065\n",
      "Epoch: 900 - Loss: 0.00026314097340218723\n",
      "Epoch: 950 - Loss: 0.00022618824732489884\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.7911754846572876\n",
      "Epoch: 50 - Loss: 0.02892685867846012\n",
      "Epoch: 100 - Loss: 0.014988677576184273\n",
      "Epoch: 150 - Loss: 0.009403896518051624\n",
      "Epoch: 200 - Loss: 0.006479337345808744\n",
      "Epoch: 250 - Loss: 0.004711000714451075\n",
      "Epoch: 300 - Loss: 0.0035570061299949884\n",
      "Epoch: 350 - Loss: 0.002760230330750346\n",
      "Epoch: 400 - Loss: 0.002194614615291357\n",
      "Epoch: 450 - Loss: 0.0017752853455021977\n",
      "Epoch: 500 - Loss: 0.0014564075972884893\n",
      "Epoch: 550 - Loss: 0.0012071984820067883\n",
      "Epoch: 600 - Loss: 0.0010095656616613269\n",
      "Epoch: 650 - Loss: 0.0008517116075381637\n",
      "Epoch: 700 - Loss: 0.0007233457290567458\n",
      "Epoch: 750 - Loss: 0.000617286772467196\n",
      "Epoch: 800 - Loss: 0.000529926735907793\n",
      "Epoch: 850 - Loss: 0.0004569632583297789\n",
      "Epoch: 900 - Loss: 0.00039609058876521885\n",
      "Epoch: 950 - Loss: 0.00034469467937014997\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.9527069330215454\n",
      "Epoch: 50 - Loss: 0.04279899224638939\n",
      "Epoch: 100 - Loss: 0.02256336435675621\n",
      "Epoch: 150 - Loss: 0.014299116097390652\n",
      "Epoch: 200 - Loss: 0.009879864752292633\n",
      "Epoch: 250 - Loss: 0.007169313263148069\n",
      "Epoch: 300 - Loss: 0.005390011705458164\n",
      "Epoch: 350 - Loss: 0.004153820686042309\n",
      "Epoch: 400 - Loss: 0.003262842074036598\n",
      "Epoch: 450 - Loss: 0.002602041931822896\n",
      "Epoch: 500 - Loss: 0.0020963610149919987\n",
      "Epoch: 550 - Loss: 0.0017037844518199563\n",
      "Epoch: 600 - Loss: 0.0013979594223201275\n",
      "Epoch: 650 - Loss: 0.0011557743418961763\n",
      "Epoch: 700 - Loss: 0.0009607410174794495\n",
      "Epoch: 750 - Loss: 0.0008034099591895938\n",
      "Epoch: 800 - Loss: 0.0006749739404767752\n",
      "Epoch: 850 - Loss: 0.0005698621389456093\n",
      "Epoch: 900 - Loss: 0.0004834272840525955\n",
      "Epoch: 950 - Loss: 0.0004120396915823221\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.126102089881897\n",
      "Epoch: 50 - Loss: 0.0655687153339386\n",
      "Epoch: 100 - Loss: 0.037734631448984146\n",
      "Epoch: 150 - Loss: 0.025759179145097733\n",
      "Epoch: 200 - Loss: 0.018894540145993233\n",
      "Epoch: 250 - Loss: 0.014478101395070553\n",
      "Epoch: 300 - Loss: 0.011412427760660648\n",
      "Epoch: 350 - Loss: 0.009183669462800026\n",
      "Epoch: 400 - Loss: 0.007502981927245855\n",
      "Epoch: 450 - Loss: 0.00621993700042367\n",
      "Epoch: 500 - Loss: 0.005218229256570339\n",
      "Epoch: 550 - Loss: 0.0044240267015993595\n",
      "Epoch: 600 - Loss: 0.003782382234930992\n",
      "Epoch: 650 - Loss: 0.0032594394870102406\n",
      "Epoch: 700 - Loss: 0.002829540055245161\n",
      "Epoch: 750 - Loss: 0.0024741238448768854\n",
      "Epoch: 800 - Loss: 0.0021749725565314293\n",
      "Epoch: 850 - Loss: 0.0019207585137337446\n",
      "Epoch: 900 - Loss: 0.0017037892248481512\n",
      "Epoch: 950 - Loss: 0.0015189015539363027\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.7855663895606995\n",
      "Epoch: 50 - Loss: 0.0528978630900383\n",
      "Epoch: 100 - Loss: 0.03147729113698006\n",
      "Epoch: 150 - Loss: 0.02185518480837345\n",
      "Epoch: 200 - Loss: 0.016283392906188965\n",
      "Epoch: 250 - Loss: 0.012624547816812992\n",
      "Epoch: 300 - Loss: 0.010053446516394615\n",
      "Epoch: 350 - Loss: 0.0081879748031497\n",
      "Epoch: 400 - Loss: 0.006782186217606068\n",
      "Epoch: 450 - Loss: 0.005691987462341785\n",
      "Epoch: 500 - Loss: 0.004834281280636787\n",
      "Epoch: 550 - Loss: 0.004143679980188608\n",
      "Epoch: 600 - Loss: 0.0035754842683672905\n",
      "Epoch: 650 - Loss: 0.003108469769358635\n",
      "Epoch: 700 - Loss: 0.002715426729992032\n",
      "Epoch: 750 - Loss: 0.0023807815741747618\n",
      "Epoch: 800 - Loss: 0.002095611533150077\n",
      "Epoch: 850 - Loss: 0.0018520742887631059\n",
      "Epoch: 900 - Loss: 0.0016422600019723177\n",
      "Epoch: 950 - Loss: 0.0014598103007301688\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.7206735014915466\n",
      "Epoch: 50 - Loss: 0.04352105036377907\n",
      "Epoch: 100 - Loss: 0.025189468637108803\n",
      "Epoch: 150 - Loss: 0.017195172607898712\n",
      "Epoch: 200 - Loss: 0.012621438130736351\n",
      "Epoch: 250 - Loss: 0.009674365632236004\n",
      "Epoch: 300 - Loss: 0.00762957101687789\n",
      "Epoch: 350 - Loss: 0.006141025573015213\n",
      "Epoch: 400 - Loss: 0.005009731277823448\n",
      "Epoch: 450 - Loss: 0.0041318368166685104\n",
      "Epoch: 500 - Loss: 0.003444163827225566\n",
      "Epoch: 550 - Loss: 0.002893024357035756\n",
      "Epoch: 600 - Loss: 0.002445003017783165\n",
      "Epoch: 650 - Loss: 0.002078998601064086\n",
      "Epoch: 700 - Loss: 0.001778983511030674\n",
      "Epoch: 750 - Loss: 0.0015324020059779286\n",
      "Epoch: 800 - Loss: 0.0013261169660836458\n",
      "Epoch: 850 - Loss: 0.0011520483531057835\n",
      "Epoch: 900 - Loss: 0.0010040773777291179\n",
      "Epoch: 950 - Loss: 0.0008779268828220665\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.7433865070343018\n",
      "Epoch: 50 - Loss: 0.025048842653632164\n",
      "Epoch: 100 - Loss: 0.012630589306354523\n",
      "Epoch: 150 - Loss: 0.00775129022076726\n",
      "Epoch: 200 - Loss: 0.005174792837351561\n",
      "Epoch: 250 - Loss: 0.00366990570910275\n",
      "Epoch: 300 - Loss: 0.0027006985619664192\n",
      "Epoch: 350 - Loss: 0.0020485559944063425\n",
      "Epoch: 400 - Loss: 0.0015903424937278032\n",
      "Epoch: 450 - Loss: 0.0012559923343360424\n",
      "Epoch: 500 - Loss: 0.0010072155855596066\n",
      "Epoch: 550 - Loss: 0.0008183041936717927\n",
      "Epoch: 600 - Loss: 0.000672802038025111\n",
      "Epoch: 650 - Loss: 0.0005587374907918274\n",
      "Epoch: 700 - Loss: 0.0004687007167376578\n",
      "Epoch: 750 - Loss: 0.0003961829061154276\n",
      "Epoch: 800 - Loss: 0.00033747401903383434\n",
      "Epoch: 850 - Loss: 0.00028940243646502495\n",
      "Epoch: 900 - Loss: 0.000249955162871629\n",
      "Epoch: 950 - Loss: 0.00021714229660574347\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.021339774131775\n",
      "Epoch: 50 - Loss: 0.07001768052577972\n",
      "Epoch: 100 - Loss: 0.04243450239300728\n",
      "Epoch: 150 - Loss: 0.03020549565553665\n",
      "Epoch: 200 - Loss: 0.02299487590789795\n",
      "Epoch: 250 - Loss: 0.018177960067987442\n",
      "Epoch: 300 - Loss: 0.014753325842320919\n",
      "Epoch: 350 - Loss: 0.012227199971675873\n",
      "Epoch: 400 - Loss: 0.010313354432582855\n",
      "Epoch: 450 - Loss: 0.008809256367385387\n",
      "Epoch: 500 - Loss: 0.007594974245876074\n",
      "Epoch: 550 - Loss: 0.0066002365201711655\n",
      "Epoch: 600 - Loss: 0.005774859804660082\n",
      "Epoch: 650 - Loss: 0.005081033334136009\n",
      "Epoch: 700 - Loss: 0.004496475216001272\n",
      "Epoch: 750 - Loss: 0.003997056279331446\n",
      "Epoch: 800 - Loss: 0.0035668343771249056\n",
      "Epoch: 850 - Loss: 0.003193193580955267\n",
      "Epoch: 900 - Loss: 0.0028668437153100967\n",
      "Epoch: 950 - Loss: 0.002580358413979411\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.0943760871887207\n",
      "Epoch: 50 - Loss: 0.12284506112337112\n",
      "Epoch: 100 - Loss: 0.07712633162736893\n",
      "Epoch: 150 - Loss: 0.05617719143629074\n",
      "Epoch: 200 - Loss: 0.04366416484117508\n",
      "Epoch: 250 - Loss: 0.03532349318265915\n",
      "Epoch: 300 - Loss: 0.029315192252397537\n",
      "Epoch: 350 - Loss: 0.02478395402431488\n",
      "Epoch: 400 - Loss: 0.02127070724964142\n",
      "Epoch: 450 - Loss: 0.018472135066986084\n",
      "Epoch: 500 - Loss: 0.016188228502869606\n",
      "Epoch: 550 - Loss: 0.014298510737717152\n",
      "Epoch: 600 - Loss: 0.012707647867500782\n",
      "Epoch: 650 - Loss: 0.011356916278600693\n",
      "Epoch: 700 - Loss: 0.010206948965787888\n",
      "Epoch: 750 - Loss: 0.009210562333464622\n",
      "Epoch: 800 - Loss: 0.00834711268544197\n",
      "Epoch: 850 - Loss: 0.007592933252453804\n",
      "Epoch: 900 - Loss: 0.006931252311915159\n",
      "Epoch: 950 - Loss: 0.0063468096777796745\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.7591738700866699\n",
      "Epoch: 50 - Loss: 0.0348028689622879\n",
      "Epoch: 100 - Loss: 0.018899349495768547\n",
      "Epoch: 150 - Loss: 0.012170507572591305\n",
      "Epoch: 200 - Loss: 0.0084983566775918\n",
      "Epoch: 250 - Loss: 0.006243841722607613\n",
      "Epoch: 300 - Loss: 0.004757996648550034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 350 - Loss: 0.003715457860380411\n",
      "Epoch: 400 - Loss: 0.0029555174987763166\n",
      "Epoch: 450 - Loss: 0.002384962048381567\n",
      "Epoch: 500 - Loss: 0.0019513065926730633\n",
      "Epoch: 550 - Loss: 0.0016143402317538857\n",
      "Epoch: 600 - Loss: 0.0013483643997460604\n",
      "Epoch: 650 - Loss: 0.0011386633850634098\n",
      "Epoch: 700 - Loss: 0.0009683822863735259\n",
      "Epoch: 750 - Loss: 0.0008286248776130378\n",
      "Epoch: 800 - Loss: 0.000713776214979589\n",
      "Epoch: 850 - Loss: 0.0006181070348247886\n",
      "Epoch: 900 - Loss: 0.0005377389024943113\n",
      "Epoch: 950 - Loss: 0.00046978372847661376\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.0461411476135254\n",
      "Epoch: 50 - Loss: 0.049887966364622116\n",
      "Epoch: 100 - Loss: 0.027060270309448242\n",
      "Epoch: 150 - Loss: 0.01792936399579048\n",
      "Epoch: 200 - Loss: 0.012947792187333107\n",
      "Epoch: 250 - Loss: 0.009864257648587227\n",
      "Epoch: 300 - Loss: 0.007781837601214647\n",
      "Epoch: 350 - Loss: 0.006294539198279381\n",
      "Epoch: 400 - Loss: 0.0051876818761229515\n",
      "Epoch: 450 - Loss: 0.004336128011345863\n",
      "Epoch: 500 - Loss: 0.003663352457806468\n",
      "Epoch: 550 - Loss: 0.0031239911913871765\n",
      "Epoch: 600 - Loss: 0.002688588108867407\n",
      "Epoch: 650 - Loss: 0.0023321553599089384\n",
      "Epoch: 700 - Loss: 0.0020373810548335314\n",
      "Epoch: 750 - Loss: 0.0017905451823025942\n",
      "Epoch: 800 - Loss: 0.0015818856190890074\n",
      "Epoch: 850 - Loss: 0.0014035421190783381\n",
      "Epoch: 900 - Loss: 0.001250682631507516\n",
      "Epoch: 950 - Loss: 0.0011190917575731874\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.0849751234054565\n",
      "Epoch: 50 - Loss: 0.01874062977731228\n",
      "Epoch: 100 - Loss: 0.00884693581610918\n",
      "Epoch: 150 - Loss: 0.005179911851882935\n",
      "Epoch: 200 - Loss: 0.0033470806665718555\n",
      "Epoch: 250 - Loss: 0.0022819815203547478\n",
      "Epoch: 300 - Loss: 0.001617815694771707\n",
      "Epoch: 350 - Loss: 0.00118268805090338\n",
      "Epoch: 400 - Loss: 0.0008846468408592045\n",
      "Epoch: 450 - Loss: 0.0006744380807504058\n",
      "Epoch: 500 - Loss: 0.0005211714887991548\n",
      "Epoch: 550 - Loss: 0.00040726951556280255\n",
      "Epoch: 600 - Loss: 0.0003212312003597617\n",
      "Epoch: 650 - Loss: 0.00025540179922245443\n",
      "Epoch: 700 - Loss: 0.00020453961042221636\n",
      "Epoch: 750 - Loss: 0.00016491953283548355\n",
      "Epoch: 800 - Loss: 0.00013380007294472307\n",
      "Epoch: 850 - Loss: 0.00010908182593993843\n",
      "Epoch: 900 - Loss: 8.94419354153797e-05\n",
      "Epoch: 950 - Loss: 7.364648627117276e-05\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.6508102416992188\n",
      "Epoch: 50 - Loss: 0.04824703186750412\n",
      "Epoch: 100 - Loss: 0.025328055024147034\n",
      "Epoch: 150 - Loss: 0.016179917380213737\n",
      "Epoch: 200 - Loss: 0.011258798651397228\n",
      "Epoch: 250 - Loss: 0.008253372274339199\n",
      "Epoch: 300 - Loss: 0.006265586707741022\n",
      "Epoch: 350 - Loss: 0.0048770359717309475\n",
      "Epoch: 400 - Loss: 0.003866617102175951\n",
      "Epoch: 450 - Loss: 0.0031172907911241055\n",
      "Epoch: 500 - Loss: 0.0025396570563316345\n",
      "Epoch: 550 - Loss: 0.002088771900162101\n",
      "Epoch: 600 - Loss: 0.0017321030609309673\n",
      "Epoch: 650 - Loss: 0.001446927897632122\n",
      "Epoch: 700 - Loss: 0.0012167245149612427\n",
      "Epoch: 750 - Loss: 0.0010290651116520166\n",
      "Epoch: 800 - Loss: 0.0008750914130359888\n",
      "Epoch: 850 - Loss: 0.0007472163415513933\n",
      "Epoch: 900 - Loss: 0.0006406685570254922\n",
      "Epoch: 950 - Loss: 0.000552017823792994\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.9357651472091675\n",
      "Epoch: 50 - Loss: 0.09710530191659927\n",
      "Epoch: 100 - Loss: 0.05642816424369812\n",
      "Epoch: 150 - Loss: 0.03824710473418236\n",
      "Epoch: 200 - Loss: 0.027766037732362747\n",
      "Epoch: 250 - Loss: 0.0210702046751976\n",
      "Epoch: 300 - Loss: 0.016476841643452644\n",
      "Epoch: 350 - Loss: 0.013178639113903046\n",
      "Epoch: 400 - Loss: 0.010736347176134586\n",
      "Epoch: 450 - Loss: 0.00886103231459856\n",
      "Epoch: 500 - Loss: 0.007396087050437927\n",
      "Epoch: 550 - Loss: 0.006232445128262043\n",
      "Epoch: 600 - Loss: 0.005296579096466303\n",
      "Epoch: 650 - Loss: 0.004527650307863951\n",
      "Epoch: 700 - Loss: 0.0038938289508223534\n",
      "Epoch: 750 - Loss: 0.0033649771939963102\n",
      "Epoch: 800 - Loss: 0.002921472303569317\n",
      "Epoch: 850 - Loss: 0.002545636845752597\n",
      "Epoch: 900 - Loss: 0.0022253086790442467\n",
      "Epoch: 950 - Loss: 0.0019536642357707024\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.6799862384796143\n",
      "Epoch: 50 - Loss: 0.11808572709560394\n",
      "Epoch: 100 - Loss: 0.06158961355686188\n",
      "Epoch: 150 - Loss: 0.039366573095321655\n",
      "Epoch: 200 - Loss: 0.027856407687067986\n",
      "Epoch: 250 - Loss: 0.020892884582281113\n",
      "Epoch: 300 - Loss: 0.01631704717874527\n",
      "Epoch: 350 - Loss: 0.0131072998046875\n",
      "Epoch: 400 - Loss: 0.01073922123759985\n",
      "Epoch: 450 - Loss: 0.008940122090280056\n",
      "Epoch: 500 - Loss: 0.007551809772849083\n",
      "Epoch: 550 - Loss: 0.0064634764567017555\n",
      "Epoch: 600 - Loss: 0.005575255956500769\n",
      "Epoch: 650 - Loss: 0.004852405283600092\n",
      "Epoch: 700 - Loss: 0.004256511107087135\n",
      "Epoch: 750 - Loss: 0.003755192970857024\n",
      "Epoch: 800 - Loss: 0.0033322584349662066\n",
      "Epoch: 850 - Loss: 0.002970572793856263\n",
      "Epoch: 900 - Loss: 0.00265665166079998\n",
      "Epoch: 950 - Loss: 0.002383786253631115\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.1221957206726074\n",
      "Epoch: 50 - Loss: 0.09283512085676193\n",
      "Epoch: 100 - Loss: 0.05721832439303398\n",
      "Epoch: 150 - Loss: 0.040401890873909\n",
      "Epoch: 200 - Loss: 0.03037806786596775\n",
      "Epoch: 250 - Loss: 0.02381640486419201\n",
      "Epoch: 300 - Loss: 0.01924671232700348\n",
      "Epoch: 350 - Loss: 0.015901321545243263\n",
      "Epoch: 400 - Loss: 0.013354115188121796\n",
      "Epoch: 450 - Loss: 0.011375696398317814\n",
      "Epoch: 500 - Loss: 0.00978409219533205\n",
      "Epoch: 550 - Loss: 0.008504455909132957\n",
      "Epoch: 600 - Loss: 0.00745365209877491\n",
      "Epoch: 650 - Loss: 0.006581172812730074\n",
      "Epoch: 700 - Loss: 0.005842201877385378\n",
      "Epoch: 750 - Loss: 0.005215981975197792\n",
      "Epoch: 800 - Loss: 0.004681119695305824\n",
      "Epoch: 850 - Loss: 0.004220296628773212\n",
      "Epoch: 900 - Loss: 0.0038201031275093555\n",
      "Epoch: 950 - Loss: 0.003471456468105316\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.2331901788711548\n",
      "Epoch: 50 - Loss: 0.02951827459037304\n",
      "Epoch: 100 - Loss: 0.013863351196050644\n",
      "Epoch: 150 - Loss: 0.008297939784824848\n",
      "Epoch: 200 - Loss: 0.005564117804169655\n",
      "Epoch: 250 - Loss: 0.003993402235209942\n",
      "Epoch: 300 - Loss: 0.0029988884925842285\n",
      "Epoch: 350 - Loss: 0.0023388101253658533\n",
      "Epoch: 400 - Loss: 0.0018751355819404125\n",
      "Epoch: 450 - Loss: 0.0015330832684412599\n",
      "Epoch: 500 - Loss: 0.0012725102715194225\n",
      "Epoch: 550 - Loss: 0.0010686710011214018\n",
      "Epoch: 600 - Loss: 0.000907395500689745\n",
      "Epoch: 650 - Loss: 0.0007777483551762998\n",
      "Epoch: 700 - Loss: 0.0006720770034007728\n",
      "Epoch: 750 - Loss: 0.0005847469437867403\n",
      "Epoch: 800 - Loss: 0.0005122555303387344\n",
      "Epoch: 850 - Loss: 0.0004521190421655774\n",
      "Epoch: 900 - Loss: 0.00040121772326529026\n",
      "Epoch: 950 - Loss: 0.0003577137249521911\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.3011581897735596\n",
      "Epoch: 50 - Loss: 0.13965636491775513\n",
      "Epoch: 100 - Loss: 0.08411969244480133\n",
      "Epoch: 150 - Loss: 0.05955909192562103\n",
      "Epoch: 200 - Loss: 0.045387376099824905\n",
      "Epoch: 250 - Loss: 0.03616197034716606\n",
      "Epoch: 300 - Loss: 0.029774436727166176\n",
      "Epoch: 350 - Loss: 0.025066914036870003\n",
      "Epoch: 400 - Loss: 0.021436434239149094\n",
      "Epoch: 450 - Loss: 0.01855490729212761\n",
      "Epoch: 500 - Loss: 0.016216367483139038\n",
      "Epoch: 550 - Loss: 0.014291361905634403\n",
      "Epoch: 600 - Loss: 0.01269070990383625\n",
      "Epoch: 650 - Loss: 0.011345705948770046\n",
      "Epoch: 700 - Loss: 0.01019421499222517\n",
      "Epoch: 750 - Loss: 0.009209481999278069\n",
      "Epoch: 800 - Loss: 0.008355806581676006\n",
      "Epoch: 850 - Loss: 0.00761753274127841\n",
      "Epoch: 900 - Loss: 0.006972838658839464\n",
      "Epoch: 950 - Loss: 0.00640488788485527\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.9767674803733826\n",
      "Epoch: 50 - Loss: 0.10798580199480057\n",
      "Epoch: 100 - Loss: 0.06179187819361687\n",
      "Epoch: 150 - Loss: 0.04174009710550308\n",
      "Epoch: 200 - Loss: 0.030546586960554123\n",
      "Epoch: 250 - Loss: 0.023419050499796867\n",
      "Epoch: 300 - Loss: 0.01865461654961109\n",
      "Epoch: 350 - Loss: 0.01523781567811966\n",
      "Epoch: 400 - Loss: 0.012687363661825657\n",
      "Epoch: 450 - Loss: 0.010699528269469738\n",
      "Epoch: 500 - Loss: 0.00913342833518982\n",
      "Epoch: 550 - Loss: 0.007877125404775143\n",
      "Epoch: 600 - Loss: 0.006847174372524023\n",
      "Epoch: 650 - Loss: 0.0059899985790252686\n",
      "Epoch: 700 - Loss: 0.005277693271636963\n",
      "Epoch: 750 - Loss: 0.004674383904784918\n",
      "Epoch: 800 - Loss: 0.004158053081482649\n",
      "Epoch: 850 - Loss: 0.0037144809029996395\n",
      "Epoch: 900 - Loss: 0.0033282912336289883\n",
      "Epoch: 950 - Loss: 0.0029943648260086775\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.1682430505752563\n",
      "Epoch: 50 - Loss: 0.0902525782585144\n",
      "Epoch: 100 - Loss: 0.05002826079726219\n",
      "Epoch: 150 - Loss: 0.03314341977238655\n",
      "Epoch: 200 - Loss: 0.023929495364427567\n",
      "Epoch: 250 - Loss: 0.018232623115181923\n",
      "Epoch: 300 - Loss: 0.014409062452614307\n",
      "Epoch: 350 - Loss: 0.01169641874730587\n",
      "Epoch: 400 - Loss: 0.009666279889643192\n",
      "Epoch: 450 - Loss: 0.008100401610136032\n",
      "Epoch: 500 - Loss: 0.0068637351505458355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 550 - Loss: 0.005872558336704969\n",
      "Epoch: 600 - Loss: 0.00507056899368763\n",
      "Epoch: 650 - Loss: 0.004399806726723909\n",
      "Epoch: 700 - Loss: 0.0038365526124835014\n",
      "Epoch: 750 - Loss: 0.0033643280621618032\n",
      "Epoch: 800 - Loss: 0.0029611270874738693\n",
      "Epoch: 850 - Loss: 0.0026140848640352488\n",
      "Epoch: 900 - Loss: 0.002316240454092622\n",
      "Epoch: 950 - Loss: 0.0020605477038770914\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.5298782587051392\n",
      "Epoch: 50 - Loss: 0.053414419293403625\n",
      "Epoch: 100 - Loss: 0.027714626863598824\n",
      "Epoch: 150 - Loss: 0.01820148155093193\n",
      "Epoch: 200 - Loss: 0.013207389041781425\n",
      "Epoch: 250 - Loss: 0.010102933272719383\n",
      "Epoch: 300 - Loss: 0.00799767579883337\n",
      "Epoch: 350 - Loss: 0.006483563221991062\n",
      "Epoch: 400 - Loss: 0.005370852537453175\n",
      "Epoch: 450 - Loss: 0.004519835114479065\n",
      "Epoch: 500 - Loss: 0.0038504491094499826\n",
      "Epoch: 550 - Loss: 0.0033115672413259745\n",
      "Epoch: 600 - Loss: 0.0028710730839520693\n",
      "Epoch: 650 - Loss: 0.0025080807972699404\n",
      "Epoch: 700 - Loss: 0.0022050149273127317\n",
      "Epoch: 750 - Loss: 0.0019497284665703773\n",
      "Epoch: 800 - Loss: 0.001732589560560882\n",
      "Epoch: 850 - Loss: 0.0015469853533431888\n",
      "Epoch: 900 - Loss: 0.0013877454912289977\n",
      "Epoch: 950 - Loss: 0.001249953405931592\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.27974534034729\n",
      "Epoch: 50 - Loss: 0.14813977479934692\n",
      "Epoch: 100 - Loss: 0.08671516180038452\n",
      "Epoch: 150 - Loss: 0.059697043150663376\n",
      "Epoch: 200 - Loss: 0.04446205496788025\n",
      "Epoch: 250 - Loss: 0.03475738689303398\n",
      "Epoch: 300 - Loss: 0.027961932122707367\n",
      "Epoch: 350 - Loss: 0.022966044023633003\n",
      "Epoch: 400 - Loss: 0.01918565109372139\n",
      "Epoch: 450 - Loss: 0.016272593289613724\n",
      "Epoch: 500 - Loss: 0.013961846940219402\n",
      "Epoch: 550 - Loss: 0.012106694281101227\n",
      "Epoch: 600 - Loss: 0.010601951740682125\n",
      "Epoch: 650 - Loss: 0.009353338740766048\n",
      "Epoch: 700 - Loss: 0.008306753821671009\n",
      "Epoch: 750 - Loss: 0.007434792350977659\n",
      "Epoch: 800 - Loss: 0.006701028440147638\n",
      "Epoch: 850 - Loss: 0.00606858916580677\n",
      "Epoch: 900 - Loss: 0.0055185346864163876\n",
      "Epoch: 950 - Loss: 0.005038161762058735\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.9033657312393188\n",
      "Epoch: 50 - Loss: 0.06003982573747635\n",
      "Epoch: 100 - Loss: 0.03326725587248802\n",
      "Epoch: 150 - Loss: 0.022332539781928062\n",
      "Epoch: 200 - Loss: 0.016245240345597267\n",
      "Epoch: 250 - Loss: 0.012411759234964848\n",
      "Epoch: 300 - Loss: 0.00977289117872715\n",
      "Epoch: 350 - Loss: 0.007859251461923122\n",
      "Epoch: 400 - Loss: 0.00643910700455308\n",
      "Epoch: 450 - Loss: 0.005351727828383446\n",
      "Epoch: 500 - Loss: 0.004493245854973793\n",
      "Epoch: 550 - Loss: 0.003807835979387164\n",
      "Epoch: 600 - Loss: 0.0032509001903235912\n",
      "Epoch: 650 - Loss: 0.0027973069809377193\n",
      "Epoch: 700 - Loss: 0.002423070138320327\n",
      "Epoch: 750 - Loss: 0.0021112700924277306\n",
      "Epoch: 800 - Loss: 0.0018498270073905587\n",
      "Epoch: 850 - Loss: 0.001629383536055684\n",
      "Epoch: 900 - Loss: 0.0014405206311494112\n",
      "Epoch: 950 - Loss: 0.0012791386106982827\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.9898731708526611\n",
      "Epoch: 50 - Loss: 0.036852143704891205\n",
      "Epoch: 100 - Loss: 0.017363252118229866\n",
      "Epoch: 150 - Loss: 0.010639474727213383\n",
      "Epoch: 200 - Loss: 0.007258367724716663\n",
      "Epoch: 250 - Loss: 0.005258167162537575\n",
      "Epoch: 300 - Loss: 0.0039749545976519585\n",
      "Epoch: 350 - Loss: 0.0030999896116554737\n",
      "Epoch: 400 - Loss: 0.002468851860612631\n",
      "Epoch: 450 - Loss: 0.001999705797061324\n",
      "Epoch: 500 - Loss: 0.0016419392777606845\n",
      "Epoch: 550 - Loss: 0.0013638549717143178\n",
      "Epoch: 600 - Loss: 0.001144916983321309\n",
      "Epoch: 650 - Loss: 0.0009705330594442785\n",
      "Epoch: 700 - Loss: 0.0008306405507028103\n",
      "Epoch: 750 - Loss: 0.0007169866003096104\n",
      "Epoch: 800 - Loss: 0.0006237316410988569\n",
      "Epoch: 850 - Loss: 0.0005463894922286272\n",
      "Epoch: 900 - Loss: 0.0004814524145331234\n",
      "Epoch: 950 - Loss: 0.00042624055640771985\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.9852170348167419\n",
      "Epoch: 50 - Loss: 0.10647377371788025\n",
      "Epoch: 100 - Loss: 0.060542479157447815\n",
      "Epoch: 150 - Loss: 0.03980574384331703\n",
      "Epoch: 200 - Loss: 0.028305521234869957\n",
      "Epoch: 250 - Loss: 0.021236412227153778\n",
      "Epoch: 300 - Loss: 0.01652897521853447\n",
      "Epoch: 350 - Loss: 0.013223972171545029\n",
      "Epoch: 400 - Loss: 0.01082637533545494\n",
      "Epoch: 450 - Loss: 0.009029563516378403\n",
      "Epoch: 500 - Loss: 0.0076370458118617535\n",
      "Epoch: 550 - Loss: 0.006544094532728195\n",
      "Epoch: 600 - Loss: 0.005671899765729904\n",
      "Epoch: 650 - Loss: 0.00496712327003479\n",
      "Epoch: 700 - Loss: 0.00439115334302187\n",
      "Epoch: 750 - Loss: 0.00391099788248539\n",
      "Epoch: 800 - Loss: 0.0035079943481832743\n",
      "Epoch: 850 - Loss: 0.003165208036080003\n",
      "Epoch: 900 - Loss: 0.0028706896118819714\n",
      "Epoch: 950 - Loss: 0.00261535681784153\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.9116342067718506\n",
      "Epoch: 50 - Loss: 0.1054953932762146\n",
      "Epoch: 100 - Loss: 0.049268294125795364\n",
      "Epoch: 150 - Loss: 0.030018996447324753\n",
      "Epoch: 200 - Loss: 0.020553845912218094\n",
      "Epoch: 250 - Loss: 0.015053089708089828\n",
      "Epoch: 300 - Loss: 0.01149563118815422\n",
      "Epoch: 350 - Loss: 0.009065443649888039\n",
      "Epoch: 400 - Loss: 0.007312993984669447\n",
      "Epoch: 450 - Loss: 0.006005570758134127\n",
      "Epoch: 500 - Loss: 0.005012164823710918\n",
      "Epoch: 550 - Loss: 0.004239276051521301\n",
      "Epoch: 600 - Loss: 0.00362917548045516\n",
      "Epoch: 650 - Loss: 0.0031356485560536385\n",
      "Epoch: 700 - Loss: 0.0027329190634191036\n",
      "Epoch: 750 - Loss: 0.002400993602350354\n",
      "Epoch: 800 - Loss: 0.0021236080210655928\n",
      "Epoch: 850 - Loss: 0.0018904388416558504\n",
      "Epoch: 900 - Loss: 0.0016926095122471452\n",
      "Epoch: 950 - Loss: 0.0015231120632961392\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 4.259442329406738\n",
      "Epoch: 50 - Loss: 0.5619970560073853\n",
      "Epoch: 100 - Loss: 0.2986045777797699\n",
      "Epoch: 150 - Loss: 0.1932951807975769\n",
      "Epoch: 200 - Loss: 0.13966524600982666\n",
      "Epoch: 250 - Loss: 0.10811509937047958\n",
      "Epoch: 300 - Loss: 0.08737005293369293\n",
      "Epoch: 350 - Loss: 0.07251203805208206\n",
      "Epoch: 400 - Loss: 0.06133320927619934\n",
      "Epoch: 450 - Loss: 0.05266399681568146\n",
      "Epoch: 500 - Loss: 0.04580560326576233\n",
      "Epoch: 550 - Loss: 0.04024028405547142\n",
      "Epoch: 600 - Loss: 0.03566926345229149\n",
      "Epoch: 650 - Loss: 0.031870897859334946\n",
      "Epoch: 700 - Loss: 0.028674378991127014\n",
      "Epoch: 750 - Loss: 0.025941872969269753\n",
      "Epoch: 800 - Loss: 0.02362119033932686\n",
      "Epoch: 850 - Loss: 0.021627221256494522\n",
      "Epoch: 900 - Loss: 0.019894421100616455\n",
      "Epoch: 950 - Loss: 0.018362121656537056\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.026536226272583\n",
      "Epoch: 50 - Loss: 0.07885939627885818\n",
      "Epoch: 100 - Loss: 0.0455823615193367\n",
      "Epoch: 150 - Loss: 0.030797936022281647\n",
      "Epoch: 200 - Loss: 0.02251078188419342\n",
      "Epoch: 250 - Loss: 0.017291706055402756\n",
      "Epoch: 300 - Loss: 0.0136878015473485\n",
      "Epoch: 350 - Loss: 0.011089708656072617\n",
      "Epoch: 400 - Loss: 0.009151832200586796\n",
      "Epoch: 450 - Loss: 0.007656492758542299\n",
      "Epoch: 500 - Loss: 0.006479591131210327\n",
      "Epoch: 550 - Loss: 0.005539618898183107\n",
      "Epoch: 600 - Loss: 0.004774749744683504\n",
      "Epoch: 650 - Loss: 0.004149159416556358\n",
      "Epoch: 700 - Loss: 0.0036311799194663763\n",
      "Epoch: 750 - Loss: 0.003200321225449443\n",
      "Epoch: 800 - Loss: 0.002836292376741767\n",
      "Epoch: 850 - Loss: 0.0025255042128264904\n",
      "Epoch: 900 - Loss: 0.0022574583999812603\n",
      "Epoch: 950 - Loss: 0.0020282678306102753\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.8531501889228821\n",
      "Epoch: 50 - Loss: 0.044596463441848755\n",
      "Epoch: 100 - Loss: 0.02325838804244995\n",
      "Epoch: 150 - Loss: 0.014810126274824142\n",
      "Epoch: 200 - Loss: 0.010295161977410316\n",
      "Epoch: 250 - Loss: 0.007561892736703157\n",
      "Epoch: 300 - Loss: 0.005769041832536459\n",
      "Epoch: 350 - Loss: 0.004519543144851923\n",
      "Epoch: 400 - Loss: 0.0036158335860818624\n",
      "Epoch: 450 - Loss: 0.0029412058647722006\n",
      "Epoch: 500 - Loss: 0.002427160507068038\n",
      "Epoch: 550 - Loss: 0.002026532543823123\n",
      "Epoch: 600 - Loss: 0.001709447242319584\n",
      "Epoch: 650 - Loss: 0.001455338904634118\n",
      "Epoch: 700 - Loss: 0.0012501258170232177\n",
      "Epoch: 750 - Loss: 0.001083092880435288\n",
      "Epoch: 800 - Loss: 0.0009444100433029234\n",
      "Epoch: 850 - Loss: 0.0008282701019197702\n",
      "Epoch: 900 - Loss: 0.0007309261709451675\n",
      "Epoch: 950 - Loss: 0.0006483366596512496\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.9373425841331482\n",
      "Epoch: 50 - Loss: 0.023311521857976913\n",
      "Epoch: 100 - Loss: 0.01045138482004404\n",
      "Epoch: 150 - Loss: 0.005874084774404764\n",
      "Epoch: 200 - Loss: 0.0036784764379262924\n",
      "Epoch: 250 - Loss: 0.002455990295857191\n",
      "Epoch: 300 - Loss: 0.001714491518214345\n",
      "Epoch: 350 - Loss: 0.0012411229545250535\n",
      "Epoch: 400 - Loss: 0.0009254433098249137\n",
      "Epoch: 450 - Loss: 0.0007075101020745933\n",
      "Epoch: 500 - Loss: 0.0005520699196495116\n",
      "Epoch: 550 - Loss: 0.00043862327584065497\n",
      "Epoch: 600 - Loss: 0.0003546126827131957\n",
      "Epoch: 650 - Loss: 0.00029074811027385294\n",
      "Epoch: 700 - Loss: 0.00024164063506759703\n",
      "Epoch: 750 - Loss: 0.00020307805971242487\n",
      "Epoch: 800 - Loss: 0.00017243371985387057\n",
      "Epoch: 850 - Loss: 0.00014769670087844133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 900 - Loss: 0.00012757112563122064\n",
      "Epoch: 950 - Loss: 0.00011101752170361578\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.6216921806335449\n",
      "Epoch: 50 - Loss: 0.06174043193459511\n",
      "Epoch: 100 - Loss: 0.033363379538059235\n",
      "Epoch: 150 - Loss: 0.021533038467168808\n",
      "Epoch: 200 - Loss: 0.015095148235559464\n",
      "Epoch: 250 - Loss: 0.011135953478515148\n",
      "Epoch: 300 - Loss: 0.008528005331754684\n",
      "Epoch: 350 - Loss: 0.006703898310661316\n",
      "Epoch: 400 - Loss: 0.005381637718528509\n",
      "Epoch: 450 - Loss: 0.004395617637783289\n",
      "Epoch: 500 - Loss: 0.003639297094196081\n",
      "Epoch: 550 - Loss: 0.003047462785616517\n",
      "Epoch: 600 - Loss: 0.002578070154413581\n",
      "Epoch: 650 - Loss: 0.0022005753125995398\n",
      "Epoch: 700 - Loss: 0.0018944703042507172\n",
      "Epoch: 750 - Loss: 0.0016444738721475005\n",
      "Epoch: 800 - Loss: 0.0014378251507878304\n",
      "Epoch: 850 - Loss: 0.0012647180119529366\n",
      "Epoch: 900 - Loss: 0.0011185541516169906\n",
      "Epoch: 950 - Loss: 0.0009947885992005467\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.3897671699523926\n",
      "Epoch: 50 - Loss: 0.08858039230108261\n",
      "Epoch: 100 - Loss: 0.05367326736450195\n",
      "Epoch: 150 - Loss: 0.03823893889784813\n",
      "Epoch: 200 - Loss: 0.02918052114546299\n",
      "Epoch: 250 - Loss: 0.02324894815683365\n",
      "Epoch: 300 - Loss: 0.019010497257113457\n",
      "Epoch: 350 - Loss: 0.01584433764219284\n",
      "Epoch: 400 - Loss: 0.013406152836978436\n",
      "Epoch: 450 - Loss: 0.011500325985252857\n",
      "Epoch: 500 - Loss: 0.009969435632228851\n",
      "Epoch: 550 - Loss: 0.00872043240815401\n",
      "Epoch: 600 - Loss: 0.007682629860937595\n",
      "Epoch: 650 - Loss: 0.006813437212258577\n",
      "Epoch: 700 - Loss: 0.006083662621676922\n",
      "Epoch: 750 - Loss: 0.005455521866679192\n",
      "Epoch: 800 - Loss: 0.0049132187850773335\n",
      "Epoch: 850 - Loss: 0.004441563040018082\n",
      "Epoch: 900 - Loss: 0.004027297720313072\n",
      "Epoch: 950 - Loss: 0.0036607738584280014\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.8425377011299133\n",
      "Epoch: 50 - Loss: 0.04052651301026344\n",
      "Epoch: 100 - Loss: 0.021115416660904884\n",
      "Epoch: 150 - Loss: 0.013275288045406342\n",
      "Epoch: 200 - Loss: 0.009144552052021027\n",
      "Epoch: 250 - Loss: 0.0066585480235517025\n",
      "Epoch: 300 - Loss: 0.005021789111196995\n",
      "Epoch: 350 - Loss: 0.0038757144939154387\n",
      "Epoch: 400 - Loss: 0.003045465098693967\n",
      "Epoch: 450 - Loss: 0.0024315740447491407\n",
      "Epoch: 500 - Loss: 0.0019653234630823135\n",
      "Epoch: 550 - Loss: 0.0016062766080722213\n",
      "Epoch: 600 - Loss: 0.0013247983297333121\n",
      "Epoch: 650 - Loss: 0.0011001824168488383\n",
      "Epoch: 700 - Loss: 0.0009202056098729372\n",
      "Epoch: 750 - Loss: 0.0007748471689410508\n",
      "Epoch: 800 - Loss: 0.0006562849157489836\n",
      "Epoch: 850 - Loss: 0.0005588663043454289\n",
      "Epoch: 900 - Loss: 0.00047806455404497683\n",
      "Epoch: 950 - Loss: 0.0004109121218789369\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.9187240600585938\n",
      "Epoch: 50 - Loss: 0.03177035227417946\n",
      "Epoch: 100 - Loss: 0.016054904088377953\n",
      "Epoch: 150 - Loss: 0.009975132532417774\n",
      "Epoch: 200 - Loss: 0.006787542253732681\n",
      "Epoch: 250 - Loss: 0.004864714574068785\n",
      "Epoch: 300 - Loss: 0.003625494195148349\n",
      "Epoch: 350 - Loss: 0.0027963798493146896\n",
      "Epoch: 400 - Loss: 0.0022071271669119596\n",
      "Epoch: 450 - Loss: 0.001780375256203115\n",
      "Epoch: 500 - Loss: 0.0014648359501734376\n",
      "Epoch: 550 - Loss: 0.001225530169904232\n",
      "Epoch: 600 - Loss: 0.0010405110660940409\n",
      "Epoch: 650 - Loss: 0.0008954724762588739\n",
      "Epoch: 700 - Loss: 0.0007797376019880176\n",
      "Epoch: 750 - Loss: 0.0006847467157058418\n",
      "Epoch: 800 - Loss: 0.0006061243475414813\n",
      "Epoch: 850 - Loss: 0.0005405748379416764\n",
      "Epoch: 900 - Loss: 0.0004852024430874735\n",
      "Epoch: 950 - Loss: 0.00043778918916359544\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.9063105583190918\n",
      "Epoch: 50 - Loss: 0.0461534708738327\n",
      "Epoch: 100 - Loss: 0.025301028043031693\n",
      "Epoch: 150 - Loss: 0.016332855448126793\n",
      "Epoch: 200 - Loss: 0.011398345232009888\n",
      "Epoch: 250 - Loss: 0.008323105983436108\n",
      "Epoch: 300 - Loss: 0.006269033998250961\n",
      "Epoch: 350 - Loss: 0.004830008838325739\n",
      "Epoch: 400 - Loss: 0.0037828676868230104\n",
      "Epoch: 450 - Loss: 0.0030005418229848146\n",
      "Epoch: 500 - Loss: 0.002406843239441514\n",
      "Epoch: 550 - Loss: 0.0019540523644536734\n",
      "Epoch: 600 - Loss: 0.0016024664510041475\n",
      "Epoch: 650 - Loss: 0.0013252794742584229\n",
      "Epoch: 700 - Loss: 0.0011036820942535996\n",
      "Epoch: 750 - Loss: 0.000924448948353529\n",
      "Epoch: 800 - Loss: 0.0007780832820571959\n",
      "Epoch: 850 - Loss: 0.0006581348134204745\n",
      "Epoch: 900 - Loss: 0.000559041160158813\n",
      "Epoch: 950 - Loss: 0.00047666887985542417\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.2471717596054077\n",
      "Epoch: 50 - Loss: 0.040610767900943756\n",
      "Epoch: 100 - Loss: 0.019679540768265724\n",
      "Epoch: 150 - Loss: 0.012059485539793968\n",
      "Epoch: 200 - Loss: 0.00815565139055252\n",
      "Epoch: 250 - Loss: 0.005830260459333658\n",
      "Epoch: 300 - Loss: 0.004331063479185104\n",
      "Epoch: 350 - Loss: 0.0032962453551590443\n",
      "Epoch: 400 - Loss: 0.0025586960837244987\n",
      "Epoch: 450 - Loss: 0.0020194100216031075\n",
      "Epoch: 500 - Loss: 0.0016174549236893654\n",
      "Epoch: 550 - Loss: 0.0013113090535625815\n",
      "Epoch: 600 - Loss: 0.0010734089883044362\n",
      "Epoch: 650 - Loss: 0.0008878913358785212\n",
      "Epoch: 700 - Loss: 0.0007410338730551302\n",
      "Epoch: 750 - Loss: 0.0006245109252631664\n",
      "Epoch: 800 - Loss: 0.0005298791802488267\n",
      "Epoch: 850 - Loss: 0.0004528491699602455\n",
      "Epoch: 900 - Loss: 0.00038949697045609355\n",
      "Epoch: 950 - Loss: 0.00033693856676109135\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.8749516010284424\n",
      "Epoch: 50 - Loss: 0.02640785649418831\n",
      "Epoch: 100 - Loss: 0.01159194391220808\n",
      "Epoch: 150 - Loss: 0.0064147901721298695\n",
      "Epoch: 200 - Loss: 0.003926383331418037\n",
      "Epoch: 250 - Loss: 0.0025552203878760338\n",
      "Epoch: 300 - Loss: 0.0017311044503003359\n",
      "Epoch: 350 - Loss: 0.0012082061730325222\n",
      "Epoch: 400 - Loss: 0.0008630675147287548\n",
      "Epoch: 450 - Loss: 0.0006275868508964777\n",
      "Epoch: 500 - Loss: 0.0004639394173864275\n",
      "Epoch: 550 - Loss: 0.0003486009663902223\n",
      "Epoch: 600 - Loss: 0.0002661848848219961\n",
      "Epoch: 650 - Loss: 0.0002056027587968856\n",
      "Epoch: 700 - Loss: 0.00016033709107432514\n",
      "Epoch: 750 - Loss: 0.00012615183368325233\n",
      "Epoch: 800 - Loss: 0.0001001152049866505\n",
      "Epoch: 850 - Loss: 8.006890129763633e-05\n",
      "Epoch: 900 - Loss: 6.453288369812071e-05\n",
      "Epoch: 950 - Loss: 5.2438306738622487e-05\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.5509413480758667\n",
      "Epoch: 50 - Loss: 0.3215028643608093\n",
      "Epoch: 100 - Loss: 0.21267704665660858\n",
      "Epoch: 150 - Loss: 0.1592673510313034\n",
      "Epoch: 200 - Loss: 0.12609724700450897\n",
      "Epoch: 250 - Loss: 0.1033436506986618\n",
      "Epoch: 300 - Loss: 0.08685891330242157\n",
      "Epoch: 350 - Loss: 0.07437731325626373\n",
      "Epoch: 400 - Loss: 0.0645124614238739\n",
      "Epoch: 450 - Loss: 0.05654863268136978\n",
      "Epoch: 500 - Loss: 0.05002559721469879\n",
      "Epoch: 550 - Loss: 0.04460126906633377\n",
      "Epoch: 600 - Loss: 0.04001285880804062\n",
      "Epoch: 650 - Loss: 0.036109838634729385\n",
      "Epoch: 700 - Loss: 0.032736845314502716\n",
      "Epoch: 750 - Loss: 0.02978634275496006\n",
      "Epoch: 800 - Loss: 0.027193019166588783\n",
      "Epoch: 850 - Loss: 0.02491515874862671\n",
      "Epoch: 900 - Loss: 0.02289796620607376\n",
      "Epoch: 950 - Loss: 0.021111343055963516\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.1870461702346802\n",
      "Epoch: 50 - Loss: 0.043920569121837616\n",
      "Epoch: 100 - Loss: 0.020444367080926895\n",
      "Epoch: 150 - Loss: 0.011945481412112713\n",
      "Epoch: 200 - Loss: 0.007787080481648445\n",
      "Epoch: 250 - Loss: 0.005429647397249937\n",
      "Epoch: 300 - Loss: 0.003959668334573507\n",
      "Epoch: 350 - Loss: 0.0029843654483556747\n",
      "Epoch: 400 - Loss: 0.0023030347656458616\n",
      "Epoch: 450 - Loss: 0.001811354304663837\n",
      "Epoch: 500 - Loss: 0.0014497969532385468\n",
      "Epoch: 550 - Loss: 0.0011786106042563915\n",
      "Epoch: 600 - Loss: 0.0009725630516186357\n",
      "Epoch: 650 - Loss: 0.000810501049272716\n",
      "Epoch: 700 - Loss: 0.0006816870882175863\n",
      "Epoch: 750 - Loss: 0.0005780454957857728\n",
      "Epoch: 800 - Loss: 0.0004940278013236821\n",
      "Epoch: 850 - Loss: 0.0004251196514815092\n",
      "Epoch: 900 - Loss: 0.0003679186920635402\n",
      "Epoch: 950 - Loss: 0.0003203062806278467\n"
     ]
    }
   ],
   "source": [
    "pr, re = [[],[],[]], [[],[],[]]\n",
    "ndcgs = defaultdict(list)\n",
    "\n",
    "lr = 0.001\n",
    "epochs = 1000\n",
    "constraintHigh=1\n",
    "constraintLow=0\n",
    "# constraintHigh=float('inf')\n",
    "# constraintLow=-float('inf')\n",
    "loss_weight = [1, 0, 0]\n",
    "L1, L2 = 0, 0\n",
    "rand_type = 0\n",
    "\n",
    "total_mul = 1\n",
    "\n",
    "for uid, uemb in enumerate(tqdm(doc_embs[:50])):\n",
    "    x = word_embs.T\n",
    "    y = uemb\n",
    "    total = len(doc_answers[uid])\n",
    "    total = 1\n",
    "    \n",
    "    torch_model = PyTorchLinearRegression(x.shape[1], lr, constraintHigh, constraintLow, int(total*total_mul), rand_type, L1, L2)\n",
    "    torch_model.fit(x, y, epochs)\n",
    "    \n",
    "    m1 = metric4(torch_model.w.detach().numpy(), doc_answers[uid], w_idx=None, topk=50, verbose=0)\n",
    "    m2 = metric4(torch_model.w.detach().numpy(), doc_answers[uid], w_idx=None, topk=100, verbose=0)\n",
    "    m3 = metric4(torch_model.w.detach().numpy(), doc_answers[uid], w_idx=None, topk=200, verbose=0)\n",
    "    ndcg1 = metric_ndcg2(torch_model.w.detach().numpy(), tfidf_ans, uid, topk=50, verbose=0)\n",
    "    ndcg2 = metric_ndcg2(torch_model.w.detach().numpy(), tfidf_ans, uid, topk=100, verbose=0)\n",
    "    ndcg3 = metric_ndcg2(torch_model.w.detach().numpy(), tfidf_ans, uid, topk=200, verbose=0)\n",
    "    ndcg4 = metric_ndcg2(torch_model.w.detach().numpy(), tfidf_ans, uid, topk=None, verbose=0)\n",
    "    pr[0].append(m1[\"precision\"])\n",
    "    re[0].append(m1[\"recall\"])\n",
    "    pr[1].append(m2[\"precision\"])\n",
    "    re[1].append(m2[\"recall\"])\n",
    "    pr[2].append(m3[\"precision\"])\n",
    "    re[2].append(m3[\"recall\"])\n",
    "    \n",
    "    ndcgs[\"50\"].append(ndcg1)\n",
    "    ndcgs[\"100\"].append(ndcg2)\n",
    "    ndcgs[\"200\"].append(ndcg3)\n",
    "    ndcgs[\"-1\"].append(ndcg4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "684ddca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.1588 Recall:0.3502\n",
      "Precision:0.1090 Recall:0.4433\n",
      "Precision:0.0715 Recall:0.5457\n",
      "NDCG 50:0.5243\n",
      "NDCG 100:0.5549\n",
      "NDCG 200:0.5870\n",
      "NDCG all:0.6963\n"
     ]
    }
   ],
   "source": [
    "print(f\"Precision:{np.mean(pr[0]):.4f} Recall:{np.mean(re[0]):.4f}\")\n",
    "print(f\"Precision:{np.mean(pr[1]):.4f} Recall:{np.mean(re[1]):.4f}\")\n",
    "print(f\"Precision:{np.mean(pr[2]):.4f} Recall:{np.mean(re[2]):.4f}\")\n",
    "print(f\"NDCG 50:{np.mean(ndcgs['50']):.4f}\")\n",
    "print(f\"NDCG 100:{np.mean(ndcgs['100']):.4f}\")\n",
    "print(f\"NDCG 200:{np.mean(ndcgs['200']):.4f}\")\n",
    "print(f\"NDCG all:{np.mean(ndcgs['-1']):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "486b8282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "081db89e72a445e2b6ce220f6765209e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.1165862083435059\n",
      "Epoch: 50 - Loss: 0.08359394967556\n",
      "Epoch: 100 - Loss: 0.01915481872856617\n",
      "Epoch: 150 - Loss: 0.005619284696877003\n",
      "Epoch: 200 - Loss: 0.0020497646182775497\n",
      "Epoch: 250 - Loss: 0.0008976616081781685\n",
      "Epoch: 300 - Loss: 0.0004491099971346557\n",
      "Epoch: 350 - Loss: 0.00024542250321246684\n",
      "Epoch: 400 - Loss: 0.00014207989443093538\n",
      "Epoch: 450 - Loss: 8.557946421205997e-05\n",
      "Epoch: 500 - Loss: 5.3093506721779704e-05\n",
      "Epoch: 550 - Loss: 3.37350538757164e-05\n",
      "Epoch: 600 - Loss: 2.1878406187170185e-05\n",
      "Epoch: 650 - Loss: 1.445093766960781e-05\n",
      "Epoch: 700 - Loss: 9.705948286864441e-06\n",
      "Epoch: 750 - Loss: 6.620859494432807e-06\n",
      "Epoch: 800 - Loss: 4.581941539072432e-06\n",
      "Epoch: 850 - Loss: 3.2145687782758614e-06\n",
      "Epoch: 900 - Loss: 2.283843969053123e-06\n",
      "Epoch: 950 - Loss: 1.642202505536261e-06\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.0777878761291504\n",
      "Epoch: 50 - Loss: 0.020229818299412727\n",
      "Epoch: 100 - Loss: 0.004844055511057377\n",
      "Epoch: 150 - Loss: 0.0015648123808205128\n",
      "Epoch: 200 - Loss: 0.0006461175507865846\n",
      "Epoch: 250 - Loss: 0.0003262337704654783\n",
      "Epoch: 300 - Loss: 0.00019014834833797067\n",
      "Epoch: 350 - Loss: 0.00012170027912361547\n",
      "Epoch: 400 - Loss: 8.273614366771653e-05\n",
      "Epoch: 450 - Loss: 5.858067379449494e-05\n",
      "Epoch: 500 - Loss: 4.271741636330262e-05\n",
      "Epoch: 550 - Loss: 3.187032780260779e-05\n",
      "Epoch: 600 - Loss: 2.4230774215538986e-05\n",
      "Epoch: 650 - Loss: 1.872045686468482e-05\n",
      "Epoch: 700 - Loss: 1.4667581126559526e-05\n",
      "Epoch: 750 - Loss: 1.1635464943537954e-05\n",
      "Epoch: 800 - Loss: 9.331468390882947e-06\n",
      "Epoch: 850 - Loss: 7.556028322142083e-06\n",
      "Epoch: 900 - Loss: 6.170418146211887e-06\n",
      "Epoch: 950 - Loss: 5.076191882835701e-06\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.9535503387451172\n",
      "Epoch: 50 - Loss: 0.04865198954939842\n",
      "Epoch: 100 - Loss: 0.015009657479822636\n",
      "Epoch: 150 - Loss: 0.0064898282289505005\n",
      "Epoch: 200 - Loss: 0.0033775640185922384\n",
      "Epoch: 250 - Loss: 0.0019571229349821806\n",
      "Epoch: 300 - Loss: 0.0012146681547164917\n",
      "Epoch: 350 - Loss: 0.0007927567348815501\n",
      "Epoch: 400 - Loss: 0.0005390705191530287\n",
      "Epoch: 450 - Loss: 0.0003798471880145371\n",
      "Epoch: 500 - Loss: 0.00027622305788099766\n",
      "Epoch: 550 - Loss: 0.0002065387525362894\n",
      "Epoch: 600 - Loss: 0.00015822747081983835\n",
      "Epoch: 650 - Loss: 0.00012376235099509358\n",
      "Epoch: 700 - Loss: 9.851233335211873e-05\n",
      "Epoch: 750 - Loss: 7.955869659781456e-05\n",
      "Epoch: 800 - Loss: 6.50171423330903e-05\n",
      "Epoch: 850 - Loss: 5.3646341257262975e-05\n",
      "Epoch: 900 - Loss: 4.460594936972484e-05\n",
      "Epoch: 950 - Loss: 3.7320172850741073e-05\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.8551814556121826\n",
      "Epoch: 50 - Loss: 0.035436782985925674\n",
      "Epoch: 100 - Loss: 0.009696279652416706\n",
      "Epoch: 150 - Loss: 0.0034330138005316257\n",
      "Epoch: 200 - Loss: 0.0014981745043769479\n",
      "Epoch: 250 - Loss: 0.0007751785451546311\n",
      "Epoch: 300 - Loss: 0.00045581068843603134\n",
      "Epoch: 350 - Loss: 0.00029311678372323513\n",
      "Epoch: 400 - Loss: 0.00020028003200422972\n",
      "Epoch: 450 - Loss: 0.00014263087359722704\n",
      "Epoch: 500 - Loss: 0.00010458883480168879\n",
      "Epoch: 550 - Loss: 7.83696596045047e-05\n",
      "Epoch: 600 - Loss: 5.971781865810044e-05\n",
      "Epoch: 650 - Loss: 4.6128825488267466e-05\n",
      "Epoch: 700 - Loss: 3.60399681085255e-05\n",
      "Epoch: 750 - Loss: 2.843306901922915e-05\n",
      "Epoch: 800 - Loss: 2.2623687982559204e-05\n",
      "Epoch: 850 - Loss: 1.8136224753106944e-05\n",
      "Epoch: 900 - Loss: 1.4636449122917838e-05\n",
      "Epoch: 950 - Loss: 1.1882429134857375e-05\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.1088433265686035\n",
      "Epoch: 50 - Loss: 0.05007685720920563\n",
      "Epoch: 100 - Loss: 0.014016307890415192\n",
      "Epoch: 150 - Loss: 0.00516967847943306\n",
      "Epoch: 200 - Loss: 0.0022947117686271667\n",
      "Epoch: 250 - Loss: 0.0011598523706197739\n",
      "Epoch: 300 - Loss: 0.0006416187388822436\n",
      "Epoch: 350 - Loss: 0.00037868169602006674\n",
      "Epoch: 400 - Loss: 0.00023468300059903413\n",
      "Epoch: 450 - Loss: 0.00015116161375772208\n",
      "Epoch: 500 - Loss: 0.00010049662523670122\n",
      "Epoch: 550 - Loss: 6.863124144729227e-05\n",
      "Epoch: 600 - Loss: 4.798249210580252e-05\n",
      "Epoch: 650 - Loss: 3.426311377552338e-05\n",
      "Epoch: 700 - Loss: 2.4945386030594818e-05\n",
      "Epoch: 750 - Loss: 1.8493230527383275e-05\n",
      "Epoch: 800 - Loss: 1.39449184644036e-05\n",
      "Epoch: 850 - Loss: 1.0683436812541913e-05\n",
      "Epoch: 900 - Loss: 8.306761628773529e-06\n",
      "Epoch: 950 - Loss: 6.547216798935551e-06\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.0367999076843262\n",
      "Epoch: 50 - Loss: 0.03918088600039482\n",
      "Epoch: 100 - Loss: 0.012046785093843937\n",
      "Epoch: 150 - Loss: 0.004881587810814381\n",
      "Epoch: 200 - Loss: 0.002351297065615654\n",
      "Epoch: 250 - Loss: 0.00126645399723202\n",
      "Epoch: 300 - Loss: 0.0007370907696895301\n",
      "Epoch: 350 - Loss: 0.0004543545946944505\n",
      "Epoch: 400 - Loss: 0.00029292170074768364\n",
      "Epoch: 450 - Loss: 0.0001958703069249168\n",
      "Epoch: 500 - Loss: 0.00013507745461538434\n",
      "Epoch: 550 - Loss: 9.568370296619833e-05\n",
      "Epoch: 600 - Loss: 6.941781612113118e-05\n",
      "Epoch: 650 - Loss: 5.145786781213246e-05\n",
      "Epoch: 700 - Loss: 3.8896705518709496e-05\n",
      "Epoch: 750 - Loss: 2.992572080984246e-05\n",
      "Epoch: 800 - Loss: 2.339068305445835e-05\n",
      "Epoch: 850 - Loss: 1.8541040844866075e-05\n",
      "Epoch: 900 - Loss: 1.4878507499815896e-05\n",
      "Epoch: 950 - Loss: 1.2067135685356334e-05\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.0828191041946411\n",
      "Epoch: 50 - Loss: 0.038675423711538315\n",
      "Epoch: 100 - Loss: 0.011498013511300087\n",
      "Epoch: 150 - Loss: 0.004184448625892401\n",
      "Epoch: 200 - Loss: 0.001778275240212679\n",
      "Epoch: 250 - Loss: 0.0008531181374564767\n",
      "Epoch: 300 - Loss: 0.0004487775731831789\n",
      "Epoch: 350 - Loss: 0.0002528977347537875\n",
      "Epoch: 400 - Loss: 0.00015000547864474356\n",
      "Epoch: 450 - Loss: 9.245055844075978e-05\n",
      "Epoch: 500 - Loss: 5.865822095074691e-05\n",
      "Epoch: 550 - Loss: 3.8065369153628126e-05\n",
      "Epoch: 600 - Loss: 2.5154853574349545e-05\n",
      "Epoch: 650 - Loss: 1.6878810129128397e-05\n",
      "Epoch: 700 - Loss: 1.1479738532216288e-05\n",
      "Epoch: 750 - Loss: 7.90679951023776e-06\n",
      "Epoch: 800 - Loss: 5.513897576747695e-06\n",
      "Epoch: 850 - Loss: 3.8932075767661445e-06\n",
      "Epoch: 900 - Loss: 2.783502850434161e-06\n",
      "Epoch: 950 - Loss: 2.016053258557804e-06\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.7859926819801331\n",
      "Epoch: 50 - Loss: 0.017302488908171654\n",
      "Epoch: 100 - Loss: 0.0055086673237383366\n",
      "Epoch: 150 - Loss: 0.0023593187797814608\n",
      "Epoch: 200 - Loss: 0.0012318554800003767\n",
      "Epoch: 250 - Loss: 0.0007269140332937241\n",
      "Epoch: 300 - Loss: 0.0004612223128788173\n",
      "Epoch: 350 - Loss: 0.0003061088209506124\n",
      "Epoch: 400 - Loss: 0.00020949382451362908\n",
      "Epoch: 450 - Loss: 0.00014674027625005692\n",
      "Epoch: 500 - Loss: 0.00010476280294824392\n",
      "Epoch: 550 - Loss: 7.604081474710256e-05\n",
      "Epoch: 600 - Loss: 5.6019402109086514e-05\n",
      "Epoch: 650 - Loss: 4.183482451480813e-05\n",
      "Epoch: 700 - Loss: 3.163696965202689e-05\n",
      "Epoch: 750 - Loss: 2.4204528017435223e-05\n",
      "Epoch: 800 - Loss: 1.871942549769301e-05\n",
      "Epoch: 850 - Loss: 1.4622567505284678e-05\n",
      "Epoch: 900 - Loss: 1.1527399692567997e-05\n",
      "Epoch: 950 - Loss: 9.164808034256566e-06\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.7265727519989014\n",
      "Epoch: 50 - Loss: 0.014283022843301296\n",
      "Epoch: 100 - Loss: 0.004439973272383213\n",
      "Epoch: 150 - Loss: 0.0019311991054564714\n",
      "Epoch: 200 - Loss: 0.0010177218355238438\n",
      "Epoch: 250 - Loss: 0.0005988154443912208\n",
      "Epoch: 300 - Loss: 0.00037613327731378376\n",
      "Epoch: 350 - Loss: 0.00024634774308651686\n",
      "Epoch: 400 - Loss: 0.0001661809510551393\n",
      "Epoch: 450 - Loss: 0.00011471063044155017\n",
      "Epoch: 500 - Loss: 8.074016659520566e-05\n",
      "Epoch: 550 - Loss: 5.7830940932035446e-05\n",
      "Epoch: 600 - Loss: 4.2098537960555404e-05\n",
      "Epoch: 650 - Loss: 3.1116851459955797e-05\n",
      "Epoch: 700 - Loss: 2.3334752768278122e-05\n",
      "Epoch: 750 - Loss: 1.7739566828822717e-05\n",
      "Epoch: 800 - Loss: 1.3658902389579453e-05\n",
      "Epoch: 850 - Loss: 1.0643188943504356e-05\n",
      "Epoch: 900 - Loss: 8.38453161122743e-06\n",
      "Epoch: 950 - Loss: 6.671362370980205e-06\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.8648017048835754\n",
      "Epoch: 50 - Loss: 0.018110500648617744\n",
      "Epoch: 100 - Loss: 0.005911948624998331\n",
      "Epoch: 150 - Loss: 0.0026797314640134573\n",
      "Epoch: 200 - Loss: 0.0014251342508941889\n",
      "Epoch: 250 - Loss: 0.0008203540928661823\n",
      "Epoch: 300 - Loss: 0.0004936099285259843\n",
      "Epoch: 350 - Loss: 0.0003060902236029506\n",
      "Epoch: 400 - Loss: 0.00019464021897874773\n",
      "Epoch: 450 - Loss: 0.00012681385851465166\n",
      "Epoch: 500 - Loss: 8.473871275782585e-05\n",
      "Epoch: 550 - Loss: 5.816282646264881e-05\n",
      "Epoch: 600 - Loss: 4.105401239939965e-05\n",
      "Epoch: 650 - Loss: 2.9807244573021308e-05\n",
      "Epoch: 700 - Loss: 2.2240699763642624e-05\n",
      "Epoch: 750 - Loss: 1.7019312508637086e-05\n",
      "Epoch: 800 - Loss: 1.3319181562110316e-05\n",
      "Epoch: 850 - Loss: 1.0623737580317538e-05\n",
      "Epoch: 900 - Loss: 8.608702046331018e-06\n",
      "Epoch: 950 - Loss: 7.065194495226024e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 2.8218798637390137\n",
      "Epoch: 50 - Loss: 0.1924891620874405\n",
      "Epoch: 100 - Loss: 0.05128239467740059\n",
      "Epoch: 150 - Loss: 0.018226072192192078\n",
      "Epoch: 200 - Loss: 0.008114166557788849\n",
      "Epoch: 250 - Loss: 0.004279622808098793\n",
      "Epoch: 300 - Loss: 0.002549166791141033\n",
      "Epoch: 350 - Loss: 0.0016552986344322562\n",
      "Epoch: 400 - Loss: 0.0011435889173299074\n",
      "Epoch: 450 - Loss: 0.0008269208483397961\n",
      "Epoch: 500 - Loss: 0.0006188872503116727\n",
      "Epoch: 550 - Loss: 0.00047571948380209506\n",
      "Epoch: 600 - Loss: 0.00037344868178479373\n",
      "Epoch: 650 - Loss: 0.00029813972651027143\n",
      "Epoch: 700 - Loss: 0.00024126424978021532\n",
      "Epoch: 750 - Loss: 0.00019738548144232482\n",
      "Epoch: 800 - Loss: 0.00016292092914227396\n",
      "Epoch: 850 - Loss: 0.0001354351988993585\n",
      "Epoch: 900 - Loss: 0.00011324338265694678\n",
      "Epoch: 950 - Loss: 9.51277106651105e-05\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.9688442945480347\n",
      "Epoch: 50 - Loss: 0.019530154764652252\n",
      "Epoch: 100 - Loss: 0.005650130100548267\n",
      "Epoch: 150 - Loss: 0.002239508554339409\n",
      "Epoch: 200 - Loss: 0.0010497733019292355\n",
      "Epoch: 250 - Loss: 0.0005431401077657938\n",
      "Epoch: 300 - Loss: 0.00029920387896709144\n",
      "Epoch: 350 - Loss: 0.00017214905528817326\n",
      "Epoch: 400 - Loss: 0.00010233516513835639\n",
      "Epoch: 450 - Loss: 6.244159158086404e-05\n",
      "Epoch: 500 - Loss: 3.894169276463799e-05\n",
      "Epoch: 550 - Loss: 2.474910070304759e-05\n",
      "Epoch: 600 - Loss: 1.599486859049648e-05\n",
      "Epoch: 650 - Loss: 1.0496939466975164e-05\n",
      "Epoch: 700 - Loss: 6.987403594393982e-06\n",
      "Epoch: 750 - Loss: 4.715052909887163e-06\n",
      "Epoch: 800 - Loss: 3.224238525945111e-06\n",
      "Epoch: 850 - Loss: 2.233981149402098e-06\n",
      "Epoch: 900 - Loss: 1.5685762946304749e-06\n",
      "Epoch: 950 - Loss: 1.1164117950102082e-06\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.7911754846572876\n",
      "Epoch: 50 - Loss: 0.017848653718829155\n",
      "Epoch: 100 - Loss: 0.004967563319951296\n",
      "Epoch: 150 - Loss: 0.0019951178692281246\n",
      "Epoch: 200 - Loss: 0.0010109281865879893\n",
      "Epoch: 250 - Loss: 0.0005895216600038111\n",
      "Epoch: 300 - Loss: 0.00037440250162035227\n",
      "Epoch: 350 - Loss: 0.0002512117207515985\n",
      "Epoch: 400 - Loss: 0.00017511945043224841\n",
      "Epoch: 450 - Loss: 0.00012560925097204745\n",
      "Epoch: 500 - Loss: 9.215380850946531e-05\n",
      "Epoch: 550 - Loss: 6.888271309435368e-05\n",
      "Epoch: 600 - Loss: 5.2312854677438736e-05\n",
      "Epoch: 650 - Loss: 4.0284205169882625e-05\n",
      "Epoch: 700 - Loss: 3.1400872103404254e-05\n",
      "Epoch: 750 - Loss: 2.4743567337282002e-05\n",
      "Epoch: 800 - Loss: 1.9685938241309486e-05\n",
      "Epoch: 850 - Loss: 1.5798494132468477e-05\n",
      "Epoch: 900 - Loss: 1.2775443792634178e-05\n",
      "Epoch: 950 - Loss: 1.0401714462204836e-05\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.9527069330215454\n",
      "Epoch: 50 - Loss: 0.02704506367444992\n",
      "Epoch: 100 - Loss: 0.007796191610395908\n",
      "Epoch: 150 - Loss: 0.003111175959929824\n",
      "Epoch: 200 - Loss: 0.001494349678978324\n",
      "Epoch: 250 - Loss: 0.0007954014581628144\n",
      "Epoch: 300 - Loss: 0.0004495001630857587\n",
      "Epoch: 350 - Loss: 0.0002643860934767872\n",
      "Epoch: 400 - Loss: 0.0001604631106602028\n",
      "Epoch: 450 - Loss: 0.00010016313899541274\n",
      "Epoch: 500 - Loss: 6.425378523999825e-05\n",
      "Epoch: 550 - Loss: 4.237030225340277e-05\n",
      "Epoch: 600 - Loss: 2.8735483283526264e-05\n",
      "Epoch: 650 - Loss: 2.004941416089423e-05\n",
      "Epoch: 700 - Loss: 1.4386127986654174e-05\n",
      "Epoch: 750 - Loss: 1.0603710506984498e-05\n",
      "Epoch: 800 - Loss: 8.014019840629771e-06\n",
      "Epoch: 850 - Loss: 6.194561592565151e-06\n",
      "Epoch: 900 - Loss: 4.884192094323225e-06\n",
      "Epoch: 950 - Loss: 3.915732577297604e-06\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.126102089881897\n",
      "Epoch: 50 - Loss: 0.03665676712989807\n",
      "Epoch: 100 - Loss: 0.010561301372945309\n",
      "Epoch: 150 - Loss: 0.004289550706744194\n",
      "Epoch: 200 - Loss: 0.002159029943868518\n",
      "Epoch: 250 - Loss: 0.0012339561944827437\n",
      "Epoch: 300 - Loss: 0.0007587347645312548\n",
      "Epoch: 350 - Loss: 0.0004870037082582712\n",
      "Epoch: 400 - Loss: 0.00032099851523526013\n",
      "Epoch: 450 - Loss: 0.0002153171953978017\n",
      "Epoch: 500 - Loss: 0.00014622487651649863\n",
      "Epoch: 550 - Loss: 0.00010023830691352487\n",
      "Epoch: 600 - Loss: 6.923446198925376e-05\n",
      "Epoch: 650 - Loss: 4.812511178897694e-05\n",
      "Epoch: 700 - Loss: 3.364419171703048e-05\n",
      "Epoch: 750 - Loss: 2.364420288358815e-05\n",
      "Epoch: 800 - Loss: 1.6699843399692327e-05\n",
      "Epoch: 850 - Loss: 1.185248856927501e-05\n",
      "Epoch: 900 - Loss: 8.453976079181302e-06\n",
      "Epoch: 950 - Loss: 6.059358838683693e-06\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.7855663895606995\n",
      "Epoch: 50 - Loss: 0.029099319130182266\n",
      "Epoch: 100 - Loss: 0.00886816717684269\n",
      "Epoch: 150 - Loss: 0.0038634357042610645\n",
      "Epoch: 200 - Loss: 0.002064689062535763\n",
      "Epoch: 250 - Loss: 0.0012509494554251432\n",
      "Epoch: 300 - Loss: 0.0008253231644630432\n",
      "Epoch: 350 - Loss: 0.0005798320635221899\n",
      "Epoch: 400 - Loss: 0.000427499006036669\n",
      "Epoch: 450 - Loss: 0.0003271612513344735\n",
      "Epoch: 500 - Loss: 0.0002576147089712322\n",
      "Epoch: 550 - Loss: 0.00020726618822664022\n",
      "Epoch: 600 - Loss: 0.0001694575184956193\n",
      "Epoch: 650 - Loss: 0.00014020089292898774\n",
      "Epoch: 700 - Loss: 0.00011701851326506585\n",
      "Epoch: 750 - Loss: 9.830483031691983e-05\n",
      "Epoch: 800 - Loss: 8.298038301290944e-05\n",
      "Epoch: 850 - Loss: 7.029574044281617e-05\n",
      "Epoch: 900 - Loss: 5.9711997892009094e-05\n",
      "Epoch: 950 - Loss: 5.082380084786564e-05\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.7206735014915466\n",
      "Epoch: 50 - Loss: 0.024540919810533524\n",
      "Epoch: 100 - Loss: 0.00802373606711626\n",
      "Epoch: 150 - Loss: 0.0034872626420110464\n",
      "Epoch: 200 - Loss: 0.0018149280222132802\n",
      "Epoch: 250 - Loss: 0.0010673729702830315\n",
      "Epoch: 300 - Loss: 0.0006817967514507473\n",
      "Epoch: 350 - Loss: 0.0004606791480910033\n",
      "Epoch: 400 - Loss: 0.0003238344215787947\n",
      "Epoch: 450 - Loss: 0.00023443350801244378\n",
      "Epoch: 500 - Loss: 0.00017370411660522223\n",
      "Epoch: 550 - Loss: 0.0001312145177507773\n",
      "Epoch: 600 - Loss: 0.00010078019113279879\n",
      "Epoch: 650 - Loss: 7.854150317143649e-05\n",
      "Epoch: 700 - Loss: 6.200422649271786e-05\n",
      "Epoch: 750 - Loss: 4.950975926476531e-05\n",
      "Epoch: 800 - Loss: 3.9930288039613515e-05\n",
      "Epoch: 850 - Loss: 3.248969005653635e-05\n",
      "Epoch: 900 - Loss: 2.663637678779196e-05\n",
      "Epoch: 950 - Loss: 2.1982614271109924e-05\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.7433865070343018\n",
      "Epoch: 50 - Loss: 0.01599547639489174\n",
      "Epoch: 100 - Loss: 0.0049717016518116\n",
      "Epoch: 150 - Loss: 0.0021264783572405577\n",
      "Epoch: 200 - Loss: 0.0010727675398811698\n",
      "Epoch: 250 - Loss: 0.0005943572614341974\n",
      "Epoch: 300 - Loss: 0.0003491292882245034\n",
      "Epoch: 350 - Loss: 0.0002134808455593884\n",
      "Epoch: 400 - Loss: 0.00013449230755213648\n",
      "Epoch: 450 - Loss: 8.675757999299094e-05\n",
      "Epoch: 500 - Loss: 5.7083452702499926e-05\n",
      "Epoch: 550 - Loss: 3.821765858447179e-05\n",
      "Epoch: 600 - Loss: 2.5997589546022937e-05\n",
      "Epoch: 650 - Loss: 1.7954815120901912e-05\n",
      "Epoch: 700 - Loss: 1.2586466255015694e-05\n",
      "Epoch: 750 - Loss: 8.955077646533027e-06\n",
      "Epoch: 800 - Loss: 6.468361789302435e-06\n",
      "Epoch: 850 - Loss: 4.743211775348755e-06\n",
      "Epoch: 900 - Loss: 3.531581569404807e-06\n",
      "Epoch: 950 - Loss: 2.669184141268488e-06\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.021339774131775\n",
      "Epoch: 50 - Loss: 0.03530023247003555\n",
      "Epoch: 100 - Loss: 0.011702612973749638\n",
      "Epoch: 150 - Loss: 0.005295219831168652\n",
      "Epoch: 200 - Loss: 0.0028546922840178013\n",
      "Epoch: 250 - Loss: 0.0017154524102807045\n",
      "Epoch: 300 - Loss: 0.0011052279733121395\n",
      "Epoch: 350 - Loss: 0.0007463671499863267\n",
      "Epoch: 400 - Loss: 0.0005214970442466438\n",
      "Epoch: 450 - Loss: 0.0003742301487363875\n",
      "Epoch: 500 - Loss: 0.00027461678837426007\n",
      "Epoch: 550 - Loss: 0.00020549514738377184\n",
      "Epoch: 600 - Loss: 0.00015649526903871447\n",
      "Epoch: 650 - Loss: 0.00012108390365028754\n",
      "Epoch: 700 - Loss: 9.503354522166774e-05\n",
      "Epoch: 750 - Loss: 7.554471085313708e-05\n",
      "Epoch: 800 - Loss: 6.073474287404679e-05\n",
      "Epoch: 850 - Loss: 4.931192233925685e-05\n",
      "Epoch: 900 - Loss: 4.037936378153972e-05\n",
      "Epoch: 950 - Loss: 3.3306743716821074e-05\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.0943760871887207\n",
      "Epoch: 50 - Loss: 0.05830603465437889\n",
      "Epoch: 100 - Loss: 0.017116788774728775\n",
      "Epoch: 150 - Loss: 0.006742891855537891\n",
      "Epoch: 200 - Loss: 0.0032620078418403864\n",
      "Epoch: 250 - Loss: 0.0018282680539414287\n",
      "Epoch: 300 - Loss: 0.0011366349644958973\n",
      "Epoch: 350 - Loss: 0.0007615028298459947\n",
      "Epoch: 400 - Loss: 0.0005395982880145311\n",
      "Epoch: 450 - Loss: 0.00039924978045746684\n",
      "Epoch: 500 - Loss: 0.0003055462148040533\n",
      "Epoch: 550 - Loss: 0.000240076711634174\n",
      "Epoch: 600 - Loss: 0.0001925275137182325\n",
      "Epoch: 650 - Loss: 0.00015684610116295516\n",
      "Epoch: 700 - Loss: 0.0001293302047997713\n",
      "Epoch: 750 - Loss: 0.00010762936290120706\n",
      "Epoch: 800 - Loss: 9.020071593113244e-05\n",
      "Epoch: 850 - Loss: 7.599961099913344e-05\n",
      "Epoch: 900 - Loss: 6.429435597965494e-05\n",
      "Epoch: 950 - Loss: 5.456203507492319e-05\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.7591738700866699\n",
      "Epoch: 50 - Loss: 0.020714281126856804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 - Loss: 0.005627795588225126\n",
      "Epoch: 150 - Loss: 0.002056615427136421\n",
      "Epoch: 200 - Loss: 0.0009314077906310558\n",
      "Epoch: 250 - Loss: 0.0004946011467836797\n",
      "Epoch: 300 - Loss: 0.00029418274061754346\n",
      "Epoch: 350 - Loss: 0.00018937440472654998\n",
      "Epoch: 400 - Loss: 0.00012888365017715842\n",
      "Epoch: 450 - Loss: 9.13180410861969e-05\n",
      "Epoch: 500 - Loss: 6.667905836366117e-05\n",
      "Epoch: 550 - Loss: 4.982820610166527e-05\n",
      "Epoch: 600 - Loss: 3.792373536271043e-05\n",
      "Epoch: 650 - Loss: 2.9294811611180194e-05\n",
      "Epoch: 700 - Loss: 2.2908356186235324e-05\n",
      "Epoch: 750 - Loss: 1.80998304131208e-05\n",
      "Epoch: 800 - Loss: 1.4427649148274213e-05\n",
      "Epoch: 850 - Loss: 1.1589215318963397e-05\n",
      "Epoch: 900 - Loss: 9.371640771860257e-06\n",
      "Epoch: 950 - Loss: 7.623894362041028e-06\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.0461411476135254\n",
      "Epoch: 50 - Loss: 0.02788613550364971\n",
      "Epoch: 100 - Loss: 0.007689516060054302\n",
      "Epoch: 150 - Loss: 0.002966430736705661\n",
      "Epoch: 200 - Loss: 0.0014095440274104476\n",
      "Epoch: 250 - Loss: 0.0007713612867519259\n",
      "Epoch: 300 - Loss: 0.00046429267968051136\n",
      "Epoch: 350 - Loss: 0.0002980515419039875\n",
      "Epoch: 400 - Loss: 0.00020007271086797118\n",
      "Epoch: 450 - Loss: 0.00013871105329599231\n",
      "Epoch: 500 - Loss: 9.855795360635966e-05\n",
      "Epoch: 550 - Loss: 7.14125853846781e-05\n",
      "Epoch: 600 - Loss: 5.2591192797990516e-05\n",
      "Epoch: 650 - Loss: 3.9273701986530796e-05\n",
      "Epoch: 700 - Loss: 2.969083834614139e-05\n",
      "Epoch: 750 - Loss: 2.2692347556585446e-05\n",
      "Epoch: 800 - Loss: 1.7515580111648887e-05\n",
      "Epoch: 850 - Loss: 1.364246509183431e-05\n",
      "Epoch: 900 - Loss: 1.0713767551351339e-05\n",
      "Epoch: 950 - Loss: 8.477302799292374e-06\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.0849751234054565\n",
      "Epoch: 50 - Loss: 0.013678652234375477\n",
      "Epoch: 100 - Loss: 0.004171443171799183\n",
      "Epoch: 150 - Loss: 0.0017260509775951505\n",
      "Epoch: 200 - Loss: 0.0008550120983272791\n",
      "Epoch: 250 - Loss: 0.00047585423453710973\n",
      "Epoch: 300 - Loss: 0.000286323600448668\n",
      "Epoch: 350 - Loss: 0.0001821076002670452\n",
      "Epoch: 400 - Loss: 0.00012088549556210637\n",
      "Epoch: 450 - Loss: 8.31354409456253e-05\n",
      "Epoch: 500 - Loss: 5.895777576370165e-05\n",
      "Epoch: 550 - Loss: 4.2960644350387156e-05\n",
      "Epoch: 600 - Loss: 3.206389010301791e-05\n",
      "Epoch: 650 - Loss: 2.4440385459456593e-05\n",
      "Epoch: 700 - Loss: 1.8971388271893375e-05\n",
      "Epoch: 750 - Loss: 1.4958210158511065e-05\n",
      "Epoch: 800 - Loss: 1.1951129636145197e-05\n",
      "Epoch: 850 - Loss: 9.655635949457064e-06\n",
      "Epoch: 900 - Loss: 7.873522008594591e-06\n",
      "Epoch: 950 - Loss: 6.470244898082456e-06\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.6508102416992188\n",
      "Epoch: 50 - Loss: 0.028324443846940994\n",
      "Epoch: 100 - Loss: 0.007878574542701244\n",
      "Epoch: 150 - Loss: 0.0030791854951530695\n",
      "Epoch: 200 - Loss: 0.001505806460045278\n",
      "Epoch: 250 - Loss: 0.0008622479508630931\n",
      "Epoch: 300 - Loss: 0.0005503665888682008\n",
      "Epoch: 350 - Loss: 0.0003783229039981961\n",
      "Epoch: 400 - Loss: 0.0002738168404903263\n",
      "Epoch: 450 - Loss: 0.00020564402802847326\n",
      "Epoch: 500 - Loss: 0.0001587297738296911\n",
      "Epoch: 550 - Loss: 0.00012509444786701351\n",
      "Epoch: 600 - Loss: 0.00010019406909123063\n",
      "Epoch: 650 - Loss: 8.128080662572756e-05\n",
      "Epoch: 700 - Loss: 6.661307270405814e-05\n",
      "Epoch: 750 - Loss: 5.5043787142494693e-05\n",
      "Epoch: 800 - Loss: 4.578910011332482e-05\n",
      "Epoch: 850 - Loss: 3.8297774153761566e-05\n",
      "Epoch: 900 - Loss: 3.2174451916944236e-05\n",
      "Epoch: 950 - Loss: 2.7129055524710566e-05\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.9357651472091675\n",
      "Epoch: 50 - Loss: 0.05208190530538559\n",
      "Epoch: 100 - Loss: 0.013697058893740177\n",
      "Epoch: 150 - Loss: 0.004691231995820999\n",
      "Epoch: 200 - Loss: 0.002009885385632515\n",
      "Epoch: 250 - Loss: 0.001042005023919046\n",
      "Epoch: 300 - Loss: 0.0006254254258237779\n",
      "Epoch: 350 - Loss: 0.00041559748933650553\n",
      "Epoch: 400 - Loss: 0.0002953533548861742\n",
      "Epoch: 450 - Loss: 0.00021942231978755444\n",
      "Epoch: 500 - Loss: 0.00016803696053102612\n",
      "Epoch: 550 - Loss: 0.00013152648170944303\n",
      "Epoch: 600 - Loss: 0.0001046538382070139\n",
      "Epoch: 650 - Loss: 8.435091149294749e-05\n",
      "Epoch: 700 - Loss: 6.869677599752322e-05\n",
      "Epoch: 750 - Loss: 5.6423530622851104e-05\n",
      "Epoch: 800 - Loss: 4.666817403631285e-05\n",
      "Epoch: 850 - Loss: 3.8825794035801664e-05\n",
      "Epoch: 900 - Loss: 3.2458268833579496e-05\n",
      "Epoch: 950 - Loss: 2.7246913305134512e-05\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.6799862384796143\n",
      "Epoch: 50 - Loss: 0.06483132392168045\n",
      "Epoch: 100 - Loss: 0.01625094935297966\n",
      "Epoch: 150 - Loss: 0.006210504099726677\n",
      "Epoch: 200 - Loss: 0.0030548793729394674\n",
      "Epoch: 250 - Loss: 0.0017429819563403726\n",
      "Epoch: 300 - Loss: 0.0010874980362132192\n",
      "Epoch: 350 - Loss: 0.0007176638464443386\n",
      "Epoch: 400 - Loss: 0.0004914060700684786\n",
      "Epoch: 450 - Loss: 0.00034522745409049094\n",
      "Epoch: 500 - Loss: 0.0002471847692504525\n",
      "Epoch: 550 - Loss: 0.00017965584993362427\n",
      "Epoch: 600 - Loss: 0.00013221753761172295\n",
      "Epoch: 650 - Loss: 9.837316611083224e-05\n",
      "Epoch: 700 - Loss: 7.392030966002494e-05\n",
      "Epoch: 750 - Loss: 5.6059379858197644e-05\n",
      "Epoch: 800 - Loss: 4.288351919967681e-05\n",
      "Epoch: 850 - Loss: 3.307605948066339e-05\n",
      "Epoch: 900 - Loss: 2.571115328464657e-05\n",
      "Epoch: 950 - Loss: 2.0136543753324077e-05\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.1221957206726074\n",
      "Epoch: 50 - Loss: 0.04134580120444298\n",
      "Epoch: 100 - Loss: 0.010171124711632729\n",
      "Epoch: 150 - Loss: 0.003455130150541663\n",
      "Epoch: 200 - Loss: 0.0014875376364216208\n",
      "Epoch: 250 - Loss: 0.0007637487142346799\n",
      "Epoch: 300 - Loss: 0.00044112824252806604\n",
      "Epoch: 350 - Loss: 0.00027430153568275273\n",
      "Epoch: 400 - Loss: 0.00017878141079563648\n",
      "Epoch: 450 - Loss: 0.0001203736464958638\n",
      "Epoch: 500 - Loss: 8.309222175739706e-05\n",
      "Epoch: 550 - Loss: 5.8566973166307434e-05\n",
      "Epoch: 600 - Loss: 4.2058156395796686e-05\n",
      "Epoch: 650 - Loss: 3.073049083468504e-05\n",
      "Epoch: 700 - Loss: 2.28215467359405e-05\n",
      "Epoch: 750 - Loss: 1.7210555597557686e-05\n",
      "Epoch: 800 - Loss: 1.3168898476578761e-05\n",
      "Epoch: 850 - Loss: 1.021388470689999e-05\n",
      "Epoch: 900 - Loss: 8.022717338462826e-06\n",
      "Epoch: 950 - Loss: 6.374192253133515e-06\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.2331901788711548\n",
      "Epoch: 50 - Loss: 0.018921753391623497\n",
      "Epoch: 100 - Loss: 0.005236530210822821\n",
      "Epoch: 150 - Loss: 0.002109712455421686\n",
      "Epoch: 200 - Loss: 0.0010519034694880247\n",
      "Epoch: 250 - Loss: 0.0005962695577181876\n",
      "Epoch: 300 - Loss: 0.0003665053809527308\n",
      "Epoch: 350 - Loss: 0.00023784903169143945\n",
      "Epoch: 400 - Loss: 0.0001604775752639398\n",
      "Epoch: 450 - Loss: 0.00011151465878356248\n",
      "Epoch: 500 - Loss: 7.931436266517267e-05\n",
      "Epoch: 550 - Loss: 5.748119656345807e-05\n",
      "Epoch: 600 - Loss: 4.230520062264986e-05\n",
      "Epoch: 650 - Loss: 3.153448778903112e-05\n",
      "Epoch: 700 - Loss: 2.375630174356047e-05\n",
      "Epoch: 750 - Loss: 1.805649299058132e-05\n",
      "Epoch: 800 - Loss: 1.382848040520912e-05\n",
      "Epoch: 850 - Loss: 1.065884225681657e-05\n",
      "Epoch: 900 - Loss: 8.262109986389987e-06\n",
      "Epoch: 950 - Loss: 6.436503554141382e-06\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.3011581897735596\n",
      "Epoch: 50 - Loss: 0.073074571788311\n",
      "Epoch: 100 - Loss: 0.020667636767029762\n",
      "Epoch: 150 - Loss: 0.0076553355902433395\n",
      "Epoch: 200 - Loss: 0.0034359300043433905\n",
      "Epoch: 250 - Loss: 0.0017856806516647339\n",
      "Epoch: 300 - Loss: 0.001036180299706757\n",
      "Epoch: 350 - Loss: 0.0006527311634272337\n",
      "Epoch: 400 - Loss: 0.00043738450040109456\n",
      "Epoch: 450 - Loss: 0.0003072655526921153\n",
      "Epoch: 500 - Loss: 0.0002239174209535122\n",
      "Epoch: 550 - Loss: 0.0001679382985457778\n",
      "Epoch: 600 - Loss: 0.00012885950854979455\n",
      "Epoch: 650 - Loss: 0.00010068901610793546\n",
      "Epoch: 700 - Loss: 7.984511466929689e-05\n",
      "Epoch: 750 - Loss: 6.408373883459717e-05\n",
      "Epoch: 800 - Loss: 5.194529876462184e-05\n",
      "Epoch: 850 - Loss: 4.2456118535483256e-05\n",
      "Epoch: 900 - Loss: 3.494318661978468e-05\n",
      "Epoch: 950 - Loss: 2.892917655117344e-05\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.9767674803733826\n",
      "Epoch: 50 - Loss: 0.052997808903455734\n",
      "Epoch: 100 - Loss: 0.01452514436095953\n",
      "Epoch: 150 - Loss: 0.00547752995043993\n",
      "Epoch: 200 - Loss: 0.0025077098980545998\n",
      "Epoch: 250 - Loss: 0.0012885290198028088\n",
      "Epoch: 300 - Loss: 0.0007127006538212299\n",
      "Epoch: 350 - Loss: 0.00041569373570382595\n",
      "Epoch: 400 - Loss: 0.00025308775366283953\n",
      "Epoch: 450 - Loss: 0.00015997227455955\n",
      "Epoch: 500 - Loss: 0.00010462867794558406\n",
      "Epoch: 550 - Loss: 7.06217615515925e-05\n",
      "Epoch: 600 - Loss: 4.9063830374507234e-05\n",
      "Epoch: 650 - Loss: 3.498942169244401e-05\n",
      "Epoch: 700 - Loss: 2.5536475732224062e-05\n",
      "Epoch: 750 - Loss: 1.901714495033957e-05\n",
      "Epoch: 800 - Loss: 1.4408873539650813e-05\n",
      "Epoch: 850 - Loss: 1.1078496754635125e-05\n",
      "Epoch: 900 - Loss: 8.623690519016236e-06\n",
      "Epoch: 950 - Loss: 6.783277513022767e-06\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.1682430505752563\n",
      "Epoch: 50 - Loss: 0.054940544068813324\n",
      "Epoch: 100 - Loss: 0.01653023064136505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 150 - Loss: 0.006826911121606827\n",
      "Epoch: 200 - Loss: 0.003401305992156267\n",
      "Epoch: 250 - Loss: 0.0018967508804053068\n",
      "Epoch: 300 - Loss: 0.0011336030438542366\n",
      "Epoch: 350 - Loss: 0.0007094674510881305\n",
      "Epoch: 400 - Loss: 0.0004595527716446668\n",
      "Epoch: 450 - Loss: 0.0003063631011173129\n",
      "Epoch: 500 - Loss: 0.00020966892770957202\n",
      "Epoch: 550 - Loss: 0.00014713765995111316\n",
      "Epoch: 600 - Loss: 0.00010580323578324169\n",
      "Epoch: 650 - Loss: 7.789600203977898e-05\n",
      "Epoch: 700 - Loss: 5.8644967793952674e-05\n",
      "Epoch: 750 - Loss: 4.507609628490172e-05\n",
      "Epoch: 800 - Loss: 3.5301429306855425e-05\n",
      "Epoch: 850 - Loss: 2.8103881049901247e-05\n",
      "Epoch: 900 - Loss: 2.2691356207360514e-05\n",
      "Epoch: 950 - Loss: 1.8540333257988095e-05\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.5298782587051392\n",
      "Epoch: 50 - Loss: 0.02766413427889347\n",
      "Epoch: 100 - Loss: 0.006486124824732542\n",
      "Epoch: 150 - Loss: 0.002154451096430421\n",
      "Epoch: 200 - Loss: 0.0008771728025749326\n",
      "Epoch: 250 - Loss: 0.00041039660573005676\n",
      "Epoch: 300 - Loss: 0.00021187681704759598\n",
      "Epoch: 350 - Loss: 0.00011763581278501078\n",
      "Epoch: 400 - Loss: 6.919552106410265e-05\n",
      "Epoch: 450 - Loss: 4.277134212316014e-05\n",
      "Epoch: 500 - Loss: 2.7652004064293578e-05\n",
      "Epoch: 550 - Loss: 1.863094075815752e-05\n",
      "Epoch: 600 - Loss: 1.3036746167927049e-05\n",
      "Epoch: 650 - Loss: 9.435522770218085e-06\n",
      "Epoch: 700 - Loss: 7.03326941220439e-06\n",
      "Epoch: 750 - Loss: 5.3758481044496875e-06\n",
      "Epoch: 800 - Loss: 4.195718702248996e-06\n",
      "Epoch: 850 - Loss: 3.3311987408524146e-06\n",
      "Epoch: 900 - Loss: 2.6818547667062376e-06\n",
      "Epoch: 950 - Loss: 2.1833461687492672e-06\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.27974534034729\n",
      "Epoch: 50 - Loss: 0.07076042890548706\n",
      "Epoch: 100 - Loss: 0.018199197947978973\n",
      "Epoch: 150 - Loss: 0.006777334026992321\n",
      "Epoch: 200 - Loss: 0.003192993812263012\n",
      "Epoch: 250 - Loss: 0.0017498487140983343\n",
      "Epoch: 300 - Loss: 0.0010585521813482046\n",
      "Epoch: 350 - Loss: 0.0006853462546132505\n",
      "Epoch: 400 - Loss: 0.0004663406580220908\n",
      "Epoch: 450 - Loss: 0.00032979424577206373\n",
      "Epoch: 500 - Loss: 0.0002406249550404027\n",
      "Epoch: 550 - Loss: 0.00018018657283391804\n",
      "Epoch: 600 - Loss: 0.00013792580284643918\n",
      "Epoch: 650 - Loss: 0.00010757066047517583\n",
      "Epoch: 700 - Loss: 8.524179429514334e-05\n",
      "Epoch: 750 - Loss: 6.846603355370462e-05\n",
      "Epoch: 800 - Loss: 5.562262958846986e-05\n",
      "Epoch: 850 - Loss: 4.562192771118134e-05\n",
      "Epoch: 900 - Loss: 3.771882620640099e-05\n",
      "Epoch: 950 - Loss: 3.139265754725784e-05\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.9033657312393188\n",
      "Epoch: 50 - Loss: 0.03519979491829872\n",
      "Epoch: 100 - Loss: 0.009345262311398983\n",
      "Epoch: 150 - Loss: 0.003427743213251233\n",
      "Epoch: 200 - Loss: 0.001564605045132339\n",
      "Epoch: 250 - Loss: 0.0008243007468990982\n",
      "Epoch: 300 - Loss: 0.000475643842946738\n",
      "Epoch: 350 - Loss: 0.00029133728821761906\n",
      "Epoch: 400 - Loss: 0.00018622502102516592\n",
      "Epoch: 450 - Loss: 0.00012310856254771352\n",
      "Epoch: 500 - Loss: 8.374903700314462e-05\n",
      "Epoch: 550 - Loss: 5.844440602231771e-05\n",
      "Epoch: 600 - Loss: 4.173895649728365e-05\n",
      "Epoch: 650 - Loss: 3.0440010959864594e-05\n",
      "Epoch: 700 - Loss: 2.262403540953528e-05\n",
      "Epoch: 750 - Loss: 1.7100564946304075e-05\n",
      "Epoch: 800 - Loss: 1.3120401490596123e-05\n",
      "Epoch: 850 - Loss: 1.019949559122324e-05\n",
      "Epoch: 900 - Loss: 8.020634595595766e-06\n",
      "Epoch: 950 - Loss: 6.370612481987337e-06\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.9898731708526611\n",
      "Epoch: 50 - Loss: 0.021788248792290688\n",
      "Epoch: 100 - Loss: 0.005248205736279488\n",
      "Epoch: 150 - Loss: 0.0019598498474806547\n",
      "Epoch: 200 - Loss: 0.0009373706416226923\n",
      "Epoch: 250 - Loss: 0.0005197150749154389\n",
      "Epoch: 300 - Loss: 0.00031443030457012355\n",
      "Epoch: 350 - Loss: 0.0002006120776059106\n",
      "Epoch: 400 - Loss: 0.00013256684178486466\n",
      "Epoch: 450 - Loss: 8.989364141598344e-05\n",
      "Epoch: 500 - Loss: 6.225609104149044e-05\n",
      "Epoch: 550 - Loss: 4.392499613459222e-05\n",
      "Epoch: 600 - Loss: 3.153219950036146e-05\n",
      "Epoch: 650 - Loss: 2.300962296430953e-05\n",
      "Epoch: 700 - Loss: 1.7055535863619298e-05\n",
      "Epoch: 750 - Loss: 1.2832778338633943e-05\n",
      "Epoch: 800 - Loss: 9.79302240011748e-06\n",
      "Epoch: 850 - Loss: 7.572399226773996e-06\n",
      "Epoch: 900 - Loss: 5.9270641941111535e-06\n",
      "Epoch: 950 - Loss: 4.6900695451768115e-06\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.9852170348167419\n",
      "Epoch: 50 - Loss: 0.06413925439119339\n",
      "Epoch: 100 - Loss: 0.01922634430229664\n",
      "Epoch: 150 - Loss: 0.007569225039333105\n",
      "Epoch: 200 - Loss: 0.0036062069702893496\n",
      "Epoch: 250 - Loss: 0.0019562155939638615\n",
      "Epoch: 300 - Loss: 0.0011580144055187702\n",
      "Epoch: 350 - Loss: 0.0007293181261047721\n",
      "Epoch: 400 - Loss: 0.00048160855658352375\n",
      "Epoch: 450 - Loss: 0.00033047478063963354\n",
      "Epoch: 500 - Loss: 0.0002341601939406246\n",
      "Epoch: 550 - Loss: 0.00017047845176421106\n",
      "Epoch: 600 - Loss: 0.0001270044012926519\n",
      "Epoch: 650 - Loss: 9.648494597058743e-05\n",
      "Epoch: 700 - Loss: 7.452722638845444e-05\n",
      "Epoch: 750 - Loss: 5.838803554070182e-05\n",
      "Epoch: 800 - Loss: 4.630334296962246e-05\n",
      "Epoch: 850 - Loss: 3.710527380462736e-05\n",
      "Epoch: 900 - Loss: 3.0004379368619993e-05\n",
      "Epoch: 950 - Loss: 2.4452134312014095e-05\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.9116342067718506\n",
      "Epoch: 50 - Loss: 0.06488214433193207\n",
      "Epoch: 100 - Loss: 0.016708575189113617\n",
      "Epoch: 150 - Loss: 0.0060970052145421505\n",
      "Epoch: 200 - Loss: 0.0027024736627936363\n",
      "Epoch: 250 - Loss: 0.001364446827210486\n",
      "Epoch: 300 - Loss: 0.0007540893275290728\n",
      "Epoch: 350 - Loss: 0.0004439754702616483\n",
      "Epoch: 400 - Loss: 0.00027359166415408254\n",
      "Epoch: 450 - Loss: 0.00017458527872804552\n",
      "Epoch: 500 - Loss: 0.0001146551439887844\n",
      "Epoch: 550 - Loss: 7.722983718849719e-05\n",
      "Epoch: 600 - Loss: 5.3246902098180726e-05\n",
      "Epoch: 650 - Loss: 3.753392229555175e-05\n",
      "Epoch: 700 - Loss: 2.702150959521532e-05\n",
      "Epoch: 750 - Loss: 1.9851047909469344e-05\n",
      "Epoch: 800 - Loss: 1.4863254364172462e-05\n",
      "Epoch: 850 - Loss: 1.132708894147072e-05\n",
      "Epoch: 900 - Loss: 8.774268280831166e-06\n",
      "Epoch: 950 - Loss: 6.897724688315066e-06\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 4.259442329406738\n",
      "Epoch: 50 - Loss: 0.28696852922439575\n",
      "Epoch: 100 - Loss: 0.07619794458150864\n",
      "Epoch: 150 - Loss: 0.027505522593855858\n",
      "Epoch: 200 - Loss: 0.01213886495679617\n",
      "Epoch: 250 - Loss: 0.006194580812007189\n",
      "Epoch: 300 - Loss: 0.0035104635171592236\n",
      "Epoch: 350 - Loss: 0.002144918078556657\n",
      "Epoch: 400 - Loss: 0.001385389594361186\n",
      "Epoch: 450 - Loss: 0.0009341600816696882\n",
      "Epoch: 500 - Loss: 0.0006523510674014688\n",
      "Epoch: 550 - Loss: 0.0004692056099884212\n",
      "Epoch: 600 - Loss: 0.0003461332235019654\n",
      "Epoch: 650 - Loss: 0.0002609841467346996\n",
      "Epoch: 700 - Loss: 0.0002005142014240846\n",
      "Epoch: 750 - Loss: 0.00015654500748496503\n",
      "Epoch: 800 - Loss: 0.00012389413313940167\n",
      "Epoch: 850 - Loss: 9.918825526256114e-05\n",
      "Epoch: 900 - Loss: 8.018435619305819e-05\n",
      "Epoch: 950 - Loss: 6.535413558594882e-05\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.026536226272583\n",
      "Epoch: 50 - Loss: 0.04421137273311615\n",
      "Epoch: 100 - Loss: 0.012859330512583256\n",
      "Epoch: 150 - Loss: 0.005152627360075712\n",
      "Epoch: 200 - Loss: 0.002532954327762127\n",
      "Epoch: 250 - Loss: 0.0014072475023567677\n",
      "Epoch: 300 - Loss: 0.0008410266600549221\n",
      "Epoch: 350 - Loss: 0.0005266948137432337\n",
      "Epoch: 400 - Loss: 0.00034098129253834486\n",
      "Epoch: 450 - Loss: 0.0002265579969389364\n",
      "Epoch: 500 - Loss: 0.0001538675423944369\n",
      "Epoch: 550 - Loss: 0.00010655177175067365\n",
      "Epoch: 600 - Loss: 7.511797593906522e-05\n",
      "Epoch: 650 - Loss: 5.385390613810159e-05\n",
      "Epoch: 700 - Loss: 3.9229547837749124e-05\n",
      "Epoch: 750 - Loss: 2.9013077437411994e-05\n",
      "Epoch: 800 - Loss: 2.1768135411548428e-05\n",
      "Epoch: 850 - Loss: 1.6555215552216396e-05\n",
      "Epoch: 900 - Loss: 1.2751737813232467e-05\n",
      "Epoch: 950 - Loss: 9.937404684023932e-06\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.8531501889228821\n",
      "Epoch: 50 - Loss: 0.026330413296818733\n",
      "Epoch: 100 - Loss: 0.007371312007308006\n",
      "Epoch: 150 - Loss: 0.0030063525773584843\n",
      "Epoch: 200 - Loss: 0.0015186357777565718\n",
      "Epoch: 250 - Loss: 0.0008639015723019838\n",
      "Epoch: 300 - Loss: 0.0005250253016129136\n",
      "Epoch: 350 - Loss: 0.00033209656248800457\n",
      "Epoch: 400 - Loss: 0.00021588084928225726\n",
      "Epoch: 450 - Loss: 0.00014332885621115565\n",
      "Epoch: 500 - Loss: 9.687658166512847e-05\n",
      "Epoch: 550 - Loss: 6.654936441918835e-05\n",
      "Epoch: 600 - Loss: 4.6422737796092406e-05\n",
      "Epoch: 650 - Loss: 3.286784703959711e-05\n",
      "Epoch: 700 - Loss: 2.361181213927921e-05\n",
      "Epoch: 750 - Loss: 1.720598629617598e-05\n",
      "Epoch: 800 - Loss: 1.2714822332782205e-05\n",
      "Epoch: 850 - Loss: 9.524007509753574e-06\n",
      "Epoch: 900 - Loss: 7.22793856766657e-06\n",
      "Epoch: 950 - Loss: 5.552832135435892e-06\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.9373425841331482\n",
      "Epoch: 50 - Loss: 0.016856979578733444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100 - Loss: 0.00441238097846508\n",
      "Epoch: 150 - Loss: 0.001545302220620215\n",
      "Epoch: 200 - Loss: 0.0006496921996586025\n",
      "Epoch: 250 - Loss: 0.0003127706004306674\n",
      "Epoch: 300 - Loss: 0.00016707542818039656\n",
      "Epoch: 350 - Loss: 9.646997932577506e-05\n",
      "Epoch: 400 - Loss: 5.895249705645256e-05\n",
      "Epoch: 450 - Loss: 3.754670251510106e-05\n",
      "Epoch: 500 - Loss: 2.4669481717864983e-05\n",
      "Epoch: 550 - Loss: 1.6612413674010895e-05\n",
      "Epoch: 600 - Loss: 1.1422129318816587e-05\n",
      "Epoch: 650 - Loss: 8.000388334039599e-06\n",
      "Epoch: 700 - Loss: 5.700998372049071e-06\n",
      "Epoch: 750 - Loss: 4.129958597332006e-06\n",
      "Epoch: 800 - Loss: 3.040156116185244e-06\n",
      "Epoch: 850 - Loss: 2.2725316739524715e-06\n",
      "Epoch: 900 - Loss: 1.7241915202248492e-06\n",
      "Epoch: 950 - Loss: 1.3269178680275218e-06\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.6216921806335449\n",
      "Epoch: 50 - Loss: 0.035736799240112305\n",
      "Epoch: 100 - Loss: 0.010224903002381325\n",
      "Epoch: 150 - Loss: 0.0040999287739396095\n",
      "Epoch: 200 - Loss: 0.002004422480240464\n",
      "Epoch: 250 - Loss: 0.0011015753261744976\n",
      "Epoch: 300 - Loss: 0.0006490761879831553\n",
      "Epoch: 350 - Loss: 0.0003994865692220628\n",
      "Epoch: 400 - Loss: 0.00025310824275948107\n",
      "Epoch: 450 - Loss: 0.00016367036732845008\n",
      "Epoch: 500 - Loss: 0.00010743298480520025\n",
      "Epoch: 550 - Loss: 7.132444443413988e-05\n",
      "Epoch: 600 - Loss: 4.7774774429854006e-05\n",
      "Epoch: 650 - Loss: 3.22306586895138e-05\n",
      "Epoch: 700 - Loss: 2.187213431170676e-05\n",
      "Epoch: 750 - Loss: 1.4917319276719354e-05\n",
      "Epoch: 800 - Loss: 1.0218545867246576e-05\n",
      "Epoch: 850 - Loss: 7.026982530078385e-06\n",
      "Epoch: 900 - Loss: 4.849840934184613e-06\n",
      "Epoch: 950 - Loss: 3.3583369258849416e-06\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.3897671699523926\n",
      "Epoch: 50 - Loss: 0.05288073420524597\n",
      "Epoch: 100 - Loss: 0.016605041921138763\n",
      "Epoch: 150 - Loss: 0.006762530654668808\n",
      "Epoch: 200 - Loss: 0.003287908621132374\n",
      "Epoch: 250 - Loss: 0.0018104144837707281\n",
      "Epoch: 300 - Loss: 0.001082549337297678\n",
      "Epoch: 350 - Loss: 0.0006820082780905068\n",
      "Epoch: 400 - Loss: 0.0004440956108737737\n",
      "Epoch: 450 - Loss: 0.0002955645031761378\n",
      "Epoch: 500 - Loss: 0.00019983014499302953\n",
      "Epoch: 550 - Loss: 0.0001368113880744204\n",
      "Epoch: 600 - Loss: 9.470744407735765e-05\n",
      "Epoch: 650 - Loss: 6.624914385611191e-05\n",
      "Epoch: 700 - Loss: 4.68287653347943e-05\n",
      "Epoch: 750 - Loss: 3.3453674404881895e-05\n",
      "Epoch: 800 - Loss: 2.4161972760339268e-05\n",
      "Epoch: 850 - Loss: 1.7649650544626638e-05\n",
      "Epoch: 900 - Loss: 1.3042860700807068e-05\n",
      "Epoch: 950 - Loss: 9.752750884217676e-06\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.8425377011299133\n",
      "Epoch: 50 - Loss: 0.024595417082309723\n",
      "Epoch: 100 - Loss: 0.0063960961997509\n",
      "Epoch: 150 - Loss: 0.0022193945478647947\n",
      "Epoch: 200 - Loss: 0.0009410862112417817\n",
      "Epoch: 250 - Loss: 0.0004636606608983129\n",
      "Epoch: 300 - Loss: 0.00025622083921916783\n",
      "Epoch: 350 - Loss: 0.00015463611634913832\n",
      "Epoch: 400 - Loss: 9.989309910451993e-05\n",
      "Epoch: 450 - Loss: 6.805171869928017e-05\n",
      "Epoch: 500 - Loss: 4.836281732423231e-05\n",
      "Epoch: 550 - Loss: 3.5569923056755215e-05\n",
      "Epoch: 600 - Loss: 2.6909438020084053e-05\n",
      "Epoch: 650 - Loss: 2.0839637727476656e-05\n",
      "Epoch: 700 - Loss: 1.6454283468192443e-05\n",
      "Epoch: 750 - Loss: 1.3199860404711217e-05\n",
      "Epoch: 800 - Loss: 1.0728802408266347e-05\n",
      "Epoch: 850 - Loss: 8.813935892248992e-06\n",
      "Epoch: 900 - Loss: 7.3038645496126264e-06\n",
      "Epoch: 950 - Loss: 6.094363016018178e-06\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.9187240600585938\n",
      "Epoch: 50 - Loss: 0.021853316575288773\n",
      "Epoch: 100 - Loss: 0.006036417558789253\n",
      "Epoch: 150 - Loss: 0.00222563068382442\n",
      "Epoch: 200 - Loss: 0.000994193833321333\n",
      "Epoch: 250 - Loss: 0.000510043406393379\n",
      "Epoch: 300 - Loss: 0.00028962266515009105\n",
      "Epoch: 350 - Loss: 0.00017726940859574825\n",
      "Epoch: 400 - Loss: 0.00011482476838864386\n",
      "Epoch: 450 - Loss: 7.774164987495169e-05\n",
      "Epoch: 500 - Loss: 5.4556923714699224e-05\n",
      "Epoch: 550 - Loss: 3.9444388676201925e-05\n",
      "Epoch: 600 - Loss: 2.9249244107631966e-05\n",
      "Epoch: 650 - Loss: 2.2162166715133935e-05\n",
      "Epoch: 700 - Loss: 1.7105376173276454e-05\n",
      "Epoch: 750 - Loss: 1.3412258340395056e-05\n",
      "Epoch: 800 - Loss: 1.065980177372694e-05\n",
      "Epoch: 850 - Loss: 8.570332283852622e-06\n",
      "Epoch: 900 - Loss: 6.958135600143578e-06\n",
      "Epoch: 950 - Loss: 5.696380412700819e-06\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.9063105583190918\n",
      "Epoch: 50 - Loss: 0.028808660805225372\n",
      "Epoch: 100 - Loss: 0.008814376778900623\n",
      "Epoch: 150 - Loss: 0.003616464091464877\n",
      "Epoch: 200 - Loss: 0.0017718544695526361\n",
      "Epoch: 250 - Loss: 0.0009732951293699443\n",
      "Epoch: 300 - Loss: 0.0005778676131740212\n",
      "Epoch: 350 - Loss: 0.000362979102646932\n",
      "Epoch: 400 - Loss: 0.00023825891548767686\n",
      "Epoch: 450 - Loss: 0.00016228295862674713\n",
      "Epoch: 500 - Loss: 0.00011421523959143087\n",
      "Epoch: 550 - Loss: 8.281786722363904e-05\n",
      "Epoch: 600 - Loss: 6.171072163851932e-05\n",
      "Epoch: 650 - Loss: 4.7125980927376077e-05\n",
      "Epoch: 700 - Loss: 3.677782297017984e-05\n",
      "Epoch: 750 - Loss: 2.9242195523693226e-05\n",
      "Epoch: 800 - Loss: 2.361762562941294e-05\n",
      "Epoch: 850 - Loss: 1.9322949810884893e-05\n",
      "Epoch: 900 - Loss: 1.5973304471117444e-05\n",
      "Epoch: 950 - Loss: 1.3314321222424041e-05\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.2471717596054077\n",
      "Epoch: 50 - Loss: 0.026441432535648346\n",
      "Epoch: 100 - Loss: 0.006500429008156061\n",
      "Epoch: 150 - Loss: 0.002139476826414466\n",
      "Epoch: 200 - Loss: 0.0008918130188249052\n",
      "Epoch: 250 - Loss: 0.0004478799528442323\n",
      "Epoch: 300 - Loss: 0.0002573820238467306\n",
      "Epoch: 350 - Loss: 0.00016199224046431482\n",
      "Epoch: 400 - Loss: 0.0001082697490346618\n",
      "Epoch: 450 - Loss: 7.537614146713167e-05\n",
      "Epoch: 500 - Loss: 5.4044288845034316e-05\n",
      "Epoch: 550 - Loss: 3.964091956731863e-05\n",
      "Epoch: 600 - Loss: 2.9625281968037598e-05\n",
      "Epoch: 650 - Loss: 2.249899807793554e-05\n",
      "Epoch: 700 - Loss: 1.7328089597867802e-05\n",
      "Epoch: 750 - Loss: 1.3513506019080523e-05\n",
      "Epoch: 800 - Loss: 1.065561536961468e-05\n",
      "Epoch: 850 - Loss: 8.48536456032889e-06\n",
      "Epoch: 900 - Loss: 6.815183041908313e-06\n",
      "Epoch: 950 - Loss: 5.516377314052079e-06\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.8749516010284424\n",
      "Epoch: 50 - Loss: 0.017761103808879852\n",
      "Epoch: 100 - Loss: 0.004156850278377533\n",
      "Epoch: 150 - Loss: 0.0013766526244580746\n",
      "Epoch: 200 - Loss: 0.0005599943688139319\n",
      "Epoch: 250 - Loss: 0.0002620084851514548\n",
      "Epoch: 300 - Loss: 0.00013640621909871697\n",
      "Epoch: 350 - Loss: 7.73493738961406e-05\n",
      "Epoch: 400 - Loss: 4.7023269871715456e-05\n",
      "Epoch: 450 - Loss: 3.027686761924997e-05\n",
      "Epoch: 500 - Loss: 2.0458837752812542e-05\n",
      "Epoch: 550 - Loss: 1.4405072761292104e-05\n",
      "Epoch: 600 - Loss: 1.0506117178010754e-05\n",
      "Epoch: 650 - Loss: 7.896453098510392e-06\n",
      "Epoch: 700 - Loss: 6.087748261052184e-06\n",
      "Epoch: 750 - Loss: 4.79370828543324e-06\n",
      "Epoch: 800 - Loss: 3.841522357106442e-06\n",
      "Epoch: 850 - Loss: 3.12199904328736e-06\n",
      "Epoch: 900 - Loss: 2.5666331566753797e-06\n",
      "Epoch: 950 - Loss: 2.1292171368259005e-06\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.5509413480758667\n",
      "Epoch: 50 - Loss: 0.1257980763912201\n",
      "Epoch: 100 - Loss: 0.03420311212539673\n",
      "Epoch: 150 - Loss: 0.012549149803817272\n",
      "Epoch: 200 - Loss: 0.005823397543281317\n",
      "Epoch: 250 - Loss: 0.0032540166284888983\n",
      "Epoch: 300 - Loss: 0.0020785098895430565\n",
      "Epoch: 350 - Loss: 0.0014505842700600624\n",
      "Epoch: 400 - Loss: 0.0010710806818678975\n",
      "Epoch: 450 - Loss: 0.0008198975119739771\n",
      "Epoch: 500 - Loss: 0.0006427742773666978\n",
      "Epoch: 550 - Loss: 0.0005123571027070284\n",
      "Epoch: 600 - Loss: 0.000413404282880947\n",
      "Epoch: 650 - Loss: 0.0003366959863342345\n",
      "Epoch: 700 - Loss: 0.000276276288786903\n",
      "Epoch: 750 - Loss: 0.00022808174253441393\n",
      "Epoch: 800 - Loss: 0.0001892607833724469\n",
      "Epoch: 850 - Loss: 0.0001577251241542399\n",
      "Epoch: 900 - Loss: 0.00013192655751481652\n",
      "Epoch: 950 - Loss: 0.0001106994241126813\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 1.1870461702346802\n",
      "Epoch: 50 - Loss: 0.02650565654039383\n",
      "Epoch: 100 - Loss: 0.006055199541151524\n",
      "Epoch: 150 - Loss: 0.0019208523444831371\n",
      "Epoch: 200 - Loss: 0.0007785010966472328\n",
      "Epoch: 250 - Loss: 0.0003785149019677192\n",
      "Epoch: 300 - Loss: 0.0002093750808853656\n",
      "Epoch: 350 - Loss: 0.0001264335442101583\n",
      "Epoch: 400 - Loss: 8.097497629933059e-05\n",
      "Epoch: 450 - Loss: 5.398875873652287e-05\n",
      "Epoch: 500 - Loss: 3.704694245243445e-05\n",
      "Epoch: 550 - Loss: 2.5984198146034032e-05\n",
      "Epoch: 600 - Loss: 1.8553717382019386e-05\n",
      "Epoch: 650 - Loss: 1.3454497093334794e-05\n",
      "Epoch: 700 - Loss: 9.89370710158255e-06\n",
      "Epoch: 750 - Loss: 7.36951869839686e-06\n",
      "Epoch: 800 - Loss: 5.55698625248624e-06\n",
      "Epoch: 850 - Loss: 4.238058409100631e-06\n",
      "Epoch: 900 - Loss: 3.2675036436558003e-06\n",
      "Epoch: 950 - Loss: 2.544847120589111e-06\n"
     ]
    }
   ],
   "source": [
    "pr, re = [[],[],[]], [[],[],[]]\n",
    "ndcgs = defaultdict(list)\n",
    "\n",
    "lr = 0.001\n",
    "epochs = 1000\n",
    "# constraintHigh=1\n",
    "# constraintLow=0\n",
    "constraintHigh=float('inf')\n",
    "constraintLow=-float('inf')\n",
    "loss_weight = [1, 0, 0]\n",
    "L1, L2 = 0, 0\n",
    "rand_type = 0\n",
    "\n",
    "total_mul = 1\n",
    "\n",
    "for uid, uemb in enumerate(tqdm(doc_embs[:50])):\n",
    "    x = word_embs.T\n",
    "    y = uemb\n",
    "    total = len(doc_answers[uid])\n",
    "    total = 1\n",
    "    \n",
    "    torch_model = PyTorchLinearRegression(x.shape[1], lr, constraintHigh, constraintLow, int(total*total_mul), rand_type, L1, L2)\n",
    "    torch_model.fit(x, y, epochs)\n",
    "    \n",
    "    m1 = metric4(torch_model.w.detach().numpy(), doc_answers[uid], w_idx=None, topk=50, verbose=0)\n",
    "    m2 = metric4(torch_model.w.detach().numpy(), doc_answers[uid], w_idx=None, topk=100, verbose=0)\n",
    "    m3 = metric4(torch_model.w.detach().numpy(), doc_answers[uid], w_idx=None, topk=200, verbose=0)\n",
    "    ndcg1 = metric_ndcg2(torch_model.w.detach().numpy(), tfidf_ans, uid, topk=50, verbose=0)\n",
    "    ndcg2 = metric_ndcg2(torch_model.w.detach().numpy(), tfidf_ans, uid, topk=100, verbose=0)\n",
    "    ndcg3 = metric_ndcg2(torch_model.w.detach().numpy(), tfidf_ans, uid, topk=200, verbose=0)\n",
    "    ndcg4 = metric_ndcg2(torch_model.w.detach().numpy(), tfidf_ans, uid, topk=None, verbose=0)\n",
    "    pr[0].append(m1[\"precision\"])\n",
    "    re[0].append(m1[\"recall\"])\n",
    "    pr[1].append(m2[\"precision\"])\n",
    "    re[1].append(m2[\"recall\"])\n",
    "    pr[2].append(m3[\"precision\"])\n",
    "    re[2].append(m3[\"recall\"])\n",
    "    \n",
    "    ndcgs[\"50\"].append(ndcg1)\n",
    "    ndcgs[\"100\"].append(ndcg2)\n",
    "    ndcgs[\"200\"].append(ndcg3)\n",
    "    ndcgs[\"-1\"].append(ndcg4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12ccce5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.1604 Recall:0.3568\n",
      "Precision:0.1084 Recall:0.4408\n",
      "Precision:0.0713 Recall:0.5424\n",
      "NDCG 50:0.5117\n",
      "NDCG 100:0.5402\n",
      "NDCG 200:0.5724\n",
      "NDCG all:0.6853\n"
     ]
    }
   ],
   "source": [
    "print(f\"Precision:{np.mean(pr[0]):.4f} Recall:{np.mean(re[0]):.4f}\")\n",
    "print(f\"Precision:{np.mean(pr[1]):.4f} Recall:{np.mean(re[1]):.4f}\")\n",
    "print(f\"Precision:{np.mean(pr[2]):.4f} Recall:{np.mean(re[2]):.4f}\")\n",
    "print(f\"NDCG 50:{np.mean(ndcgs['50']):.4f}\")\n",
    "print(f\"NDCG 100:{np.mean(ndcgs['100']):.4f}\")\n",
    "print(f\"NDCG 200:{np.mean(ndcgs['200']):.4f}\")\n",
    "print(f\"NDCG all:{np.mean(ndcgs['-1']):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4fcb3a",
   "metadata": {},
   "source": [
    "## Lasso with GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f05d86f",
   "metadata": {},
   "source": [
    "1. train lasso\n",
    "    1. with mse\n",
    "    2. with BCE to D\n",
    "2. train D\n",
    "    1. positive\n",
    "    2. negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "854f9586",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 doc_vectors,\n",
    "                 tfidf_ans,\n",
    "                 ):\n",
    "        self.doc_vectors = doc_vectors\n",
    "        self.tfidf_ans = tfidf_ans\n",
    "        assert len(doc_vectors) == len(tfidf_ans)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # doc vec, tfidf-ans, doc_id\n",
    "        doc_vec = torch.FloatTensor(self.doc_vectors[idx])\n",
    "        tfidf_ans = torch.FloatTensor(self.tfidf_ans[idx])\n",
    "                \n",
    "        return doc_vec, tfidf_ans, idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doc_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0aa07e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lasso(nn.Module):\n",
    "    \"\"\"\n",
    "    Input shape: (N, 3, 64, 64)\n",
    "    Output shape: (N, )\n",
    "    \"\"\"\n",
    "    def __init__(self, num_doc, num_words, L1=0, L2=0):\n",
    "        super(Lasso, self).__init__()\n",
    "        weight = torch.zeros(num_doc, num_words)\n",
    "        self.emb = torch.nn.Embedding.from_pretrained(weight, freeze=False, max_norm=None, norm_type=1)#(num_doc, num_words)\n",
    "        \n",
    "    def forward(self, doc_ids, word_vectors):\n",
    "        return self.emb(doc_ids) @ word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ad46742",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_words, h_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(num_words, h_dim) \n",
    "        self.fc2 = nn.Linear(h_dim, h_dim)\n",
    "#         self.fc3 = nn.Linear(h_dim, h_dim)\n",
    "        self.fc4 = nn.Linear(h_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "#         x = F.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4ca8caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_GAN_NDCG(model, train_loader, verbose=1):\n",
    "    results = {}\n",
    "    model.eval()\n",
    "    \n",
    "    scores = np.array(model.emb.weight.data)\n",
    "    true_relevance = train_loader.dataset.tfidf_ans\n",
    "        \n",
    "    results['ndcg@50'] = (ndcg_score(true_relevance, scores, k=50))\n",
    "    results['ndcg@100'] = (ndcg_score(true_relevance, scores, k=100))\n",
    "    results['ndcg@200'] = (ndcg_score(true_relevance, scores, k=200))\n",
    "    results['ndcg@all'] = (ndcg_score(true_relevance, scores, k=None))\n",
    "    \n",
    "    if verbose == 1:\n",
    "        print('NDCG top50', results['ndcg@50'])\n",
    "        print('NDCG top100', results['ndcg@100'])\n",
    "        print('NDCG top200', results['ndcg@200'])\n",
    "        print('NDCG ALL', results['ndcg@all'])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ac91f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "document_vectors = np.array(dataset.document_vectors)\n",
    "\n",
    "len(document_vectors)\n",
    "train_test_split_ratio = 0.005\n",
    "select_num = int(len(document_vectors) * train_test_split_ratio)\n",
    "\n",
    "train_dataset = GANDataset(document_vectors[:select_num], tfidf_ans[:select_num])\n",
    "valid_dataset = GANDataset(document_vectors[select_num:], tfidf_ans[select_num:])\n",
    "\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True, pin_memory=True)\n",
    "test_loader  = torch.utils.data.DataLoader(valid_dataset, batch_size=10, shuffle=True, pin_memory=True)\n",
    "\n",
    "print(select_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0ad1ef76",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_tensor = torch.FloatTensor(word_vectors)\n",
    "word_vectors_tensor.shape\n",
    "\n",
    "test_word_weight_tensor = torch.FloatTensor(tfidf_ans[select_num:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cf7a5e",
   "metadata": {},
   "source": [
    "## start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31899b46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef08c53ac8b4eea86c9f239ebae957e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# setting\n",
    "lr = 0.02\n",
    "momentum = 0.\n",
    "weight_decay = 0\n",
    "nesterov = False # True\n",
    "\n",
    "bs = 10\n",
    "\n",
    "n_epoch = 200000\n",
    "n_critic = 1\n",
    "\n",
    "loss_weight = [0, 1, 1e-3]\n",
    "clip_value = 0\n",
    "\n",
    "verbose = 0\n",
    "\n",
    "model = Lasso(num_doc=select_num, num_words=word_vectors.shape[0])\n",
    "D = Discriminator(num_words=word_vectors.shape[0], h_dim=32)\n",
    "\n",
    "model.train()\n",
    "D.train()\n",
    "\n",
    "# opt_G = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "# opt_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "opt_G = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay, nesterov=nesterov)\n",
    "opt_D = torch.optim.SGD(D.parameters(), lr=lr*10)\n",
    "\n",
    "G_criterion = nn.MSELoss(reduction='mean')\n",
    "D_criterion = nn.BCELoss()\n",
    "\n",
    "results = []\n",
    "step = 0\n",
    "for epoch in tqdm(range(n_epoch)):\n",
    "    \n",
    "    D_loss = []\n",
    "    G_loss_D = []\n",
    "    G_loss_MSE = []\n",
    "    \n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        doc_embs, _, doc_ids = data\n",
    "\n",
    "        perm = torch.randperm(test_word_weight_tensor.size(0))\n",
    "        true_word_weights_sample = test_word_weight_tensor[perm[:bs]]\n",
    "\n",
    "        # Label\n",
    "        r_label = torch.ones((bs))\n",
    "        f_label = torch.zeros((doc_embs.size(0)))\n",
    "\n",
    "        r_logit = D(true_word_weights_sample.detach())\n",
    "        f_logit = D(model.emb(doc_ids).detach())\n",
    "        \n",
    "        # Compute the loss for the discriminator.\n",
    "        # r_loss = D_criterion(r_logit.squeeze(), r_label)\n",
    "        # f_loss = D_criterion(f_logit.squeeze(), f_label)\n",
    "        # loss_D = (r_loss + f_loss) / 2\n",
    "\n",
    "        # WGAN Loss\n",
    "        loss_D = -torch.mean(r_logit) + torch.mean(f_logit)\n",
    "\n",
    "        # Model backwarding\n",
    "        D.zero_grad()\n",
    "        loss_D.backward()\n",
    "\n",
    "        # Update the discriminator.\n",
    "        opt_D.step()\n",
    "        \n",
    "        D_loss.append(loss_D.item())\n",
    "        \"\"\" Clip weights of discriminator. \"\"\"\n",
    "        for p in D.parameters():\n",
    "            p.data.clamp_(-10, 10)\n",
    "\n",
    "        # ============================================\n",
    "        #  Train G\n",
    "        # ============================================\n",
    "        \n",
    "        if (epoch % n_critic == 0) and (epoch > 500):\n",
    "            # Generate some fake images.\n",
    "            f_imgs = model.emb(doc_ids)\n",
    "            f_logit = D(f_imgs)\n",
    "            \n",
    "            # Compute the loss for the generator.\n",
    "            # loss_G_Dis = D_criterion(f_logit.squeeze(), torch.ones((doc_embs.size(0))))\n",
    "            # WGAN Loss\n",
    "            loss_G_Dis = -torch.mean(f_logit)\n",
    "            \n",
    "            # MSE loss\n",
    "            pred_doc_embs = model(doc_ids, word_vectors_tensor)     \n",
    "            loss_G_MSE = G_criterion(pred_doc_embs, doc_embs)\n",
    "            \n",
    "            emb_w_sum = torch.sum(model.emb(doc_ids), axis=1)\n",
    "            \n",
    "            loss_G_SUM = G_criterion(emb_w_sum, torch.ones(emb_w_sum.shape))\n",
    "            \n",
    "            loss_G = loss_G_Dis * loss_weight[0] + loss_G_MSE * loss_weight[1] + loss_G_SUM * loss_weight[2]\n",
    "            # Model backwarding\n",
    "            model.zero_grad()\n",
    "            loss_G.backward()\n",
    "\n",
    "            # Update the generator.\n",
    "            opt_G.step()\n",
    "            \n",
    "            G_loss_D.append(loss_G_Dis.item())\n",
    "            G_loss_MSE.append(loss_G_MSE.item())\n",
    "            \n",
    "            \n",
    "            for p in model.parameters():\n",
    "                p.data.clamp_(clip_value, 100)\n",
    "        step += 1\n",
    "        \n",
    "    if epoch % 100 == 0:\n",
    "        res = {}\n",
    "        res['epoch'] = epoch\n",
    "        res['loss_D'] = np.mean(D_loss)\n",
    "        res['loss_G_D'] = np.mean(G_loss_D)\n",
    "        res['loss_G_MSE'] = np.mean(G_loss_MSE)\n",
    "\n",
    "        if verbose==1:\n",
    "            print('epoch', res['epoch'])\n",
    "            print('loss D', res['loss_D'])\n",
    "            print('loss G D', res['loss_G_D'])\n",
    "            print('loss G MSE', res['loss_G_MSE'])\n",
    "\n",
    "        res_ndcg = evaluate_GAN_NDCG(model, train_loader, verbose)\n",
    "        res.update(res_ndcg)\n",
    "        results.append(res)\n",
    "        \n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60b75d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.groupby(by=['epoch']).mean().plot()\n",
    "results_df.groupby(by=['epoch']).mean()\n",
    "# results_df.groupby(by=['epoch']).mean().iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f83887c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# setting\n",
    "lr = 0.02\n",
    "momentum = 0.\n",
    "weight_decay = 0\n",
    "nesterov = False # True\n",
    "\n",
    "bs = 10\n",
    "\n",
    "n_epoch = 200000\n",
    "n_critic = 1\n",
    "\n",
    "loss_weight = [1, 1, 1e-3]\n",
    "clip_value = 0\n",
    "\n",
    "verbose = 0\n",
    "\n",
    "model = Lasso(num_doc=select_num, num_words=word_vectors.shape[0])\n",
    "D = Discriminator(num_words=word_vectors.shape[0], h_dim=32)\n",
    "\n",
    "model.train()\n",
    "D.train()\n",
    "\n",
    "# opt_G = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "# opt_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "opt_G = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay, nesterov=nesterov)\n",
    "opt_D = torch.optim.SGD(D.parameters(), lr=lr*10)\n",
    "\n",
    "G_criterion = nn.MSELoss(reduction='mean')\n",
    "D_criterion = nn.BCELoss()\n",
    "\n",
    "results = []\n",
    "step = 0\n",
    "for epoch in tqdm(range(n_epoch)):\n",
    "    \n",
    "    D_loss = []\n",
    "    G_loss_D = []\n",
    "    G_loss_MSE = []\n",
    "    \n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        doc_embs, _, doc_ids = data\n",
    "\n",
    "        perm = torch.randperm(test_word_weight_tensor.size(0))\n",
    "        true_word_weights_sample = test_word_weight_tensor[perm[:bs]]\n",
    "\n",
    "        # Label\n",
    "        r_label = torch.ones((bs))\n",
    "        f_label = torch.zeros((doc_embs.size(0)))\n",
    "\n",
    "        r_logit = D(true_word_weights_sample.detach())\n",
    "        f_logit = D(model.emb(doc_ids).detach())\n",
    "        \n",
    "        # Compute the loss for the discriminator.\n",
    "        # r_loss = D_criterion(r_logit.squeeze(), r_label)\n",
    "        # f_loss = D_criterion(f_logit.squeeze(), f_label)\n",
    "        # loss_D = (r_loss + f_loss) / 2\n",
    "\n",
    "        # WGAN Loss\n",
    "        loss_D = -torch.mean(r_logit) + torch.mean(f_logit)\n",
    "\n",
    "        # Model backwarding\n",
    "        D.zero_grad()\n",
    "        loss_D.backward()\n",
    "\n",
    "        # Update the discriminator.\n",
    "        opt_D.step()\n",
    "        \n",
    "        D_loss.append(loss_D.item())\n",
    "        \"\"\" Clip weights of discriminator. \"\"\"\n",
    "        for p in D.parameters():\n",
    "            p.data.clamp_(-10, 10)\n",
    "\n",
    "        # ============================================\n",
    "        #  Train G\n",
    "        # ============================================\n",
    "        \n",
    "        if (epoch % n_critic == 0) and (epoch > 500):\n",
    "            # Generate some fake images.\n",
    "            f_imgs = model.emb(doc_ids)\n",
    "            f_logit = D(f_imgs)\n",
    "            \n",
    "            # Compute the loss for the generator.\n",
    "            # loss_G_Dis = D_criterion(f_logit.squeeze(), torch.ones((doc_embs.size(0))))\n",
    "            # WGAN Loss\n",
    "            loss_G_Dis = -torch.mean(f_logit)\n",
    "            \n",
    "            # MSE loss\n",
    "            pred_doc_embs = model(doc_ids, word_vectors_tensor)     \n",
    "            loss_G_MSE = G_criterion(pred_doc_embs, doc_embs)\n",
    "            \n",
    "            emb_w_sum = torch.sum(model.emb(doc_ids), axis=1)\n",
    "            \n",
    "            loss_G_SUM = G_criterion(emb_w_sum, torch.ones(emb_w_sum.shape))\n",
    "            \n",
    "            loss_G = loss_G_Dis * loss_weight[0] + loss_G_MSE * loss_weight[1] + loss_G_SUM * loss_weight[2]\n",
    "            # Model backwarding\n",
    "            model.zero_grad()\n",
    "            loss_G.backward()\n",
    "\n",
    "            # Update the generator.\n",
    "            opt_G.step()\n",
    "            \n",
    "            G_loss_D.append(loss_G_Dis.item())\n",
    "            G_loss_MSE.append(loss_G_MSE.item())\n",
    "            \n",
    "            \n",
    "            for p in model.parameters():\n",
    "                p.data.clamp_(clip_value, 100)\n",
    "        step += 1\n",
    "        \n",
    "    if epoch % 100 == 0:\n",
    "        res = {}\n",
    "        res['epoch'] = epoch\n",
    "        res['loss_D'] = np.mean(D_loss)\n",
    "        res['loss_G_D'] = np.mean(G_loss_D)\n",
    "        res['loss_G_MSE'] = np.mean(G_loss_MSE)\n",
    "\n",
    "        if verbose==1:\n",
    "            print('epoch', res['epoch'])\n",
    "            print('loss D', res['loss_D'])\n",
    "            print('loss G D', res['loss_G_D'])\n",
    "            print('loss G MSE', res['loss_G_MSE'])\n",
    "\n",
    "        res_ndcg = evaluate_GAN_NDCG(model, train_loader, verbose)\n",
    "        res.update(res_ndcg)\n",
    "        results.append(res)\n",
    "        \n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef20677",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.groupby(by=['epoch']).mean().plot()\n",
    "results_df.groupby(by=['epoch']).mean()\n",
    "# results_df.groupby(by=['epoch']).mean().iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cd79e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18bdd6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "57961c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# setting\n",
    "lr = 0.02\n",
    "momentum = 0.\n",
    "weight_decay = 0\n",
    "nesterov = False # True\n",
    "\n",
    "bs = 10\n",
    "\n",
    "n_epoch = 5000\n",
    "n_critic = 1\n",
    "\n",
    "loss_weight = [0, 1, 1e-3]\n",
    "clip_value = 0\n",
    "\n",
    "verbose = 0\n",
    "\n",
    "model = Lasso(num_doc=select_num, num_words=word_vectors.shape[0])\n",
    "D = Discriminator(num_words=word_vectors.shape[0], h_dim=32)\n",
    "\n",
    "model.train()\n",
    "D.train()\n",
    "\n",
    "# opt_G = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "# opt_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "opt_G = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay, nesterov=nesterov)\n",
    "opt_D = torch.optim.SGD(D.parameters(), lr=lr*10)\n",
    "\n",
    "G_criterion = nn.MSELoss(reduction='mean')\n",
    "D_criterion = nn.BCELoss()\n",
    "\n",
    "results = []\n",
    "step = 0\n",
    "for epoch in tqdm(range(n_epoch)):\n",
    "    \n",
    "    D_loss = []\n",
    "    G_loss_D = []\n",
    "    G_loss_MSE = []\n",
    "    \n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        doc_embs, _, doc_ids = data\n",
    "\n",
    "        perm = torch.randperm(test_word_weight_tensor.size(0))\n",
    "        true_word_weights_sample = test_word_weight_tensor[perm[:bs]]\n",
    "\n",
    "        # Label\n",
    "        r_label = torch.ones((bs))\n",
    "        f_label = torch.zeros((doc_embs.size(0)))\n",
    "\n",
    "        r_logit = D(true_word_weights_sample.detach())\n",
    "        f_logit = D(model.emb(doc_ids).detach())\n",
    "        \n",
    "        # Compute the loss for the discriminator.\n",
    "        # r_loss = D_criterion(r_logit.squeeze(), r_label)\n",
    "        # f_loss = D_criterion(f_logit.squeeze(), f_label)\n",
    "        # loss_D = (r_loss + f_loss) / 2\n",
    "\n",
    "        # WGAN Loss\n",
    "        loss_D = -torch.mean(r_logit) + torch.mean(f_logit)\n",
    "\n",
    "        # Model backwarding\n",
    "        D.zero_grad()\n",
    "        loss_D.backward()\n",
    "\n",
    "        # Update the discriminator.\n",
    "        opt_D.step()\n",
    "        \n",
    "        D_loss.append(loss_D.item())\n",
    "        \"\"\" Clip weights of discriminator. \"\"\"\n",
    "        for p in D.parameters():\n",
    "            p.data.clamp_(-10, 10)\n",
    "\n",
    "        # ============================================\n",
    "        #  Train G\n",
    "        # ============================================\n",
    "        \n",
    "        if (epoch % n_critic == 0) and (epoch > 500):\n",
    "            # Generate some fake images.\n",
    "            f_imgs = model.emb(doc_ids)\n",
    "            f_logit = D(f_imgs)\n",
    "            \n",
    "            # Compute the loss for the generator.\n",
    "            # loss_G_Dis = D_criterion(f_logit.squeeze(), torch.ones((doc_embs.size(0))))\n",
    "            # WGAN Loss\n",
    "            loss_G_Dis = -torch.mean(f_logit)\n",
    "            \n",
    "            # MSE loss\n",
    "            pred_doc_embs = model(doc_ids, word_vectors_tensor)     \n",
    "            loss_G_MSE = G_criterion(pred_doc_embs, doc_embs)\n",
    "            \n",
    "            emb_w_sum = torch.sum(model.emb(doc_ids), axis=1)\n",
    "            \n",
    "            loss_G_SUM = G_criterion(emb_w_sum, torch.ones(emb_w_sum.shape))\n",
    "            \n",
    "            loss_G = loss_G_Dis * loss_weight[0] + loss_G_MSE * loss_weight[1] + loss_G_SUM * loss_weight[2]\n",
    "            # Model backwarding\n",
    "            model.zero_grad()\n",
    "            loss_G.backward()\n",
    "\n",
    "            # Update the generator.\n",
    "            opt_G.step()\n",
    "            \n",
    "            G_loss_D.append(loss_G_Dis.item())\n",
    "            G_loss_MSE.append(loss_G_MSE.item())\n",
    "            \n",
    "            \n",
    "            for p in model.parameters():\n",
    "                p.data.clamp_(clip_value, 100)\n",
    "        step += 1\n",
    "        \n",
    "    if epoch % 100 == 0:\n",
    "        res = {}\n",
    "        res['epoch'] = epoch\n",
    "        res['loss_D'] = np.mean(D_loss)\n",
    "        res['loss_G_D'] = np.mean(G_loss_D)\n",
    "        res['loss_G_MSE'] = np.mean(G_loss_MSE)\n",
    "\n",
    "        if verbose==1:\n",
    "            print('epoch', res['epoch'])\n",
    "            print('loss D', res['loss_D'])\n",
    "            print('loss G D', res['loss_G_D'])\n",
    "            print('loss G MSE', res['loss_G_MSE'])\n",
    "\n",
    "        res_ndcg = evaluate_GAN_NDCG(model, train_loader, verbose)\n",
    "        res.update(res_ndcg)\n",
    "        results.append(res)\n",
    "        \n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f06aab7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss_D</th>\n",
       "      <th>loss_G_D</th>\n",
       "      <th>loss_G_MSE</th>\n",
       "      <th>ndcg@50</th>\n",
       "      <th>ndcg@100</th>\n",
       "      <th>ndcg@200</th>\n",
       "      <th>ndcg@all</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.002258</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008099</td>\n",
       "      <td>0.012895</td>\n",
       "      <td>0.021415</td>\n",
       "      <td>0.247514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>-0.996614</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008099</td>\n",
       "      <td>0.012895</td>\n",
       "      <td>0.021415</td>\n",
       "      <td>0.247514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>-0.998704</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008099</td>\n",
       "      <td>0.012895</td>\n",
       "      <td>0.021415</td>\n",
       "      <td>0.247514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>-0.999034</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008099</td>\n",
       "      <td>0.012895</td>\n",
       "      <td>0.021415</td>\n",
       "      <td>0.247514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>-0.998420</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008099</td>\n",
       "      <td>0.012895</td>\n",
       "      <td>0.021415</td>\n",
       "      <td>0.247514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>-0.999721</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008099</td>\n",
       "      <td>0.012895</td>\n",
       "      <td>0.021415</td>\n",
       "      <td>0.247514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>-0.999748</td>\n",
       "      <td>-0.000250</td>\n",
       "      <td>0.030407</td>\n",
       "      <td>0.517796</td>\n",
       "      <td>0.543642</td>\n",
       "      <td>0.578325</td>\n",
       "      <td>0.687964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>-0.999782</td>\n",
       "      <td>-0.000216</td>\n",
       "      <td>0.016239</td>\n",
       "      <td>0.533323</td>\n",
       "      <td>0.563391</td>\n",
       "      <td>0.596882</td>\n",
       "      <td>0.703502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>-0.999714</td>\n",
       "      <td>-0.000282</td>\n",
       "      <td>0.011254</td>\n",
       "      <td>0.542062</td>\n",
       "      <td>0.571628</td>\n",
       "      <td>0.604870</td>\n",
       "      <td>0.708807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>-0.999881</td>\n",
       "      <td>-0.000116</td>\n",
       "      <td>0.008522</td>\n",
       "      <td>0.549759</td>\n",
       "      <td>0.578380</td>\n",
       "      <td>0.611204</td>\n",
       "      <td>0.713687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>-0.979888</td>\n",
       "      <td>-0.000113</td>\n",
       "      <td>0.007025</td>\n",
       "      <td>0.555798</td>\n",
       "      <td>0.582442</td>\n",
       "      <td>0.614629</td>\n",
       "      <td>0.716459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>-0.999915</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>0.006032</td>\n",
       "      <td>0.558571</td>\n",
       "      <td>0.584135</td>\n",
       "      <td>0.618398</td>\n",
       "      <td>0.718168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>-0.999866</td>\n",
       "      <td>-0.000067</td>\n",
       "      <td>0.005342</td>\n",
       "      <td>0.560992</td>\n",
       "      <td>0.586553</td>\n",
       "      <td>0.622430</td>\n",
       "      <td>0.720050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>-0.999937</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>0.004805</td>\n",
       "      <td>0.563317</td>\n",
       "      <td>0.589558</td>\n",
       "      <td>0.625682</td>\n",
       "      <td>0.721996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>-0.999941</td>\n",
       "      <td>-0.000059</td>\n",
       "      <td>0.004402</td>\n",
       "      <td>0.567968</td>\n",
       "      <td>0.592848</td>\n",
       "      <td>0.629524</td>\n",
       "      <td>0.724713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>-0.999948</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>0.004180</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.594784</td>\n",
       "      <td>0.631388</td>\n",
       "      <td>0.725399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>-0.999947</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>0.003864</td>\n",
       "      <td>0.569754</td>\n",
       "      <td>0.597551</td>\n",
       "      <td>0.632799</td>\n",
       "      <td>0.726669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700</th>\n",
       "      <td>-0.999908</td>\n",
       "      <td>-0.000037</td>\n",
       "      <td>0.003656</td>\n",
       "      <td>0.571336</td>\n",
       "      <td>0.598607</td>\n",
       "      <td>0.634727</td>\n",
       "      <td>0.727258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>-0.999961</td>\n",
       "      <td>-0.000039</td>\n",
       "      <td>0.003422</td>\n",
       "      <td>0.572603</td>\n",
       "      <td>0.600104</td>\n",
       "      <td>0.637012</td>\n",
       "      <td>0.728264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>-0.999955</td>\n",
       "      <td>-0.000044</td>\n",
       "      <td>0.003270</td>\n",
       "      <td>0.574081</td>\n",
       "      <td>0.601254</td>\n",
       "      <td>0.638784</td>\n",
       "      <td>0.729301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>-0.999965</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>0.003121</td>\n",
       "      <td>0.575376</td>\n",
       "      <td>0.603799</td>\n",
       "      <td>0.641346</td>\n",
       "      <td>0.730762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2100</th>\n",
       "      <td>-0.979971</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.003017</td>\n",
       "      <td>0.576544</td>\n",
       "      <td>0.604990</td>\n",
       "      <td>0.641986</td>\n",
       "      <td>0.731733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>-0.999958</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>0.002924</td>\n",
       "      <td>0.577971</td>\n",
       "      <td>0.607064</td>\n",
       "      <td>0.644701</td>\n",
       "      <td>0.732981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>-0.976567</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>0.002836</td>\n",
       "      <td>0.579331</td>\n",
       "      <td>0.608705</td>\n",
       "      <td>0.646574</td>\n",
       "      <td>0.734045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2400</th>\n",
       "      <td>-0.999971</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.002732</td>\n",
       "      <td>0.579778</td>\n",
       "      <td>0.609495</td>\n",
       "      <td>0.646739</td>\n",
       "      <td>0.734467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2500</th>\n",
       "      <td>-0.999973</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.002640</td>\n",
       "      <td>0.582840</td>\n",
       "      <td>0.612669</td>\n",
       "      <td>0.648909</td>\n",
       "      <td>0.736656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600</th>\n",
       "      <td>-0.999977</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>0.002570</td>\n",
       "      <td>0.583517</td>\n",
       "      <td>0.613489</td>\n",
       "      <td>0.650032</td>\n",
       "      <td>0.737260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2700</th>\n",
       "      <td>-0.999975</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>0.002569</td>\n",
       "      <td>0.584902</td>\n",
       "      <td>0.613943</td>\n",
       "      <td>0.650202</td>\n",
       "      <td>0.737867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2800</th>\n",
       "      <td>-0.999980</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>0.002481</td>\n",
       "      <td>0.585703</td>\n",
       "      <td>0.614349</td>\n",
       "      <td>0.650533</td>\n",
       "      <td>0.737683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2900</th>\n",
       "      <td>-0.999982</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>0.585125</td>\n",
       "      <td>0.614307</td>\n",
       "      <td>0.650317</td>\n",
       "      <td>0.737259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>-0.999982</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>0.002367</td>\n",
       "      <td>0.585372</td>\n",
       "      <td>0.614491</td>\n",
       "      <td>0.651247</td>\n",
       "      <td>0.737668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3100</th>\n",
       "      <td>-0.999979</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.587088</td>\n",
       "      <td>0.615879</td>\n",
       "      <td>0.653008</td>\n",
       "      <td>0.738576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3200</th>\n",
       "      <td>-0.999984</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>0.588507</td>\n",
       "      <td>0.617742</td>\n",
       "      <td>0.654188</td>\n",
       "      <td>0.739603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3300</th>\n",
       "      <td>-0.999980</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>0.002203</td>\n",
       "      <td>0.589323</td>\n",
       "      <td>0.617916</td>\n",
       "      <td>0.654831</td>\n",
       "      <td>0.739975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3400</th>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.002215</td>\n",
       "      <td>0.589993</td>\n",
       "      <td>0.618272</td>\n",
       "      <td>0.655868</td>\n",
       "      <td>0.740498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3500</th>\n",
       "      <td>-0.999984</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.002141</td>\n",
       "      <td>0.590294</td>\n",
       "      <td>0.618823</td>\n",
       "      <td>0.656714</td>\n",
       "      <td>0.741047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3600</th>\n",
       "      <td>-0.999982</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.002154</td>\n",
       "      <td>0.591339</td>\n",
       "      <td>0.619080</td>\n",
       "      <td>0.657404</td>\n",
       "      <td>0.741143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3700</th>\n",
       "      <td>-0.999985</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.002082</td>\n",
       "      <td>0.591675</td>\n",
       "      <td>0.619319</td>\n",
       "      <td>0.658124</td>\n",
       "      <td>0.741438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3800</th>\n",
       "      <td>-0.999988</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.002072</td>\n",
       "      <td>0.592196</td>\n",
       "      <td>0.620289</td>\n",
       "      <td>0.658394</td>\n",
       "      <td>0.741710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3900</th>\n",
       "      <td>-0.999988</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.002026</td>\n",
       "      <td>0.593109</td>\n",
       "      <td>0.621001</td>\n",
       "      <td>0.658772</td>\n",
       "      <td>0.742005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>-0.999988</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.002027</td>\n",
       "      <td>0.594059</td>\n",
       "      <td>0.622207</td>\n",
       "      <td>0.659786</td>\n",
       "      <td>0.742385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4100</th>\n",
       "      <td>-0.999978</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.594810</td>\n",
       "      <td>0.623314</td>\n",
       "      <td>0.660836</td>\n",
       "      <td>0.743248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4200</th>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.001932</td>\n",
       "      <td>0.595143</td>\n",
       "      <td>0.623408</td>\n",
       "      <td>0.660497</td>\n",
       "      <td>0.743361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4300</th>\n",
       "      <td>-0.999988</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.001904</td>\n",
       "      <td>0.595560</td>\n",
       "      <td>0.624114</td>\n",
       "      <td>0.660789</td>\n",
       "      <td>0.743838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4400</th>\n",
       "      <td>-0.999980</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.001889</td>\n",
       "      <td>0.595880</td>\n",
       "      <td>0.625010</td>\n",
       "      <td>0.661081</td>\n",
       "      <td>0.743959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4500</th>\n",
       "      <td>-0.999984</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.001880</td>\n",
       "      <td>0.596865</td>\n",
       "      <td>0.626414</td>\n",
       "      <td>0.663173</td>\n",
       "      <td>0.744990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>-0.999990</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.001861</td>\n",
       "      <td>0.597126</td>\n",
       "      <td>0.626497</td>\n",
       "      <td>0.663102</td>\n",
       "      <td>0.745015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4700</th>\n",
       "      <td>-0.999990</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.001813</td>\n",
       "      <td>0.596956</td>\n",
       "      <td>0.626400</td>\n",
       "      <td>0.663166</td>\n",
       "      <td>0.744883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4800</th>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>0.597593</td>\n",
       "      <td>0.626655</td>\n",
       "      <td>0.663378</td>\n",
       "      <td>0.745294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4900</th>\n",
       "      <td>-0.999990</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>0.597916</td>\n",
       "      <td>0.627445</td>\n",
       "      <td>0.663941</td>\n",
       "      <td>0.745732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss_D  loss_G_D  loss_G_MSE   ndcg@50  ndcg@100  ndcg@200  ndcg@all\n",
       "epoch                                                                        \n",
       "0      0.002258       NaN         NaN  0.008099  0.012895  0.021415  0.247514\n",
       "100   -0.996614       NaN         NaN  0.008099  0.012895  0.021415  0.247514\n",
       "200   -0.998704       NaN         NaN  0.008099  0.012895  0.021415  0.247514\n",
       "300   -0.999034       NaN         NaN  0.008099  0.012895  0.021415  0.247514\n",
       "400   -0.998420       NaN         NaN  0.008099  0.012895  0.021415  0.247514\n",
       "500   -0.999721       NaN         NaN  0.008099  0.012895  0.021415  0.247514\n",
       "600   -0.999748 -0.000250    0.030407  0.517796  0.543642  0.578325  0.687964\n",
       "700   -0.999782 -0.000216    0.016239  0.533323  0.563391  0.596882  0.703502\n",
       "800   -0.999714 -0.000282    0.011254  0.542062  0.571628  0.604870  0.708807\n",
       "900   -0.999881 -0.000116    0.008522  0.549759  0.578380  0.611204  0.713687\n",
       "1000  -0.979888 -0.000113    0.007025  0.555798  0.582442  0.614629  0.716459\n",
       "1100  -0.999915 -0.000084    0.006032  0.558571  0.584135  0.618398  0.718168\n",
       "1200  -0.999866 -0.000067    0.005342  0.560992  0.586553  0.622430  0.720050\n",
       "1300  -0.999937 -0.000058    0.004805  0.563317  0.589558  0.625682  0.721996\n",
       "1400  -0.999941 -0.000059    0.004402  0.567968  0.592848  0.629524  0.724713\n",
       "1500  -0.999948 -0.000048    0.004180  0.568627  0.594784  0.631388  0.725399\n",
       "1600  -0.999947 -0.000042    0.003864  0.569754  0.597551  0.632799  0.726669\n",
       "1700  -0.999908 -0.000037    0.003656  0.571336  0.598607  0.634727  0.727258\n",
       "1800  -0.999961 -0.000039    0.003422  0.572603  0.600104  0.637012  0.728264\n",
       "1900  -0.999955 -0.000044    0.003270  0.574081  0.601254  0.638784  0.729301\n",
       "2000  -0.999965 -0.000035    0.003121  0.575376  0.603799  0.641346  0.730762\n",
       "2100  -0.979971 -0.000030    0.003017  0.576544  0.604990  0.641986  0.731733\n",
       "2200  -0.999958 -0.000027    0.002924  0.577971  0.607064  0.644701  0.732981\n",
       "2300  -0.976567 -0.000038    0.002836  0.579331  0.608705  0.646574  0.734045\n",
       "2400  -0.999971 -0.000029    0.002732  0.579778  0.609495  0.646739  0.734467\n",
       "2500  -0.999973 -0.000025    0.002640  0.582840  0.612669  0.648909  0.736656\n",
       "2600  -0.999977 -0.000022    0.002570  0.583517  0.613489  0.650032  0.737260\n",
       "2700  -0.999975 -0.000021    0.002569  0.584902  0.613943  0.650202  0.737867\n",
       "2800  -0.999980 -0.000020    0.002481  0.585703  0.614349  0.650533  0.737683\n",
       "2900  -0.999982 -0.000018    0.002404  0.585125  0.614307  0.650317  0.737259\n",
       "3000  -0.999982 -0.000017    0.002367  0.585372  0.614491  0.651247  0.737668\n",
       "3100  -0.999979 -0.000016    0.002296  0.587088  0.615879  0.653008  0.738576\n",
       "3200  -0.999984 -0.000016    0.002267  0.588507  0.617742  0.654188  0.739603\n",
       "3300  -0.999980 -0.000015    0.002203  0.589323  0.617916  0.654831  0.739975\n",
       "3400  -0.999986 -0.000014    0.002215  0.589993  0.618272  0.655868  0.740498\n",
       "3500  -0.999984 -0.000014    0.002141  0.590294  0.618823  0.656714  0.741047\n",
       "3600  -0.999982 -0.000013    0.002154  0.591339  0.619080  0.657404  0.741143\n",
       "3700  -0.999985 -0.000013    0.002082  0.591675  0.619319  0.658124  0.741438\n",
       "3800  -0.999988 -0.000012    0.002072  0.592196  0.620289  0.658394  0.741710\n",
       "3900  -0.999988 -0.000012    0.002026  0.593109  0.621001  0.658772  0.742005\n",
       "4000  -0.999988 -0.000012    0.002027  0.594059  0.622207  0.659786  0.742385\n",
       "4100  -0.999978 -0.000011    0.001953  0.594810  0.623314  0.660836  0.743248\n",
       "4200  -0.999986 -0.000011    0.001932  0.595143  0.623408  0.660497  0.743361\n",
       "4300  -0.999988 -0.000011    0.001904  0.595560  0.624114  0.660789  0.743838\n",
       "4400  -0.999980 -0.000010    0.001889  0.595880  0.625010  0.661081  0.743959\n",
       "4500  -0.999984 -0.000010    0.001880  0.596865  0.626414  0.663173  0.744990\n",
       "4600  -0.999990 -0.000010    0.001861  0.597126  0.626497  0.663102  0.745015\n",
       "4700  -0.999990 -0.000010    0.001813  0.596956  0.626400  0.663166  0.744883\n",
       "4800  -0.999991 -0.000009    0.001801  0.597593  0.626655  0.663378  0.745294\n",
       "4900  -0.999990 -0.000009    0.001802  0.597916  0.627445  0.663941  0.745732"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.groupby(by=['epoch']).mean().plot()\n",
    "results_df.groupby(by=['epoch']).mean()\n",
    "# results_df.groupby(by=['epoch']).mean().iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "008b61c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc43ebd0ebfe4e5499b00165fdf6fce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# setting\n",
    "lr = 0.02\n",
    "momentum = 0.\n",
    "weight_decay = 0\n",
    "nesterov = False # True\n",
    "\n",
    "bs = 10\n",
    "\n",
    "n_epoch = 5000\n",
    "n_critic = 1\n",
    "\n",
    "loss_weight = [1, 1, 1e-3]\n",
    "clip_value = 0\n",
    "\n",
    "verbose = 0\n",
    "\n",
    "model = Lasso(num_doc=select_num, num_words=word_vectors.shape[0])\n",
    "D = Discriminator(num_words=word_vectors.shape[0], h_dim=32)\n",
    "\n",
    "model.train()\n",
    "D.train()\n",
    "\n",
    "# opt_G = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "# opt_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "opt_G = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay, nesterov=nesterov)\n",
    "opt_D = torch.optim.SGD(D.parameters(), lr=lr*10)\n",
    "\n",
    "G_criterion = nn.MSELoss(reduction='mean')\n",
    "D_criterion = nn.BCELoss()\n",
    "\n",
    "results = []\n",
    "step = 0\n",
    "for epoch in tqdm(range(n_epoch)):\n",
    "    \n",
    "    D_loss = []\n",
    "    G_loss_D = []\n",
    "    G_loss_MSE = []\n",
    "    \n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        doc_embs, _, doc_ids = data\n",
    "\n",
    "        perm = torch.randperm(test_word_weight_tensor.size(0))\n",
    "        true_word_weights_sample = test_word_weight_tensor[perm[:bs]]\n",
    "\n",
    "        # Label\n",
    "        r_label = torch.ones((bs))\n",
    "        f_label = torch.zeros((doc_embs.size(0)))\n",
    "\n",
    "        r_logit = D(true_word_weights_sample.detach())\n",
    "        f_logit = D(model.emb(doc_ids).detach())\n",
    "        \n",
    "        # Compute the loss for the discriminator.\n",
    "        # r_loss = D_criterion(r_logit.squeeze(), r_label)\n",
    "        # f_loss = D_criterion(f_logit.squeeze(), f_label)\n",
    "        # loss_D = (r_loss + f_loss) / 2\n",
    "\n",
    "        # WGAN Loss\n",
    "        loss_D = -torch.mean(r_logit) + torch.mean(f_logit)\n",
    "\n",
    "        # Model backwarding\n",
    "        D.zero_grad()\n",
    "        loss_D.backward()\n",
    "\n",
    "        # Update the discriminator.\n",
    "        opt_D.step()\n",
    "        \n",
    "        D_loss.append(loss_D.item())\n",
    "        \"\"\" Clip weights of discriminator. \"\"\"\n",
    "        for p in D.parameters():\n",
    "            p.data.clamp_(-10, 10)\n",
    "\n",
    "        # ============================================\n",
    "        #  Train G\n",
    "        # ============================================\n",
    "        \n",
    "        if (epoch % n_critic == 0) and (epoch > 500):\n",
    "            # Generate some fake images.\n",
    "            f_imgs = model.emb(doc_ids)\n",
    "            f_logit = D(f_imgs)\n",
    "            \n",
    "            # Compute the loss for the generator.\n",
    "            # loss_G_Dis = D_criterion(f_logit.squeeze(), torch.ones((doc_embs.size(0))))\n",
    "            # WGAN Loss\n",
    "            loss_G_Dis = -torch.mean(f_logit)\n",
    "            \n",
    "            # MSE loss\n",
    "            pred_doc_embs = model(doc_ids, word_vectors_tensor)     \n",
    "            loss_G_MSE = G_criterion(pred_doc_embs, doc_embs)\n",
    "            \n",
    "            emb_w_sum = torch.sum(model.emb(doc_ids), axis=1)\n",
    "            \n",
    "            loss_G_SUM = G_criterion(emb_w_sum, torch.ones(emb_w_sum.shape))\n",
    "            \n",
    "            loss_G = loss_G_Dis * loss_weight[0] + loss_G_MSE * loss_weight[1] + loss_G_SUM * loss_weight[2]\n",
    "            # Model backwarding\n",
    "            model.zero_grad()\n",
    "            loss_G.backward()\n",
    "\n",
    "            # Update the generator.\n",
    "            opt_G.step()\n",
    "            \n",
    "            G_loss_D.append(loss_G_Dis.item())\n",
    "            G_loss_MSE.append(loss_G_MSE.item())\n",
    "            \n",
    "            \n",
    "            for p in model.parameters():\n",
    "                p.data.clamp_(clip_value, 100)\n",
    "        step += 1\n",
    "        \n",
    "    if epoch % 100 == 0:\n",
    "        res = {}\n",
    "        res['epoch'] = epoch\n",
    "        res['loss_D'] = np.mean(D_loss)\n",
    "        res['loss_G_D'] = np.mean(G_loss_D)\n",
    "        res['loss_G_MSE'] = np.mean(G_loss_MSE)\n",
    "\n",
    "        if verbose==1:\n",
    "            print('epoch', res['epoch'])\n",
    "            print('loss D', res['loss_D'])\n",
    "            print('loss G D', res['loss_G_D'])\n",
    "            print('loss G MSE', res['loss_G_MSE'])\n",
    "\n",
    "        res_ndcg = evaluate_GAN_NDCG(model, train_loader, verbose)\n",
    "        res.update(res_ndcg)\n",
    "        results.append(res)\n",
    "        \n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fb720c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss_D</th>\n",
       "      <th>loss_G_D</th>\n",
       "      <th>loss_G_MSE</th>\n",
       "      <th>ndcg@50</th>\n",
       "      <th>ndcg@100</th>\n",
       "      <th>ndcg@200</th>\n",
       "      <th>ndcg@all</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.008067</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008099</td>\n",
       "      <td>0.012895</td>\n",
       "      <td>0.021415</td>\n",
       "      <td>0.247514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>-0.997792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008099</td>\n",
       "      <td>0.012895</td>\n",
       "      <td>0.021415</td>\n",
       "      <td>0.247514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>-0.998910</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008099</td>\n",
       "      <td>0.012895</td>\n",
       "      <td>0.021415</td>\n",
       "      <td>0.247514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>-0.999512</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008099</td>\n",
       "      <td>0.012895</td>\n",
       "      <td>0.021415</td>\n",
       "      <td>0.247514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>-0.999736</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008099</td>\n",
       "      <td>0.012895</td>\n",
       "      <td>0.021415</td>\n",
       "      <td>0.247514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>-0.999715</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008099</td>\n",
       "      <td>0.012895</td>\n",
       "      <td>0.021415</td>\n",
       "      <td>0.247514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>-0.999777</td>\n",
       "      <td>-0.000217</td>\n",
       "      <td>0.030502</td>\n",
       "      <td>0.517476</td>\n",
       "      <td>0.542964</td>\n",
       "      <td>0.578284</td>\n",
       "      <td>0.687432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>-0.999771</td>\n",
       "      <td>-0.000217</td>\n",
       "      <td>0.015991</td>\n",
       "      <td>0.534578</td>\n",
       "      <td>0.564996</td>\n",
       "      <td>0.598504</td>\n",
       "      <td>0.704625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>-0.999879</td>\n",
       "      <td>-0.000120</td>\n",
       "      <td>0.011145</td>\n",
       "      <td>0.545412</td>\n",
       "      <td>0.573532</td>\n",
       "      <td>0.606564</td>\n",
       "      <td>0.710538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>-0.999916</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>0.008432</td>\n",
       "      <td>0.551277</td>\n",
       "      <td>0.579171</td>\n",
       "      <td>0.612358</td>\n",
       "      <td>0.714817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>-0.999914</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>0.006973</td>\n",
       "      <td>0.554990</td>\n",
       "      <td>0.583507</td>\n",
       "      <td>0.615343</td>\n",
       "      <td>0.717152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>-0.999949</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>0.005992</td>\n",
       "      <td>0.559286</td>\n",
       "      <td>0.584091</td>\n",
       "      <td>0.618387</td>\n",
       "      <td>0.718227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>-0.999951</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>0.005299</td>\n",
       "      <td>0.561886</td>\n",
       "      <td>0.587346</td>\n",
       "      <td>0.622032</td>\n",
       "      <td>0.720779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>-0.999957</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>0.004813</td>\n",
       "      <td>0.563542</td>\n",
       "      <td>0.590135</td>\n",
       "      <td>0.626129</td>\n",
       "      <td>0.722021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>-0.999966</td>\n",
       "      <td>-0.000034</td>\n",
       "      <td>0.004522</td>\n",
       "      <td>0.566999</td>\n",
       "      <td>0.593254</td>\n",
       "      <td>0.629887</td>\n",
       "      <td>0.724394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>-0.999948</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>0.004062</td>\n",
       "      <td>0.568216</td>\n",
       "      <td>0.595324</td>\n",
       "      <td>0.631348</td>\n",
       "      <td>0.725392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>-0.999956</td>\n",
       "      <td>-0.000036</td>\n",
       "      <td>0.003854</td>\n",
       "      <td>0.570683</td>\n",
       "      <td>0.598458</td>\n",
       "      <td>0.634031</td>\n",
       "      <td>0.727130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700</th>\n",
       "      <td>-0.999965</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>0.003605</td>\n",
       "      <td>0.572274</td>\n",
       "      <td>0.599881</td>\n",
       "      <td>0.634718</td>\n",
       "      <td>0.727779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>-0.999973</td>\n",
       "      <td>-0.000027</td>\n",
       "      <td>0.003392</td>\n",
       "      <td>0.573248</td>\n",
       "      <td>0.600923</td>\n",
       "      <td>0.637485</td>\n",
       "      <td>0.728812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>-0.999920</td>\n",
       "      <td>-0.000025</td>\n",
       "      <td>0.003273</td>\n",
       "      <td>0.574283</td>\n",
       "      <td>0.601827</td>\n",
       "      <td>0.639709</td>\n",
       "      <td>0.729756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>-0.979977</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>0.575475</td>\n",
       "      <td>0.602627</td>\n",
       "      <td>0.640070</td>\n",
       "      <td>0.730225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2100</th>\n",
       "      <td>-0.999955</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>0.003023</td>\n",
       "      <td>0.576320</td>\n",
       "      <td>0.604589</td>\n",
       "      <td>0.642404</td>\n",
       "      <td>0.731358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>-0.999978</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>0.002886</td>\n",
       "      <td>0.578084</td>\n",
       "      <td>0.606664</td>\n",
       "      <td>0.644632</td>\n",
       "      <td>0.733035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>-0.999982</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.002810</td>\n",
       "      <td>0.578769</td>\n",
       "      <td>0.608270</td>\n",
       "      <td>0.645653</td>\n",
       "      <td>0.733372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2400</th>\n",
       "      <td>-0.999983</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>0.002737</td>\n",
       "      <td>0.579642</td>\n",
       "      <td>0.609374</td>\n",
       "      <td>0.646120</td>\n",
       "      <td>0.733769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2500</th>\n",
       "      <td>-0.999979</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>0.002620</td>\n",
       "      <td>0.583473</td>\n",
       "      <td>0.612641</td>\n",
       "      <td>0.648868</td>\n",
       "      <td>0.736467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600</th>\n",
       "      <td>-0.999978</td>\n",
       "      <td>-0.000018</td>\n",
       "      <td>0.002563</td>\n",
       "      <td>0.583898</td>\n",
       "      <td>0.613687</td>\n",
       "      <td>0.649443</td>\n",
       "      <td>0.736930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2700</th>\n",
       "      <td>-0.999977</td>\n",
       "      <td>-0.000017</td>\n",
       "      <td>0.002488</td>\n",
       "      <td>0.584577</td>\n",
       "      <td>0.613922</td>\n",
       "      <td>0.649634</td>\n",
       "      <td>0.737300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2800</th>\n",
       "      <td>-0.999984</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>0.002456</td>\n",
       "      <td>0.585321</td>\n",
       "      <td>0.614418</td>\n",
       "      <td>0.650302</td>\n",
       "      <td>0.737311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2900</th>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.002431</td>\n",
       "      <td>0.585618</td>\n",
       "      <td>0.615119</td>\n",
       "      <td>0.650718</td>\n",
       "      <td>0.737680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.002367</td>\n",
       "      <td>0.585878</td>\n",
       "      <td>0.615110</td>\n",
       "      <td>0.652003</td>\n",
       "      <td>0.737835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3100</th>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.002285</td>\n",
       "      <td>0.586957</td>\n",
       "      <td>0.615942</td>\n",
       "      <td>0.652952</td>\n",
       "      <td>0.738349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3200</th>\n",
       "      <td>-0.999984</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.002253</td>\n",
       "      <td>0.588549</td>\n",
       "      <td>0.617111</td>\n",
       "      <td>0.653784</td>\n",
       "      <td>0.739182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3300</th>\n",
       "      <td>-0.999966</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.002201</td>\n",
       "      <td>0.589273</td>\n",
       "      <td>0.617266</td>\n",
       "      <td>0.654562</td>\n",
       "      <td>0.739493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3400</th>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.002195</td>\n",
       "      <td>0.589588</td>\n",
       "      <td>0.617955</td>\n",
       "      <td>0.655619</td>\n",
       "      <td>0.739905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3500</th>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.002151</td>\n",
       "      <td>0.591452</td>\n",
       "      <td>0.619504</td>\n",
       "      <td>0.656770</td>\n",
       "      <td>0.741229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3600</th>\n",
       "      <td>-0.999990</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.002101</td>\n",
       "      <td>0.592105</td>\n",
       "      <td>0.619800</td>\n",
       "      <td>0.657598</td>\n",
       "      <td>0.741512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3700</th>\n",
       "      <td>-0.999987</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.002064</td>\n",
       "      <td>0.591801</td>\n",
       "      <td>0.620134</td>\n",
       "      <td>0.658653</td>\n",
       "      <td>0.741859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3800</th>\n",
       "      <td>-0.999986</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.002058</td>\n",
       "      <td>0.592897</td>\n",
       "      <td>0.620609</td>\n",
       "      <td>0.658657</td>\n",
       "      <td>0.741894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3900</th>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>0.593261</td>\n",
       "      <td>0.621879</td>\n",
       "      <td>0.659854</td>\n",
       "      <td>0.742270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>-0.999989</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.001994</td>\n",
       "      <td>0.593394</td>\n",
       "      <td>0.622698</td>\n",
       "      <td>0.660059</td>\n",
       "      <td>0.742553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4100</th>\n",
       "      <td>-0.999987</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.001958</td>\n",
       "      <td>0.594867</td>\n",
       "      <td>0.623461</td>\n",
       "      <td>0.660992</td>\n",
       "      <td>0.743129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4200</th>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.001922</td>\n",
       "      <td>0.595417</td>\n",
       "      <td>0.623588</td>\n",
       "      <td>0.661312</td>\n",
       "      <td>0.743377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4300</th>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.001908</td>\n",
       "      <td>0.595789</td>\n",
       "      <td>0.624437</td>\n",
       "      <td>0.662186</td>\n",
       "      <td>0.743853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4400</th>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.001905</td>\n",
       "      <td>0.596158</td>\n",
       "      <td>0.624997</td>\n",
       "      <td>0.662875</td>\n",
       "      <td>0.744157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4500</th>\n",
       "      <td>-0.979986</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.001861</td>\n",
       "      <td>0.597081</td>\n",
       "      <td>0.626641</td>\n",
       "      <td>0.664226</td>\n",
       "      <td>0.745182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>-0.999991</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.001850</td>\n",
       "      <td>0.597768</td>\n",
       "      <td>0.627512</td>\n",
       "      <td>0.664680</td>\n",
       "      <td>0.745691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4700</th>\n",
       "      <td>-0.999992</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>0.597654</td>\n",
       "      <td>0.627123</td>\n",
       "      <td>0.664364</td>\n",
       "      <td>0.745598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4800</th>\n",
       "      <td>-0.999982</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.001809</td>\n",
       "      <td>0.598444</td>\n",
       "      <td>0.628012</td>\n",
       "      <td>0.665495</td>\n",
       "      <td>0.746171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4900</th>\n",
       "      <td>-0.999975</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.001798</td>\n",
       "      <td>0.598591</td>\n",
       "      <td>0.628570</td>\n",
       "      <td>0.665689</td>\n",
       "      <td>0.746242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss_D  loss_G_D  loss_G_MSE   ndcg@50  ndcg@100  ndcg@200  ndcg@all\n",
       "epoch                                                                        \n",
       "0     -0.008067       NaN         NaN  0.008099  0.012895  0.021415  0.247514\n",
       "100   -0.997792       NaN         NaN  0.008099  0.012895  0.021415  0.247514\n",
       "200   -0.998910       NaN         NaN  0.008099  0.012895  0.021415  0.247514\n",
       "300   -0.999512       NaN         NaN  0.008099  0.012895  0.021415  0.247514\n",
       "400   -0.999736       NaN         NaN  0.008099  0.012895  0.021415  0.247514\n",
       "500   -0.999715       NaN         NaN  0.008099  0.012895  0.021415  0.247514\n",
       "600   -0.999777 -0.000217    0.030502  0.517476  0.542964  0.578284  0.687432\n",
       "700   -0.999771 -0.000217    0.015991  0.534578  0.564996  0.598504  0.704625\n",
       "800   -0.999879 -0.000120    0.011145  0.545412  0.573532  0.606564  0.710538\n",
       "900   -0.999916 -0.000084    0.008432  0.551277  0.579171  0.612358  0.714817\n",
       "1000  -0.999914 -0.000062    0.006973  0.554990  0.583507  0.615343  0.717152\n",
       "1100  -0.999949 -0.000051    0.005992  0.559286  0.584091  0.618387  0.718227\n",
       "1200  -0.999951 -0.000049    0.005299  0.561886  0.587346  0.622032  0.720779\n",
       "1300  -0.999957 -0.000041    0.004813  0.563542  0.590135  0.626129  0.722021\n",
       "1400  -0.999966 -0.000034    0.004522  0.566999  0.593254  0.629887  0.724394\n",
       "1500  -0.999948 -0.000031    0.004062  0.568216  0.595324  0.631348  0.725392\n",
       "1600  -0.999956 -0.000036    0.003854  0.570683  0.598458  0.634031  0.727130\n",
       "1700  -0.999965 -0.000031    0.003605  0.572274  0.599881  0.634718  0.727779\n",
       "1800  -0.999973 -0.000027    0.003392  0.573248  0.600923  0.637485  0.728812\n",
       "1900  -0.999920 -0.000025    0.003273  0.574283  0.601827  0.639709  0.729756\n",
       "2000  -0.979977 -0.000022    0.003187  0.575475  0.602627  0.640070  0.730225\n",
       "2100  -0.999955 -0.000021    0.003023  0.576320  0.604589  0.642404  0.731358\n",
       "2200  -0.999978 -0.000020    0.002886  0.578084  0.606664  0.644632  0.733035\n",
       "2300  -0.999982 -0.000018    0.002810  0.578769  0.608270  0.645653  0.733372\n",
       "2400  -0.999983 -0.000017    0.002737  0.579642  0.609374  0.646120  0.733769\n",
       "2500  -0.999979 -0.000016    0.002620  0.583473  0.612641  0.648868  0.736467\n",
       "2600  -0.999978 -0.000018    0.002563  0.583898  0.613687  0.649443  0.736930\n",
       "2700  -0.999977 -0.000017    0.002488  0.584577  0.613922  0.649634  0.737300\n",
       "2800  -0.999984 -0.000016    0.002456  0.585321  0.614418  0.650302  0.737311\n",
       "2900  -0.999986 -0.000014    0.002431  0.585618  0.615119  0.650718  0.737680\n",
       "3000  -0.999986 -0.000013    0.002367  0.585878  0.615110  0.652003  0.737835\n",
       "3100  -0.999986 -0.000013    0.002285  0.586957  0.615942  0.652952  0.738349\n",
       "3200  -0.999984 -0.000012    0.002253  0.588549  0.617111  0.653784  0.739182\n",
       "3300  -0.999966 -0.000012    0.002201  0.589273  0.617266  0.654562  0.739493\n",
       "3400  -0.999989 -0.000011    0.002195  0.589588  0.617955  0.655619  0.739905\n",
       "3500  -0.999989 -0.000011    0.002151  0.591452  0.619504  0.656770  0.741229\n",
       "3600  -0.999990 -0.000010    0.002101  0.592105  0.619800  0.657598  0.741512\n",
       "3700  -0.999987 -0.000010    0.002064  0.591801  0.620134  0.658653  0.741859\n",
       "3800  -0.999986 -0.000010    0.002058  0.592897  0.620609  0.658657  0.741894\n",
       "3900  -0.999991 -0.000009    0.002006  0.593261  0.621879  0.659854  0.742270\n",
       "4000  -0.999989 -0.000009    0.001994  0.593394  0.622698  0.660059  0.742553\n",
       "4100  -0.999987 -0.000009    0.001958  0.594867  0.623461  0.660992  0.743129\n",
       "4200  -0.999991 -0.000008    0.001922  0.595417  0.623588  0.661312  0.743377\n",
       "4300  -0.999992 -0.000008    0.001908  0.595789  0.624437  0.662186  0.743853\n",
       "4400  -0.999991 -0.000008    0.001905  0.596158  0.624997  0.662875  0.744157\n",
       "4500  -0.979986 -0.000008    0.001861  0.597081  0.626641  0.664226  0.745182\n",
       "4600  -0.999991 -0.000008    0.001850  0.597768  0.627512  0.664680  0.745691\n",
       "4700  -0.999992 -0.000007    0.001811  0.597654  0.627123  0.664364  0.745598\n",
       "4800  -0.999982 -0.000007    0.001809  0.598444  0.628012  0.665495  0.746171\n",
       "4900  -0.999975 -0.000007    0.001798  0.598591  0.628570  0.665689  0.746242"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.groupby(by=['epoch']).mean().plot()\n",
    "results_df.groupby(by=['epoch']).mean()\n",
    "# results_df.groupby(by=['epoch']).mean().iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7c2ec4",
   "metadata": {},
   "source": [
    "## Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a800cf13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth\n",
      " ['homeless', 'street', 'bet', 'molli', 'rich', 'mel', 'brook', 'lose', 'blaze', 'frankenstein', 'monitor', 'iraq', 'sidewalk', 'sissi', 'mutual', 'bathroom', 'sailor', 'elect', 'racism', 'pal', 'warren', 'flight', 'survivor', 'slapstick', 'prior', 'wilson', 'leg', 'tender', 'howard', 'rival', 'succeed', 'thrown', 'ann', 'vote', 'wall', 'lesson', 'month', 'stupid', 'project', 'step', 'connect', 'reach', 'plan', 'georg', 'build', 'caus', 'state', 'futur', 'dean', 'pan']\n",
      "prediction\n",
      " ['street', 'homeless', 'poor', 'whether', 'bet', 'public', 'alright', 'fund', 'dig', 'manhattan', 'elect', 'vote', 'rival', 'fix', 'project', 'hell', 'worst', 'trust', 'sharon', 'fire', 'sick', 'theater', 'ride', 'nina', 'break', 'neighborhood', 'chase', 'harlem', 'pay', 'bond', 'hurt', 'wall', 'pregnant', 'brent', 'park', 'million', 'fox', 'dog', 'ted', 'subway', 'oil', 'cliff', 'eve', 'drain', 'hey', 'program', 'grade', 'difficult', 'alley', 'rich']\n"
     ]
    }
   ],
   "source": [
    "word_list = dataset.vocab.itos\n",
    "# select doc_id and k\n",
    "doc_id = 1\n",
    "topk = 50\n",
    "\n",
    "gt = [word_list[word_idx] for word_idx in np.argsort(tfidf_ans[doc_id])[::-1][:topk]]\n",
    "pred = [word_list[word_idx] for word_idx in np.argsort(model.emb.weight.data[doc_id].numpy())[::-1][:topk]]\n",
    "\n",
    "print('ground truth\\n', gt)\n",
    "print('prediction\\n', pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc34eb9b",
   "metadata": {},
   "source": [
    "## LassoGAN train seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c031f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_GAN_NDCG(w, tfidf_ans, doc_id, verbose=1):\n",
    "    results = {}\n",
    "    \n",
    "    scores = w.detach().numpy().reshape(1, -1)\n",
    "    true_relevance = tfidf_ans[doc_id].reshape(1, -1)\n",
    "\n",
    "    \n",
    "    results['ndcg@50'] = (ndcg_score(true_relevance, scores, k=50))\n",
    "    results['ndcg@100'] = (ndcg_score(true_relevance, scores, k=100))\n",
    "    results['ndcg@200'] = (ndcg_score(true_relevance, scores, k=200))\n",
    "    results['ndcg@all'] = (ndcg_score(true_relevance, scores, k=None))\n",
    "    \n",
    "    if verbose == 1:\n",
    "        print('NDCG top50', results['ndcg@50'])\n",
    "        print('NDCG top100', results['ndcg@100'])\n",
    "        print('NDCG top200', results['ndcg@200'])\n",
    "        print('NDCG ALL', results['ndcg@all'])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fcf420",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "n_epoch = 1000\n",
    "\n",
    "n_critic = 1\n",
    "clip_value = 1\n",
    "bs = 1\n",
    "loss_weight = [0, 1]\n",
    "\n",
    "num_words = word_vectors.shape[0]\n",
    "results_all = []\n",
    "\n",
    "for doc_id in tqdm(range(len(document_vectors_tensor[:10]))):\n",
    "    doc_emb = document_vectors_tensor[doc_id]\n",
    "    test_word_weight_tensor = torch.FloatTensor(np.concatenate((tfidf_ans[:doc_id], tfidf_ans[doc_id+1:])))\n",
    "\n",
    "    w = torch.zeros(num_words).requires_grad_(True)\n",
    "    D = Discriminator(num_words=num_words, h_dim=64)\n",
    "    D.train()\n",
    "\n",
    "    # opt_G = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    # opt_D = torch.optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    opt_G = torch.optim.SGD([w], lr=lr)\n",
    "    opt_D = torch.optim.SGD(D.parameters(), lr=lr)\n",
    "\n",
    "    G_criterion = nn.MSELoss(reduction='mean')\n",
    "    D_criterion = nn.BCELoss()\n",
    "\n",
    "    results = []\n",
    "    for epoch in range(n_epoch):\n",
    "\n",
    "        D_loss = []\n",
    "        G_loss_D = []\n",
    "        G_loss_MSE = []\n",
    "\n",
    "        perm = torch.randperm(test_word_weight_tensor.size(0))\n",
    "        true_word_weights_sample = test_word_weight_tensor[perm[:bs]]\n",
    "        \"\"\" Medium: Use WGAN Loss. \"\"\"\n",
    "        # Label\n",
    "        r_label = torch.ones((bs))\n",
    "        f_label = torch.zeros((bs))\n",
    "\n",
    "        # Model forwarding\n",
    "        r_logit = D(true_word_weights_sample.detach())\n",
    "        f_logit = D(w.detach())\n",
    "\n",
    "        # Compute the loss for the discriminator.\n",
    "        # r_loss = D_criterion(r_logit.squeeze(), r_label)\n",
    "        # f_loss = D_criterion(f_logit.squeeze(), f_label)\n",
    "        # loss_D = (r_loss + f_loss) / 2\n",
    "\n",
    "        # WGAN Loss\n",
    "        loss_D = -torch.mean(r_logit) + torch.mean(f_logit)\n",
    "\n",
    "        # Model backwarding\n",
    "        D.zero_grad()\n",
    "        loss_D.backward()\n",
    "\n",
    "        # Update the discriminator.\n",
    "        opt_D.step()\n",
    "\n",
    "        D_loss.append(loss_D.item())\n",
    "        \"\"\" Medium: Clip weights of discriminator. \"\"\"\n",
    "        for p in D.parameters():\n",
    "            p.data.clamp_(-clip_value, clip_value)\n",
    "\n",
    "        # ============================================\n",
    "        #  Train G\n",
    "        # ============================================\n",
    "        if epoch % n_critic == 0:\n",
    "            # Generate some fake images.\n",
    "            f_logit = D(w)\n",
    "\n",
    "            \"\"\" Medium: Use WGAN Loss\"\"\"\n",
    "            # Compute the loss for the generator.\n",
    "            # loss_G_Dis = D_criterion(f_logit.squeeze(), r_label)\n",
    "            # WGAN Loss\n",
    "            loss_G_Dis = -torch.mean(f_logit)\n",
    "\n",
    "            # MSE loss\n",
    "            pred_doc_embs = w @ word_vectors_tensor\n",
    "            loss_G_MSE = G_criterion(pred_doc_embs, doc_emb)\n",
    "\n",
    "            loss_G = loss_G_Dis * loss_weight[0] + loss_G_MSE * loss_weight[1]\n",
    "\n",
    "            # Model backwarding\n",
    "            if w.grad is not None:\n",
    "                w.grad.zero_()\n",
    "            \n",
    "            loss_G.backward()\n",
    "            \n",
    "            # Update the generator.\n",
    "            opt_G.step()\n",
    "\n",
    "            G_loss_D.append(loss_G_Dis.item())\n",
    "            G_loss_MSE.append(loss_G_MSE.item())\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            res = {}\n",
    "            res['epoch'] = epoch\n",
    "            res['loss_D'] = np.mean(D_loss)\n",
    "            res['loss_G_D'] = np.mean(G_loss_D)\n",
    "            res['loss_G_MSE'] = np.mean(G_loss_MSE)\n",
    "            \n",
    "            print('epoch', epoch)\n",
    "            print('loss D', np.mean(D_loss))\n",
    "            print('loss G D', np.mean(G_loss_D))\n",
    "            print('loss G MSE', np.mean(G_loss_MSE))\n",
    "            \n",
    "            res_ndcg = evaluate_GAN_NDCG(w, tfidf_ans, doc_id)\n",
    "            res.update(res_ndcg)\n",
    "            results.append(res)\n",
    "\n",
    "    results_all.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40069345",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = []\n",
    "for r in results_all:\n",
    "    r = pd.DataFrame(r)\n",
    "    results_df.append(r)\n",
    "\n",
    "results_df = pd.concat(results_df)\n",
    "results_df.groupby(by=['epoch']).mean().plot()\n",
    "results_df.groupby(by=['epoch']).mean()\n",
    "# results_df.groupby(by=['epoch']).mean().iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3827045d",
   "metadata": {},
   "source": [
    "## Nearest Guessing, so bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd99ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embs_IDF = word_embs.copy()\n",
    "\n",
    "for word, IDF in dataset.vocab.IDF.items():\n",
    "    word_idx = dataset.vocab.stoi[word]\n",
    "    word_embs_IDF[word_idx] *= IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db6d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr, re = [[],[],[]], [[],[],[]]\n",
    "ndcgs = defaultdict(list)\n",
    "\n",
    "for uid, uemb in enumerate(tqdm(doc_embs[:100])):\n",
    "    y = uemb\n",
    "    word_embs_IDF\n",
    "    word_weight = np.dot(word_embs_IDF, y)\n",
    "    \n",
    "    m1 = metric4(word_weight, doc_answers[uid], w_idx=None, topk=50, verbose=0)\n",
    "    m2 = metric4(word_weight, doc_answers[uid], w_idx=None, topk=100, verbose=0)\n",
    "    m3 = metric4(word_weight, doc_answers[uid], w_idx=None, topk=200, verbose=0)\n",
    "    ndcg1 = metric_ndcg(word_weight, doc_answers[uid], topk=50, verbose=0)\n",
    "    ndcg2 = metric_ndcg(word_weight, doc_answers[uid], topk=100, verbose=0)\n",
    "    ndcg3 = metric_ndcg(word_weight, doc_answers[uid], topk=200, verbose=0)\n",
    "    ndcg4 = metric_ndcg(word_weight, doc_answers[uid], topk=None, verbose=0)\n",
    "    pr[0].append(m1[\"precision\"])\n",
    "    re[0].append(m1[\"recall\"])\n",
    "    pr[1].append(m2[\"precision\"])\n",
    "    re[1].append(m2[\"recall\"])\n",
    "    pr[2].append(m3[\"precision\"])\n",
    "    re[2].append(m3[\"recall\"])\n",
    "    \n",
    "    ndcgs[\"50\"].append(ndcg1)\n",
    "    ndcgs[\"100\"].append(ndcg2)\n",
    "    ndcgs[\"200\"].append(ndcg3)\n",
    "    ndcgs[\"-1\"].append(ndcg4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d4d93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Precision:{np.mean(pr[0]):.4f} Recall:{np.mean(re[0]):.4f}\")\n",
    "print(f\"Precision:{np.mean(pr[1]):.4f} Recall:{np.mean(re[1]):.4f}\")\n",
    "print(f\"Precision:{np.mean(pr[2]):.4f} Recall:{np.mean(re[2]):.4f}\")\n",
    "print(f\"NDCG 50:{np.mean(ndcgs['50']):.4f}\")\n",
    "print(f\"NDCG 100:{np.mean(ndcgs['100']):.4f}\")\n",
    "print(f\"NDCG 200:{np.mean(ndcgs['200']):.4f}\")\n",
    "print(f\"NDCG all:{np.mean(ndcgs['-1']):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a9e30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## test dcg\n",
    "# from sklearn.metrics import ndcg_score, dcg_score\n",
    "# k=2\n",
    "\n",
    "# true_relevance = np.asarray([[1, 2, 3, 4]])\n",
    "# scores = np.asarray([[1, 2, 3, 2.5]])\n",
    "# print('dcg',dcg_score(true_relevance, scores,k=k))\n",
    "# print('ndcg',ndcg_score(true_relevance, scores,k=k))\n",
    "\n",
    "\n",
    "# w = 1 / (np.log(np.arange(true_relevance.shape[1])[:k] + 2) / np.log(2))\n",
    "# dcg = true_relevance[0][np.argsort(scores)[0][::-1][:k]].dot(w)\n",
    "# print(dcg)\n",
    "\n",
    "# idcg = np.sort(true_relevance[0])[::-1][:k].dot(w)\n",
    "# print(dcg/idcg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
