{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a67d8234",
   "metadata": {},
   "source": [
    "### raw data\n",
    "* word embedding: glove\n",
    "* doc text: ./data/IMDB.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792dad29",
   "metadata": {},
   "source": [
    "### dataset\n",
    "1. IMDB\n",
    "2. CNNNews\n",
    "3. [PubMed](https://github.com/LIAAD/KeywordExtractor-Datasets/blob/master/datasets/PubMed.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b007799a",
   "metadata": {},
   "source": [
    "### preprocess\n",
    "1. filter too frequent and less frequent words\n",
    "2. stemming\n",
    "3. document vector aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6a82ca",
   "metadata": {},
   "source": [
    "### model\n",
    "1. TopK\n",
    "2. Sklearn\n",
    "3. Our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b9967b",
   "metadata": {},
   "source": [
    "### evaluation\n",
    "1. F1\n",
    "2. NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9beff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np \n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from itertools import cycle\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Used to get the data\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "seed = 33\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2341254",
   "metadata": {},
   "source": [
    "## Preprocess config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc01df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "config[\"dataset\"] = \"CNN\" # \"IMDB\" \"CNN\", \"PubMed\"\n",
    "config[\"n_document\"] = 100\n",
    "config[\"normalize_word_embedding\"] = False\n",
    "config[\"min_word_freq_threshold\"] = 20\n",
    "config[\"topk_word_freq_threshold\"] = 100\n",
    "config[\"document_vector_agg_weight\"] = 'IDF' # ['mean', 'IDF', 'uniform', 'gaussian', 'exponential', 'pmi']\n",
    "config[\"document_vector_weight_normalize\"] = True # weighted sum or mean, True for mean, False for sum \n",
    "config[\"select_topk_TFIDF\"] = None # ignore\n",
    "config[\"embedding_file\"] = \"../data/glove.6B.100d.txt\"\n",
    "config[\"topk\"] = [10, 30, 50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b0b76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_notebook():\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        if 'IPKernelApp' not in get_ipython().config:  # pragma: no cover\n",
    "            return False\n",
    "    except ImportError:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc91521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word2emb(embedding_file):\n",
    "    word2embedding = dict()\n",
    "    word_dim = int(re.findall(r\".(\\d+)d\", embedding_file)[0])\n",
    "\n",
    "    with open(embedding_file, \"r\") as f:\n",
    "        for line in tqdm(f):\n",
    "            line = line.strip().split()\n",
    "            word = line[0]\n",
    "            embedding = list(map(float, line[1:]))\n",
    "            word2embedding[word] = np.array(embedding)\n",
    "\n",
    "    print(\"Number of words:%d\" % len(word2embedding))\n",
    "\n",
    "    return word2embedding\n",
    "\n",
    "word2embedding = load_word2emb(config[\"embedding_file\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bfc175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_wordemb(word2embedding):\n",
    "    # Every word emb should have norm 1\n",
    "    \n",
    "    word_emb = []\n",
    "    word_list = []\n",
    "    for word, emb in word2embedding.items():\n",
    "        word_list.append(word)\n",
    "        word_emb.append(emb)\n",
    "\n",
    "    word_emb = np.array(word_emb)\n",
    "\n",
    "    for i in range(len(word_emb)):\n",
    "        norm = np.linalg.norm(word_emb[i])\n",
    "        word_emb[i] = word_emb[i] / norm\n",
    "\n",
    "    for word, emb in tqdm(zip(word_list, word_emb)):\n",
    "        word2embedding[word] = emb\n",
    "    return word2embedding\n",
    "\n",
    "if config[\"normalize_word_embedding\"]:\n",
    "    normalize_wordemb(word2embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1233053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, word2embedding, config):\n",
    "        # The low frequency words will be assigned as <UNK> token\n",
    "        self.itos = {0: \"<UNK>\"}\n",
    "        self.stoi = {\"<UNK>\": 0}\n",
    "        \n",
    "        self.word2embedding = word2embedding\n",
    "        self.config = config\n",
    "\n",
    "        self.word_freq_in_corpus = defaultdict(int)\n",
    "        self.IDF = {}\n",
    "        self.ps = PorterStemmer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        self.word_dim = len(word2embedding['the'])\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def tokenizer_eng(self, text):\n",
    "        text = re.sub(r'[^A-Za-z0-9 ]+', '', text)\n",
    "        text = text.strip().split()\n",
    "        \n",
    "        return [self.ps.stem(w) for w in text if w.lower() not in self.stop_words]\n",
    "    \n",
    "    def read_raw(self):        \n",
    "        if self.config[\"dataset\"] == 'IMDB':\n",
    "            data_file_path = '../data/IMDB.txt'\n",
    "        elif self.config[\"dataset\"] == 'CNN':\n",
    "            data_file_path = '../data/CNN.txt'\n",
    "        elif self.config[\"dataset\"] == 'PubMed':\n",
    "            data_file_path = '../data/PubMed.txt'\n",
    "        \n",
    "        # raw documents\n",
    "        self.raw_documents = []\n",
    "        with open(data_file_path,'r',encoding='utf-8') as f:\n",
    "            for line in tqdm(f, desc=\"Loading documents\"):\n",
    "                self.raw_documents.append(line.strip(\"\\n\"))\n",
    "                \n",
    "        return self.raw_documents\n",
    "    \n",
    "    def build_vocabulary(self):\n",
    "        sentence_list = self.raw_documents\n",
    "        \n",
    "        self.doc_freq = defaultdict(int) # # of document a word appear\n",
    "        self.document_num = len(sentence_list)\n",
    "        self.word_vectors = [[0]*self.word_dim] # unknown word emb\n",
    "        \n",
    "        for sentence in tqdm(sentence_list, desc=\"Preprocessing documents\"):\n",
    "            # for doc_freq\n",
    "            document_words = set()\n",
    "            \n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                # pass unknown word\n",
    "                if word not in self.word2embedding:\n",
    "                    continue\n",
    "                    \n",
    "                # calculate word freq\n",
    "                self.word_freq_in_corpus[word] += 1\n",
    "                document_words.add(word)\n",
    "                \n",
    "            for word in document_words:\n",
    "                self.doc_freq[word] += 1\n",
    "        \n",
    "        # calculate IDF\n",
    "        print('doc num', self.document_num)\n",
    "        for word, freq in self.doc_freq.items():\n",
    "            self.IDF[word] = math.log(self.document_num / (freq+1))\n",
    "        \n",
    "        # delete less freq words:\n",
    "        delete_words = []\n",
    "        for word, v in self.word_freq_in_corpus.items():\n",
    "            if v < self.config[\"min_word_freq_threshold\"]:\n",
    "                delete_words.append(word)     \n",
    "        for word in delete_words:\n",
    "            del self.IDF[word]    \n",
    "            del self.word_freq_in_corpus[word]    \n",
    "        \n",
    "        # delete too freq words\n",
    "        print('eliminate freq words')\n",
    "        IDF = [(word, freq) for word, freq in self.IDF.items()]\n",
    "        IDF.sort(key=lambda x: x[1])\n",
    "\n",
    "        for i in range(self.config[\"topk_word_freq_threshold\"]):\n",
    "            print(word)\n",
    "            word = IDF[i][0]\n",
    "            del self.IDF[word]\n",
    "            del self.word_freq_in_corpus[word]\n",
    "        \n",
    "        # construct word_vectors\n",
    "        idx = 1\n",
    "        for word in self.word_freq_in_corpus:\n",
    "            self.word_vectors.append(self.word2embedding[word])\n",
    "            self.stoi[word] = idx\n",
    "            self.itos[idx] = word\n",
    "            idx += 1\n",
    "            \n",
    "    def init_word_weight(self,sentence_list, agg):\n",
    "        if agg == 'mean':\n",
    "            self.word_weight = {word: 1 for word in self.IDF.keys()}\n",
    "        elif agg == 'IDF':\n",
    "            self.word_weight = self.IDF\n",
    "        elif agg == 'uniform':\n",
    "            self.word_weight = {word: np.random.uniform(low=0.0, high=1.0) for word in self.IDF.keys()}\n",
    "        elif agg == 'gaussian':\n",
    "            mu, sigma = 10, 1 # mean and standard deviation\n",
    "            self.word_weight = {word: np.random.normal(mu, sigma) for word in self.IDF.keys()}\n",
    "        elif agg == 'exponential':\n",
    "            self.word_weight = {word: np.random.exponential(scale=1.0) for word in self.IDF.keys()}\n",
    "        elif agg == 'pmi':\n",
    "            trigram_measures = BigramAssocMeasures()\n",
    "            self.word_weight = defaultdict(int)\n",
    "            corpus = []\n",
    "\n",
    "            for text in tqdm(sentence_list):\n",
    "                corpus.extend(text.split())\n",
    "\n",
    "            finder = BigramCollocationFinder.from_words(corpus)\n",
    "            for pmi_score in finder.score_ngrams(trigram_measures.pmi):\n",
    "                pair, score = pmi_score\n",
    "                self.word_weight[pair[0]] += score\n",
    "                self.word_weight[pair[1]] += score\n",
    "                \n",
    "    def calculate_document_vector(self):\n",
    "        # Return\n",
    "        # document_vectors: weighted sum of word emb\n",
    "        # document_answers_idx: doc to word index list\n",
    "        # document_answers_wsum: word weight summation, e.g. total TFIDF score of a doc\n",
    "        \n",
    "        document_vectors = [] \n",
    "        document_answers = []\n",
    "        document_answers_wsum = []\n",
    "        \n",
    "        sentence_list = self.raw_documents\n",
    "        agg = self.config[\"document_vector_agg_weight\"]\n",
    "        n_document = self.config[\"n_document\"]\n",
    "        select_topk_TFIDF = self.config[\"select_topk_TFIDF\"]\n",
    "        \n",
    "        self.init_word_weight(sentence_list, agg)\n",
    "        for sentence in tqdm(sentence_list[:min(n_document, len(sentence_list))], desc=\"calculate document vectors\"):\n",
    "            document_vector = np.zeros(len(self.word_vectors[0]))\n",
    "            select_words = []\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                # pass unknown word\n",
    "                if word not in self.stoi:\n",
    "                    continue\n",
    "                else:\n",
    "                    select_words.append(word)\n",
    "\n",
    "            # select topk TDIDF\n",
    "            if select_topk_TFIDF is not None:\n",
    "                doc_TFIDF = defaultdict(float)\n",
    "                for word in select_words:    \n",
    "                    doc_TFIDF[word] += self.IDF[word]\n",
    "\n",
    "                doc_TFIDF_l = [(word, TFIDF) for word, TFIDF in doc_TFIDF.items()]\n",
    "                doc_TFIDF_l.sort(key=lambda x:x[1], reverse=True)\n",
    "                \n",
    "                select_topk_words = set(list(map(lambda x:x[0], doc_TFIDF_l[:select_topk_TFIDF])))\n",
    "                select_words = [word for word in select_words if word in select_topk_words]\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            total_weight = 0\n",
    "            # aggregate to doc vectors\n",
    "            for word in select_words:\n",
    "                document_vector += np.array(self.word2embedding[word]) * self.word_weight[word]\n",
    "                total_weight += self.word_weight[word]\n",
    "                \n",
    "            if len(select_words) == 0:\n",
    "                print('error', sentence)\n",
    "                continue\n",
    "            else:\n",
    "                if self.config[\"document_vector_weight_normalize\"]:\n",
    "                    document_vector /= total_weight\n",
    "                    total_weight = 1\n",
    "            \n",
    "            document_vectors.append(document_vector)\n",
    "            document_answers.append(select_words)\n",
    "            document_answers_wsum.append(total_weight)\n",
    "        \n",
    "        # get answers\n",
    "        document_answers_idx = []    \n",
    "        for ans in document_answers:\n",
    "            ans_idx = []\n",
    "            for token in ans:\n",
    "                if token in self.stoi:\n",
    "                    ans_idx.append(self.stoi[token])                    \n",
    "            document_answers_idx.append(ans_idx)\n",
    "        \n",
    "        self.document_vectors = document_vectors\n",
    "        self.document_answers_idx = document_answers_idx\n",
    "        self.document_answers_wsum = document_answers_wsum\n",
    "        \n",
    "        return document_vectors, document_answers_idx, document_answers_wsum\n",
    "        \n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]\n",
    "    \n",
    "    def check_docemb(self):\n",
    "        word_vectors = np.array(self.word_vectors)\n",
    "        pred = np.zeros(word_vectors.shape[1])\n",
    "        cnt = 0\n",
    "\n",
    "        for word_idx in self.document_answers_idx[0]:\n",
    "            pred += word_vectors[word_idx] * self.word_weight[self.itos[word_idx]]\n",
    "            cnt += self.word_weight[self.itos[word_idx]]\n",
    "        \n",
    "        if self.config[\"document_vector_weight_normalize\"]:\n",
    "            pred /= cnt\n",
    "        assert np.sum(self.document_vectors[0]) - np.sum(pred) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361a0b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(config, word2embedding):\n",
    "    # build vocabulary\n",
    "    vocab = Vocabulary(word2embedding, config)\n",
    "    vocab.read_raw()\n",
    "    vocab.build_vocabulary()\n",
    "    vocab_size = len(vocab)\n",
    "    # get doc emb\n",
    "    vocab.calculate_document_vector()\n",
    "    vocab.check_docemb()\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(config, word2embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e6d657",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Finish building dataset!\")\n",
    "print(f\"Number of documents:{len(vocab.raw_documents)}\")\n",
    "print(f\"Number of words:{len(vocab)}\")\n",
    "\n",
    "l = list(map(len, vocab.document_answers_idx))\n",
    "print(\"Average length of document:\", np.mean(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c161ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = np.array(vocab.word_vectors)\n",
    "print(\"word_vectors:\", word_vectors.shape)\n",
    "\n",
    "document_vectors = np.array(vocab.document_vectors)\n",
    "print(\"document_vectors\", document_vectors.shape)\n",
    "\n",
    "document_answers_wsum = np.array(vocab.document_answers_wsum).reshape(-1, 1)\n",
    "print(\"document_answers_wsum\", document_answers_wsum.shape)\n",
    "\n",
    "# create weight_ans\n",
    "document_answers_idx = vocab.document_answers_idx\n",
    "\n",
    "# random shuffle\n",
    "shuffle_idx = list(range(len(document_vectors)))\n",
    "random.Random(seed).shuffle(shuffle_idx)\n",
    "\n",
    "document_vectors = document_vectors[shuffle_idx]\n",
    "document_answers_wsum = document_answers_wsum[shuffle_idx]\n",
    "document_answers_idx = [document_answers_idx[idx] for idx in shuffle_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe65f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# onthot_ans: word freq matrix\n",
    "# weight_ans: TFIDF matrix\n",
    "\n",
    "onehot_ans = np.zeros((len(document_answers_idx), word_vectors.shape[0]))\n",
    "weight_ans = np.zeros((len(document_answers_idx), word_vectors.shape[0]))\n",
    "print(weight_ans.shape)\n",
    "\n",
    "for i in tqdm(range(len(document_answers_idx))):\n",
    "    for word_idx in document_answers_idx[i]:\n",
    "        weight_ans[i, word_idx] += vocab.word_weight[vocab.itos[word_idx]]\n",
    "        onehot_ans[i, word_idx] += 1\n",
    "        \n",
    "    if config[\"document_vector_weight_normalize\"]:\n",
    "        weight_ans[i] /= np.sum(weight_ans[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ab6376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "assert np.sum(document_vectors - np.dot(weight_ans, word_vectors) > 1e-10) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbca2e49",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52979e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = []\n",
    "select_columns = ['model']\n",
    "for topk in config[\"topk\"]:\n",
    "    select_columns.append('percision@{}'.format(topk))\n",
    "for topk in config[\"topk\"]:\n",
    "    select_columns.append('recall@{}'.format(topk))\n",
    "for topk in config[\"topk\"]:\n",
    "    select_columns.append('F1@{}'.format(topk))\n",
    "for topk in config[\"topk\"]:\n",
    "    select_columns.append('ndcg@{}'.format(topk))\n",
    "select_columns.append('ndcg@all')\n",
    "select_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87d5379",
   "metadata": {},
   "source": [
    "## setting training size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9ddbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size_ratio = 1\n",
    "train_size = int(len(document_answers_idx) * train_size_ratio)\n",
    "train_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d31a21",
   "metadata": {},
   "source": [
    "## Top K freq word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c664900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251dbcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ans = document_answers_idx[:train_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1efec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = [(word, freq) for word, freq in vocab.word_freq_in_corpus.items()]\n",
    "word_freq.sort(key=lambda x:x[1], reverse=True)\n",
    "word_freq[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04793e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_word_evaluation(k=50):\n",
    "    topk_word = [word for (word, freq) in word_freq[:k]]\n",
    "\n",
    "    pr, re = [], []\n",
    "    for ans in tqdm(test_ans):\n",
    "        ans = set(ans)\n",
    "        ans = [vocab.itos[a] for a in ans]\n",
    "\n",
    "        hit = []\n",
    "        for word in ans:\n",
    "            if word in topk_word:\n",
    "                hit.append(word)\n",
    "\n",
    "        precision = len(hit) / k\n",
    "        recall = len(hit) / len(ans)\n",
    "        pr.append(precision)\n",
    "        re.append(recall)\n",
    "\n",
    "    pr = np.mean(pr)\n",
    "    re = np.mean(re)\n",
    "    f1 = 2 * pr * re / (pr + re) if (pr + re) != 0 else 0\n",
    "    print('top {} word'.format(k))\n",
    "    print('percision', np.mean(pr))\n",
    "    print('recall', np.mean(re))\n",
    "    print('F1', f1)\n",
    "    return f1\n",
    "\n",
    "\n",
    "for topk in config['topk']:\n",
    "    topk_results[\"F1@{}\".format(topk)] = topk_word_evaluation(k=topk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26308789",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def topk_word_evaluation_NDCG(k=50):\n",
    "    freq_word =[word for (word, freq) in word_freq]\n",
    "    freq_word_idx = [vocab.stoi[word] for word in freq_word if word in vocab.stoi]\n",
    "    \n",
    "    scores = np.zeros(len(vocab.word_vectors))\n",
    "    for rank, idx in enumerate(freq_word_idx):\n",
    "        scores[idx] = len(vocab.word_vectors) - rank\n",
    "    \n",
    "    NDCGs = []\n",
    "    \n",
    "    for ans in tqdm(test_ans):\n",
    "        weight_ans = np.zeros(len(vocab.word_vectors))\n",
    "        \n",
    "        for word_idx in ans:\n",
    "            if word_idx == 0:\n",
    "                continue\n",
    "            word = vocab.itos[word_idx]\n",
    "            weight_ans[word_idx] += vocab.IDF[word]\n",
    "\n",
    "        NDCG_score = ndcg_score(weight_ans.reshape(1,-1), scores.reshape(1,-1), k=k)\n",
    "        NDCGs.append(NDCG_score)\n",
    "\n",
    "    print('top {} NDCG:{}'.format(k, np.mean(NDCGs)))\n",
    "    \n",
    "    return np.mean(NDCGs)\n",
    "\n",
    "\n",
    "# for topk in config['topk']:\n",
    "#     topk_results[\"ndcg@{}\".format(topk)] = topk_word_evaluation_NDCG(k=topk)\n",
    "    \n",
    "# topk_results[\"ndcg@all\"] = topk_word_evaluation_NDCG(k=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1cbb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_results[\"model\"] = \"topk\"\n",
    "final_results.append(pd.Series(topk_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731c7c3c",
   "metadata": {},
   "source": [
    "## Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a091701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091f8b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(document_vectors.shape)\n",
    "print(weight_ans.shape)\n",
    "print(word_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75933b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sklearn(pred, ans):\n",
    "    results = {}\n",
    "        \n",
    "    one_hot_ans = np.arange(ans.shape[0])[ans > 0]\n",
    "    \n",
    "    for topk in config[\"topk\"]:\n",
    "        one_hot_pred = np.argsort(pred)[-topk:]\n",
    "        hit = np.intersect1d(one_hot_pred, one_hot_ans)\n",
    "        percision = len(hit) / topk\n",
    "        recall = len(hit) / len(one_hot_ans)\n",
    "        f1 = 2 * percision * recall / (percision + recall) if (percision + recall) > 0 else 0\n",
    "        \n",
    "        results['percision@{}'.format(topk)] = percision\n",
    "        results['recall@{}'.format(topk)] = recall\n",
    "        results['F1@{}'.format(topk)] = f1\n",
    "        \n",
    "    ans = ans.reshape(1, -1)\n",
    "    pred = pred.reshape(1, -1)\n",
    "    for topk in config[\"topk\"]:\n",
    "        results['ndcg@{}'.format(topk)] = ndcg_score(ans, pred, k=topk)\n",
    "\n",
    "    results['ndcg@all'] = (ndcg_score(ans, pred, k=None))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbdc9ed",
   "metadata": {},
   "source": [
    "### linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e09dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for doc_id, doc_emb in enumerate(tqdm(document_vectors[:train_size])):\n",
    "    x = word_vectors.T\n",
    "    y = doc_emb\n",
    "    \n",
    "    ans = weight_ans[doc_id]\n",
    "    model = LinearRegression(fit_intercept=False).fit(x, y)\n",
    "    r2 = model.score(x, y)\n",
    "\n",
    "    res = evaluate_sklearn(model.coef_, ans)\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18befbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results).mean()\n",
    "results['model'] = 'sk-linear-regression'\n",
    "final_results.append(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafc846b",
   "metadata": {},
   "source": [
    "### lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42cf137",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "sk_lasso_epoch = 10000\n",
    "\n",
    "for doc_id, doc_emb in enumerate(tqdm(document_vectors[:train_size])):\n",
    "    x = word_vectors.T\n",
    "    y = doc_emb\n",
    "    \n",
    "    ans = weight_ans[doc_id]\n",
    "    model = Lasso(positive=True, fit_intercept=False, alpha=0.0001, max_iter=sk_lasso_epoch, tol=0).fit(x, y)\n",
    "    r2 = model.score(x, y)\n",
    "\n",
    "    res = evaluate_sklearn(model.coef_, ans)\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07772202",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results).mean()\n",
    "results['model'] = 'sk-lasso'\n",
    "final_results.append(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4fcb3a",
   "metadata": {},
   "source": [
    "## Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f53f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854f9586",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Lasso_Dataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 doc_vectors,\n",
    "                 doc_w_sum,\n",
    "                 weight_ans\n",
    "                 ):\n",
    "        self.doc_vectors = torch.FloatTensor(doc_vectors)\n",
    "        self.doc_w_sum = torch.FloatTensor(doc_w_sum)\n",
    "        self.weight_ans = weight_ans\n",
    "        assert len(doc_vectors) == len(doc_w_sum)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "                \n",
    "        return self.doc_vectors[idx], self.doc_w_sum[idx], idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doc_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa07e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR(nn.Module):\n",
    "    \"\"\"\n",
    "    Input shape: (N, 3, 64, 64)\n",
    "    Output shape: (N, )\n",
    "    \"\"\"\n",
    "    def __init__(self, num_doc, num_words):\n",
    "        super(LR, self).__init__()\n",
    "        weight = torch.zeros(num_doc, num_words).to(device)\n",
    "        self.emb = torch.nn.Embedding.from_pretrained(weight, freeze=False)\n",
    "        \n",
    "    def forward(self, doc_ids, word_vectors):\n",
    "        return self.emb(doc_ids) @ word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c135bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_Custom_Lasso(model, train_loader):\n",
    "    results = {}\n",
    "    model.eval()\n",
    "    \n",
    "    scores = np.array(model.emb.cpu().weight.data)\n",
    "    model.emb.to(device)\n",
    "    true_relevance = train_loader.dataset.weight_ans\n",
    "\n",
    "    # F1\n",
    "    F1s = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for i in range(true_relevance.shape[0]):\n",
    "        one_hot_ans = np.arange(true_relevance.shape[1])[true_relevance[i] > 0]\n",
    "        pred = scores[i]\n",
    "        \n",
    "        F1_ = []\n",
    "        percision_ = []\n",
    "        recall_ = []\n",
    "        for topk in config[\"topk\"]:\n",
    "            one_hot_pred = np.argsort(pred)[-topk:]\n",
    "            \n",
    "            hit = np.intersect1d(one_hot_pred, one_hot_ans)\n",
    "            percision = len(hit) / topk\n",
    "            recall = len(hit) / len(one_hot_ans)\n",
    "            \n",
    "            F1 = 2 * percision * recall / (percision + recall) if (percision + recall) > 0 else 0\n",
    "            F1_.append(F1)\n",
    "            percision_.append(percision)\n",
    "            recall_.append(recall)\n",
    "            \n",
    "        F1s.append(F1_)\n",
    "        precisions.append(percision_)\n",
    "        recalls.append(recall_)\n",
    "        \n",
    "    F1s = np.mean(F1s, axis=0)\n",
    "    precisions = np.mean(precisions, axis=0)\n",
    "    recalls = np.mean(recalls, axis=0)\n",
    "    \n",
    "    for i, topk in enumerate(config[\"topk\"]):\n",
    "        results['F1@{}'.format(topk)] = F1s[i]\n",
    "        results['percision@{}'.format(topk)] = precisions[i]\n",
    "        results['recall@{}'.format(topk)] = recalls[i]\n",
    "\n",
    "    # NDCG\n",
    "    for topk in config[\"topk\"]:\n",
    "        results['ndcg@{}'.format(topk)] = ndcg_score(true_relevance, scores, k=topk)\n",
    "    results['ndcg@all'] = ndcg_score(true_relevance, scores, k=None)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac91f68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "print('document num', train_size)\n",
    "\n",
    "train_dataset = Custom_Lasso_Dataset(document_vectors[:train_size], document_answers_wsum[:train_size], weight_ans[:train_size])\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e533d31",
   "metadata": {},
   "source": [
    "## start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3f76e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting\n",
    "lr = 0.1\n",
    "momentum = 0.999\n",
    "weight_decay = 0\n",
    "nesterov = False # True\n",
    "\n",
    "n_epoch = 50000\n",
    "\n",
    "w_sum_reg = 1e-2\n",
    "w_sum_reg_mul = 1\n",
    "w_clip_value = 0\n",
    "\n",
    "L1 = 1e-5\n",
    "\n",
    "verbose = True\n",
    "valid_epoch = 100\n",
    "\n",
    "model = LR(num_doc=train_size, num_words=word_vectors.shape[0]).to(device)\n",
    "model.train()\n",
    "\n",
    "word_vectors_tensor = torch.FloatTensor(word_vectors).to(device)\n",
    "    \n",
    "opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay, nesterov=nesterov)\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "results = []\n",
    "step = 0\n",
    "for epoch in tqdm(range(n_epoch)):    \n",
    "    loss_mse_his = []\n",
    "    loss_w_reg_his = []\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:\n",
    "        doc_embs, doc_w_sum, doc_ids = data\n",
    "        \n",
    "        doc_embs = doc_embs.to(device)\n",
    "        doc_w_sum = doc_w_sum.to(device)\n",
    "        doc_ids = doc_ids.to(device)\n",
    "        \n",
    "        w_reg = doc_w_sum * w_sum_reg_mul\n",
    "        # w_reg = (torch.ones(doc_embs.size(0), 1) * w_sum_reg_mul).to(device)\n",
    "        \n",
    "        # MSE loss\n",
    "        pred_doc_embs = model(doc_ids, word_vectors_tensor)     \n",
    "        loss_mse = criterion(pred_doc_embs, doc_embs)\n",
    "\n",
    "        pred_w_sum = torch.sum(model.emb(doc_ids), axis=1).view(-1, 1)\n",
    "        loss_w_reg = criterion(pred_w_sum, w_reg)\n",
    "        \n",
    "        loss_l1 = torch.sum(torch.abs(model.emb(doc_ids)))\n",
    "        loss = loss_mse + loss_w_reg * w_sum_reg + loss_l1 * L1\n",
    "        \n",
    "        # Model backwarding\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        loss_mse_his.append(loss_mse.item())\n",
    "        loss_w_reg_his.append(loss_w_reg.item())\n",
    "\n",
    "        for p in model.parameters():\n",
    "            p.data.clamp_(w_clip_value, float('inf'))\n",
    "\n",
    "        \n",
    "    if epoch % valid_epoch == 0:\n",
    "        res = {}\n",
    "        res['epoch'] = epoch\n",
    "        res['loss_mse'] = np.mean(loss_mse_his)\n",
    "        res['loss_w_reg'] = np.mean(loss_w_reg_his)\n",
    "        \n",
    "        res_ndcg = evaluate_Custom_Lasso(model, train_loader)\n",
    "        res.update(res_ndcg)\n",
    "        results.append(res)\n",
    "        \n",
    "        if verbose:\n",
    "            print()\n",
    "            for k, v in res.items():\n",
    "                print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49290d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "results_df = pd.DataFrame(results).set_index('epoch')\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60936db",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['model'] = 'our-lasso'\n",
    "final_results.append(results_df[select_columns].iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2115b07e",
   "metadata": {},
   "source": [
    "## Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5658c77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select doc_id and k\n",
    "doc_id = 40\n",
    "topk = 30\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31562cf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import colored\n",
    "from colored import stylize\n",
    "\n",
    "word_list = vocab.itos\n",
    "\n",
    "gt = [word_list[word_idx] for word_idx in np.argsort(weight_ans[doc_id])[::-1][:topk]]\n",
    "pred = [word_list[word_idx] for word_idx in np.argsort(model.emb.cpu().weight.data[doc_id].numpy())[::-1][:topk]]\n",
    "\n",
    "print('ground truth')\n",
    "for word in gt:\n",
    "    if word in pred:\n",
    "        print(stylize(word, colored.bg(\"yellow\")), end=' ')\n",
    "    else:\n",
    "        print(word, end=' ')\n",
    "\n",
    "print()\n",
    "print('\\nprediction')\n",
    "for word in pred:\n",
    "    if word in gt:\n",
    "        print(stylize(word, colored.bg(\"yellow\")), end=' ')\n",
    "    else:\n",
    "        print(word, end=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fa0add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw document\n",
    "print()\n",
    "ps = PorterStemmer()\n",
    "    \n",
    "for word in vocab.raw_documents[doc_id].split():\n",
    "    word_stem = ps.stem(word).lower()\n",
    "\n",
    "    if word_stem in gt:\n",
    "        if word_stem in pred:\n",
    "            print(stylize(word, colored.bg(\"yellow\")), end=' ')\n",
    "        else:\n",
    "            print(stylize(word, colored.bg(\"light_gray\")), end=' ')\n",
    "    else:\n",
    "        print(word, end=' ')\n",
    "# print(dataset.documents[doc_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc21f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "   \n",
    "scores = np.array(model.emb.weight.data)[doc_id].reshape(1, -1)\n",
    "true_relevance = train_loader.dataset.weight_ans[doc_id].reshape(1, -1)\n",
    "\n",
    "results['ndcg@50'] = (ndcg_score(true_relevance, scores, k=50))\n",
    "results['ndcg@100'] = (ndcg_score(true_relevance, scores, k=100))\n",
    "results['ndcg@200'] = (ndcg_score(true_relevance, scores, k=200))\n",
    "results['ndcg@all'] = (ndcg_score(true_relevance, scores, k=None))\n",
    "\n",
    "print('This document ndcg:')\n",
    "print('ground truth length:', np.sum(weight_ans[doc_id] > 0))\n",
    "print('NDCG top50', results['ndcg@50'])\n",
    "print('NDCG top100', results['ndcg@100'])\n",
    "print('NDCG top200', results['ndcg@200'])\n",
    "print('NDCG ALL', results['ndcg@all'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae49eac",
   "metadata": {},
   "source": [
    "## Final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48834bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_notebook = in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb5ecc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_results_df = pd.DataFrame(final_results).reset_index(drop=True)\n",
    "\n",
    "experiment_dir = './records/dataset-{}-n_document-{}-wdist-{}-filtertopk-{}'.format(\n",
    "                                        config['dataset'],\n",
    "                                        config['n_document'],\n",
    "                                        config[\"document_vector_agg_weight\"],\n",
    "                                        config[\"topk_word_freq_threshold\"])\n",
    "\n",
    "print('Saving to directory', experiment_dir)\n",
    "os.makedirs(experiment_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b704cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results_df.to_csv(os.path.join(experiment_dir, 'result.csv'), index=False)\n",
    "\n",
    "import json\n",
    "with open(os.path.join(experiment_dir, 'config.json'), 'w') as json_file:\n",
    "    json.dump(config, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e6cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat in final_results_df.set_index('model').columns:\n",
    "    plt.bar(final_results_df['model'],\n",
    "            final_results_df[feat], \n",
    "            width=0.5, \n",
    "            bottom=None, \n",
    "            align='center', \n",
    "            color=['lightsteelblue', \n",
    "                   'cornflowerblue', \n",
    "                   'royalblue', \n",
    "                   'navy'])\n",
    "    plt.title(feat)\n",
    "    plt.savefig(os.path.join(experiment_dir, '{}.png'.format(feat)))\n",
    "    plt.clf()\n",
    "    if is_notebook:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c679b721",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_results_df)\n",
    "final_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04a4563",
   "metadata": {},
   "source": [
    "## MLP Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3b320c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f60a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPDecoderDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 doc_vectors,\n",
    "                 weight_ans,\n",
    "                 topk=50):\n",
    "        self.doc_vectors = torch.FloatTensor(doc_vectors)\n",
    "        self.weight_ans = torch.FloatTensor(weight_ans)\n",
    "        self.weight_ans_s = torch.argsort(self.weight_ans, dim=1, descending=True)\n",
    "        self.weight_ans_s[:, topk:] = -1\n",
    "        \n",
    "        assert len(doc_vectors) == len(weight_ans)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.doc_vectors[idx], self.weight_ans[idx], self.weight_ans_s[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doc_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5d0eaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_size_ratio = 0.9\n",
    "train_size = int(len(document_answers_idx) * train_size_ratio)\n",
    "train_size\n",
    "\n",
    "print('train size', train_size)\n",
    "print('valid size', len(document_vectors) - train_size)\n",
    "\n",
    "train_dataset = MLPDecoderDataset(document_vectors[:train_size], weight_ans[:train_size], topk=50)\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "valid_dataset = MLPDecoderDataset(document_vectors[train_size:], weight_ans[train_size:], topk=50)\n",
    "valid_loader  = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e0e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPDecoder(nn.Module):\n",
    "    def __init__(self, doc_emb_dim, num_words, h_dim=300):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(doc_emb_dim, h_dim) \n",
    "#         self.fc2 = nn.Linear(h_dim, h_dim)\n",
    "#         self.fc3 = nn.Linear(h_dim, h_dim)\n",
    "        self.fc4 = nn.Linear(h_dim, num_words)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.fc1(x))\n",
    "#         x = F.tanh(self.fc2(x))\n",
    "#         x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c7d1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import timeit\n",
    "from torchmetrics import F1\n",
    "from torchmetrics.functional import retrieval_normalized_dcg\n",
    "\n",
    "def evaluate_MLPDecoder(model, data_loader):\n",
    "    results = {}\n",
    "    model.eval()\n",
    "    \n",
    "    pred_all = []\n",
    "    target_all = []\n",
    "    \n",
    "    # predict all data\n",
    "    start = timeit.default_timer()\n",
    "    for data in data_loader:\n",
    "        doc_embs, target, _ = data\n",
    "        \n",
    "        doc_embs = doc_embs.to(device)\n",
    "        target = target.to(device)\n",
    "                \n",
    "        pred = model(doc_embs)\n",
    "        pred_all.append(pred)\n",
    "        target_all.append(target)\n",
    "        \n",
    "    pred_all = torch.cat(pred_all, dim=0)\n",
    "    target_all = torch.cat(target_all, dim=0)\n",
    "    stop1 = timeit.default_timer()\n",
    "    print('Time1: ', stop1 - start)  \n",
    "    \n",
    "    # F1\n",
    "    for topk in config[\"topk\"]:\n",
    "        f1 = F1(top_k=topk)\n",
    "        f1_score = f1(pred_all.cpu(), torch.sign(target_all).int().cpu())\n",
    "        results['F1@{}'.format(topk)] = f1_score.item()\n",
    "    stop2 = timeit.default_timer()\n",
    "    print('Time2: ', stop2 - stop1)  \n",
    "    \n",
    "    # NDCG\n",
    "    for topk in config[\"topk\"]:\n",
    "        ndcg_scores = []\n",
    "        for i in range(pred_all.shape[0]):\n",
    "            ndcg_scores.append(retrieval_normalized_dcg(pred_all[i], target_all[i], k=topk).item())     \n",
    "        results['ndcg@{}'.format(topk)] = np.mean(ndcg_scores)\n",
    "        \n",
    "    ndcg_scores = []\n",
    "    for i in range(pred_all.shape[0]):\n",
    "        ndcg_scores.append(retrieval_normalized_dcg(pred_all[i], target_all[i]).item())     \n",
    "    results['ndcg@all'] = np.mean(ndcg_scores)\n",
    "    stop3 = timeit.default_timer()\n",
    "    print('Time3: ', stop3 - stop2)      \n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce51a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_EPS = 1e-10\n",
    "PADDED_Y_VALUE = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e16e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting\n",
    "lr = 0.05\n",
    "momentum = 0.\n",
    "weight_decay = 0\n",
    "nesterov = False # True\n",
    "\n",
    "n_epoch = 5000\n",
    "verbose = True\n",
    "valid_epoch = 50\n",
    "\n",
    "h_dim = 3000\n",
    "\n",
    "model = MLPDecoder(doc_emb_dim=document_vectors.shape[1], num_words=len(word_vectors), h_dim=h_dim).to(device)\n",
    "model.train()\n",
    "\n",
    "word_vectors_tensor = torch.FloatTensor(word_vectors).to(device)\n",
    "\n",
    "opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay, nesterov=nesterov)\n",
    "# opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# criterion = nn.MSELoss(reduction='mean')\n",
    "criterion = nn.MultiLabelMarginLoss(reduction='mean')\n",
    "# criterion = nn.MultiLabelSoftMarginLoss(reduction='mean')\n",
    "\n",
    "results = []\n",
    "step = 0\n",
    "clip_value = 1\n",
    "\n",
    "for epoch in tqdm(range(n_epoch)):    \n",
    "    train_loss_his = []\n",
    "    valid_loss_his = []\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:\n",
    "        doc_embs, target, target_rank = data\n",
    "        \n",
    "        doc_embs = doc_embs.to(device)\n",
    "        target = target.to(device)\n",
    "        target_rank = target_rank.to(device)\n",
    "        \n",
    "        # MSE loss\n",
    "        pred = model(doc_embs)     \n",
    "#         loss = criterion(pred, target)\n",
    "        loss = criterion(pred, target_rank)\n",
    "#         loss = criterion(pred, torch.sign(target))\n",
    "        \n",
    "        # Model backwarding\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(model.parameters(), clip_value)\n",
    "        \n",
    "        opt.step()\n",
    "\n",
    "        train_loss_his.append(loss.item())\n",
    "        \n",
    "    model.eval()\n",
    "    for data in valid_loader:\n",
    "        doc_embs, target, target_rank = data\n",
    "        \n",
    "        doc_embs = doc_embs.to(device)\n",
    "        target = target.to(device)\n",
    "        target_rank = target_rank.to(device)\n",
    "        \n",
    "        # MSE loss\n",
    "        pred = model(doc_embs)     \n",
    "        \n",
    "#         loss = criterion(pred, target)\n",
    "        loss = criterion(pred, target_rank)\n",
    "#         loss = criterion(pred, torch.sign(target))\n",
    "        \n",
    "        valid_loss_his.append(loss.item())\n",
    "    \n",
    "    print(\"Epoch\", epoch, np.mean(train_loss_his), np.mean(valid_loss_his))\n",
    "    \n",
    "    if epoch % valid_epoch == 0:\n",
    "        res = {}\n",
    "        res['epoch'] = epoch\n",
    "        \n",
    "        train_res_ndcg = evaluate_MLPDecoder(model, train_loader)\n",
    "        valid_res_ndcg = evaluate_MLPDecoder(model, valid_loader)\n",
    "        \n",
    "        res.update(valid_res_ndcg)\n",
    "        results.append(res)\n",
    "        \n",
    "        if verbose:\n",
    "            print()\n",
    "            print('train', train_res_ndcg)\n",
    "            print('valid', valid_res_ndcg)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba700d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def listNet(y_pred, y_true, eps=DEFAULT_EPS, padded_value_indicator=PADDED_Y_VALUE):\n",
    "    \"\"\"\n",
    "    ListNet loss introduced in \"Learning to Rank: From Pairwise Approach to Listwise Approach\".\n",
    "    :param y_pred: predictions from the model, shape [batch_size, slate_length]\n",
    "    :param y_true: ground truth labels, shape [batch_size, slate_length]\n",
    "    :param eps: epsilon value, used for numerical stability\n",
    "    :param padded_value_indicator: an indicator of the y_true index containing a padded item, e.g. -1\n",
    "    :return: loss value, a torch.Tensor\n",
    "    \"\"\"\n",
    "    y_pred = y_pred.clone()\n",
    "    y_true = y_true.clone()\n",
    "\n",
    "    mask = y_true == padded_value_indicator\n",
    "    y_pred[mask] = float('-inf')\n",
    "    y_true[mask] = float('-inf')\n",
    "\n",
    "    preds_smax = F.softmax(y_pred, dim=1)\n",
    "    true_smax = F.softmax(y_true, dim=1)\n",
    "\n",
    "    preds_smax = preds_smax + eps\n",
    "    preds_log = torch.log(preds_smax)\n",
    "\n",
    "    return torch.mean(-torch.sum(true_smax * preds_log, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7241c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting\n",
    "lr = 0.05\n",
    "momentum = 0.\n",
    "weight_decay = 0\n",
    "nesterov = False # True\n",
    "\n",
    "n_epoch = 5000\n",
    "verbose = True\n",
    "valid_epoch = 50\n",
    "\n",
    "h_dim = 3000\n",
    "\n",
    "model = MLPDecoder(doc_emb_dim=document_vectors.shape[1], num_words=len(word_vectors), h_dim=h_dim).to(device)\n",
    "model.train()\n",
    "\n",
    "word_vectors_tensor = torch.FloatTensor(word_vectors).to(device)\n",
    "\n",
    "opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay, nesterov=nesterov)\n",
    "# opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# criterion = nn.MSELoss(reduction='mean')\n",
    "criterion = nn.MultiLabelMarginLoss(reduction='mean')\n",
    "# criterion = nn.MultiLabelSoftMarginLoss(reduction='mean')\n",
    "\n",
    "results = []\n",
    "step = 0\n",
    "clip_value = 1\n",
    "\n",
    "for epoch in tqdm(range(n_epoch)):    \n",
    "    train_loss_his = []\n",
    "    valid_loss_his = []\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:\n",
    "        doc_embs, target, target_rank = data\n",
    "        \n",
    "        doc_embs = doc_embs.to(device)\n",
    "        target = target.to(device)\n",
    "        target_rank = target_rank.to(device)\n",
    "        \n",
    "        # MSE loss\n",
    "        pred = model(doc_embs)     \n",
    "#         loss = criterion(pred, target)\n",
    "#         loss = criterion(pred, target_rank)\n",
    "#         loss = criterion(pred, torch.sign(target))\n",
    "        loss = listNet(pred, target)\n",
    "    \n",
    "        # Model backwarding\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_value_(model.parameters(), clip_value)\n",
    "        \n",
    "        opt.step()\n",
    "\n",
    "        train_loss_his.append(loss.item())\n",
    "        \n",
    "    model.eval()\n",
    "    for data in valid_loader:\n",
    "        doc_embs, target, target_rank = data\n",
    "        \n",
    "        doc_embs = doc_embs.to(device)\n",
    "        target = target.to(device)\n",
    "        target_rank = target_rank.to(device)\n",
    "        \n",
    "        # MSE loss\n",
    "        pred = model(doc_embs)     \n",
    "        \n",
    "#         loss = criterion(pred, target)\n",
    "#         loss = criterion(pred, target_rank)\n",
    "#         loss = criterion(pred, torch.sign(target))\n",
    "        loss = listNet(pred, target)\n",
    "    \n",
    "        valid_loss_his.append(loss.item())\n",
    "    \n",
    "    print(\"Epoch\", epoch, np.mean(train_loss_his), np.mean(valid_loss_his))\n",
    "    \n",
    "    if epoch % valid_epoch == 0:\n",
    "        res = {}\n",
    "        res['epoch'] = epoch\n",
    "        \n",
    "        train_res_ndcg = evaluate_MLPDecoder(model, train_loader)\n",
    "        valid_res_ndcg = evaluate_MLPDecoder(model, valid_loader)\n",
    "        \n",
    "        res.update(valid_res_ndcg)\n",
    "        results.append(res)\n",
    "        \n",
    "        if verbose:\n",
    "            print()\n",
    "            print('train', train_res_ndcg)\n",
    "            print('valid', valid_res_ndcg)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadfda8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7fbb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deterministic_neural_sort(s, tau, mask):\n",
    "    \"\"\"\n",
    "    Deterministic neural sort.\n",
    "    Code taken from \"Stochastic Optimization of Sorting Networks via Continuous Relaxations\", ICLR 2019.\n",
    "    Minor modifications applied to the original code (masking).\n",
    "    :param s: values to sort, shape [batch_size, slate_length]\n",
    "    :param tau: temperature for the final softmax function\n",
    "    :param mask: mask indicating padded elements\n",
    "    :return: approximate permutation matrices of shape [batch_size, slate_length, slate_length]\n",
    "    \"\"\"\n",
    "    dev = get_torch_device()\n",
    "\n",
    "    n = s.size()[1]\n",
    "    one = torch.ones((n, 1), dtype=torch.float32, device=dev)\n",
    "    s = s.masked_fill(mask[:, :, None], -1e8)\n",
    "    A_s = torch.abs(s - s.permute(0, 2, 1))\n",
    "    A_s = A_s.masked_fill(mask[:, :, None] | mask[:, None, :], 0.0)\n",
    "\n",
    "    B = torch.matmul(A_s, torch.matmul(one, torch.transpose(one, 0, 1)))\n",
    "\n",
    "    temp = [n - m + 1 - 2 * (torch.arange(n - m, device=dev) + 1) for m in mask.squeeze(-1).sum(dim=1)]\n",
    "    temp = [t.type(torch.float32) for t in temp]\n",
    "    temp = [torch.cat((t, torch.zeros(n - len(t), device=dev))) for t in temp]\n",
    "    scaling = torch.stack(temp).type(torch.float32).to(dev)  # type: ignore\n",
    "\n",
    "    s = s.masked_fill(mask[:, :, None], 0.0)\n",
    "    C = torch.matmul(s, scaling.unsqueeze(-2))\n",
    "\n",
    "    P_max = (C - B).permute(0, 2, 1)\n",
    "    P_max = P_max.masked_fill(mask[:, :, None] | mask[:, None, :], -np.inf)\n",
    "    P_max = P_max.masked_fill(mask[:, :, None] & mask[:, None, :], 1.0)\n",
    "    sm = torch.nn.Softmax(-1)\n",
    "    P_hat = sm(P_max / tau)\n",
    "    return P_hat\n",
    "\n",
    "def sinkhorn_scaling(mat, mask=None, tol=1e-6, max_iter=50):\n",
    "    \"\"\"\n",
    "    Sinkhorn scaling procedure.\n",
    "    :param mat: a tensor of square matrices of shape N x M x M, where N is batch size\n",
    "    :param mask: a tensor of masks of shape N x M\n",
    "    :param tol: Sinkhorn scaling tolerance\n",
    "    :param max_iter: maximum number of iterations of the Sinkhorn scaling\n",
    "    :return: a tensor of (approximately) doubly stochastic matrices\n",
    "    \"\"\"\n",
    "    if mask is not None:\n",
    "        mat = mat.masked_fill(mask[:, None, :] | mask[:, :, None], 0.0)\n",
    "        mat = mat.masked_fill(mask[:, None, :] & mask[:, :, None], 1.0)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        mat = mat / mat.sum(dim=1, keepdim=True).clamp(min=DEFAULT_EPS)\n",
    "        mat = mat / mat.sum(dim=2, keepdim=True).clamp(min=DEFAULT_EPS)\n",
    "\n",
    "        if torch.max(torch.abs(mat.sum(dim=2) - 1.)) < tol and torch.max(torch.abs(mat.sum(dim=1) - 1.)) < tol:\n",
    "            break\n",
    "\n",
    "    if mask is not None:\n",
    "        mat = mat.masked_fill(mask[:, None, :] | mask[:, :, None], 0.0)\n",
    "\n",
    "    return mat\n",
    "\n",
    "def stochastic_neural_sort(s, n_samples, tau, mask, beta=1.0, log_scores=True, eps=1e-10):\n",
    "    \"\"\"\n",
    "    Stochastic neural sort. Please note that memory complexity grows by factor n_samples.\n",
    "    Code taken from \"Stochastic Optimization of Sorting Networks via Continuous Relaxations\", ICLR 2019.\n",
    "    Minor modifications applied to the original code (masking).\n",
    "    :param s: values to sort, shape [batch_size, slate_length]\n",
    "    :param n_samples: number of samples (approximations) for each permutation matrix\n",
    "    :param tau: temperature for the final softmax function\n",
    "    :param mask: mask indicating padded elements\n",
    "    :param beta: scale parameter for the Gumbel distribution\n",
    "    :param log_scores: whether to apply the logarithm function to scores prior to Gumbel perturbation\n",
    "    :param eps: epsilon for the logarithm function\n",
    "    :return: approximate permutation matrices of shape [n_samples, batch_size, slate_length, slate_length]\n",
    "    \"\"\"\n",
    "    dev = get_torch_device()\n",
    "\n",
    "    batch_size = s.size()[0]\n",
    "    n = s.size()[1]\n",
    "    s_positive = s + torch.abs(s.min())\n",
    "    samples = beta * sample_gumbel([n_samples, batch_size, n, 1], device=dev)\n",
    "    if log_scores:\n",
    "        s_positive = torch.log(s_positive + eps)\n",
    "\n",
    "    s_perturb = (s_positive + samples).view(n_samples * batch_size, n, 1)\n",
    "    mask_repeated = mask.repeat_interleave(n_samples, dim=0)\n",
    "\n",
    "    P_hat = deterministic_neural_sort(s_perturb, tau, mask_repeated)\n",
    "    P_hat = P_hat.view(n_samples, batch_size, n, n)\n",
    "    return P_hat\n",
    "\n",
    "def dcg(y_pred, y_true, ats=None, gain_function=lambda x: torch.pow(2, x) - 1, padding_indicator=PADDED_Y_VALUE):\n",
    "    \"\"\"\n",
    "    Discounted Cumulative Gain at k.\n",
    "    Compute DCG at ranks given by ats or at the maximum rank if ats is None.\n",
    "    :param y_pred: predictions from the model, shape [batch_size, slate_length]\n",
    "    :param y_true: ground truth labels, shape [batch_size, slate_length]\n",
    "    :param ats: optional list of ranks for DCG evaluation, if None, maximum rank is used\n",
    "    :param gain_function: callable, gain function for the ground truth labels, e.g. torch.pow(2, x) - 1\n",
    "    :param padding_indicator: an indicator of the y_true index containing a padded item, e.g. -1\n",
    "    :return: DCG values for each slate and evaluation position, shape [batch_size, len(ats)]\n",
    "    \"\"\"\n",
    "    y_true = y_true.clone()\n",
    "    y_pred = y_pred.clone()\n",
    "\n",
    "    actual_length = y_true.shape[1]\n",
    "\n",
    "    if ats is None:\n",
    "        ats = [actual_length]\n",
    "    ats = [min(at, actual_length) for at in ats]\n",
    "\n",
    "    true_sorted_by_preds = __apply_mask_and_get_true_sorted_by_preds(y_pred, y_true, padding_indicator)\n",
    "\n",
    "    discounts = (torch.tensor(1) / torch.log2(torch.arange(true_sorted_by_preds.shape[1], dtype=torch.float) + 2.0)).to(\n",
    "        device=true_sorted_by_preds.device)\n",
    "\n",
    "    gains = gain_function(true_sorted_by_preds)\n",
    "\n",
    "    discounted_gains = (gains * discounts)[:, :np.max(ats)]\n",
    "\n",
    "    cum_dcg = torch.cumsum(discounted_gains, dim=1)\n",
    "\n",
    "    ats_tensor = torch.tensor(ats, dtype=torch.long) - torch.tensor(1)\n",
    "\n",
    "    dcg = cum_dcg[:, ats_tensor]\n",
    "\n",
    "    return dcg\n",
    "\n",
    "def get_torch_device():\n",
    "    \"\"\"\n",
    "    Getter for an available pyTorch device.\n",
    "    :return: CUDA-capable GPU if available, CPU otherwise\n",
    "    \"\"\"\n",
    "    return \"cpu\"\n",
    "    return torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def neuralNDCG(y_pred, y_true, padded_value_indicator=PADDED_Y_VALUE, temperature=1., powered_relevancies=True, k=None,\n",
    "               stochastic=False, n_samples=32, beta=0.1, log_scores=True):\n",
    "    \"\"\"\n",
    "    NeuralNDCG loss introduced in \"NeuralNDCG: Direct Optimisation of a Ranking Metric via Differentiable\n",
    "    Relaxation of Sorting\" - https://arxiv.org/abs/2102.07831. Based on the NeuralSort algorithm.\n",
    "    :param y_pred: predictions from the model, shape [batch_size, slate_length]\n",
    "    :param y_true: ground truth labels, shape [batch_size, slate_length]\n",
    "    :param padded_value_indicator: an indicator of the y_true index containing a padded item, e.g. -1\n",
    "    :param temperature: temperature for the NeuralSort algorithm\n",
    "    :param powered_relevancies: whether to apply 2^x - 1 gain function, x otherwise\n",
    "    :param k: rank at which the loss is truncated\n",
    "    :param stochastic: whether to calculate the stochastic variant\n",
    "    :param n_samples: how many stochastic samples are taken, used if stochastic == True\n",
    "    :param beta: beta parameter for NeuralSort algorithm, used if stochastic == True\n",
    "    :param log_scores: log_scores parameter for NeuralSort algorithm, used if stochastic == True\n",
    "    :return: loss value, a torch.Tensor\n",
    "    \"\"\"\n",
    "    dev = get_torch_device()\n",
    "\n",
    "    if k is None:\n",
    "        k = y_true.shape[1]\n",
    "\n",
    "    mask = (y_true == padded_value_indicator)\n",
    "    # Choose the deterministic/stochastic variant\n",
    "    if stochastic:\n",
    "        P_hat = stochastic_neural_sort(y_pred.unsqueeze(-1), n_samples=n_samples, tau=temperature, mask=mask,\n",
    "                                       beta=beta, log_scores=log_scores)\n",
    "    else:\n",
    "        P_hat = deterministic_neural_sort(y_pred.unsqueeze(-1), tau=temperature, mask=mask).unsqueeze(0)\n",
    "\n",
    "    # Perform sinkhorn scaling to obtain doubly stochastic permutation matrices\n",
    "    P_hat = sinkhorn_scaling(P_hat.view(P_hat.shape[0] * P_hat.shape[1], P_hat.shape[2], P_hat.shape[3]),\n",
    "                             mask.repeat_interleave(P_hat.shape[0], dim=0), tol=1e-6, max_iter=50)\n",
    "    P_hat = P_hat.view(int(P_hat.shape[0] / y_pred.shape[0]), y_pred.shape[0], P_hat.shape[1], P_hat.shape[2])\n",
    "\n",
    "    # Mask P_hat and apply to true labels, ie approximately sort them\n",
    "    P_hat = P_hat.masked_fill(mask[None, :, :, None] | mask[None, :, None, :], 0.)\n",
    "    y_true_masked = y_true.masked_fill(mask, 0.).unsqueeze(-1).unsqueeze(0)\n",
    "    if powered_relevancies:\n",
    "        y_true_masked = torch.pow(2., y_true_masked) - 1.\n",
    "\n",
    "    ground_truth = torch.matmul(P_hat, y_true_masked).squeeze(-1)\n",
    "    discounts = (torch.tensor(1.) / torch.log2(torch.arange(y_true.shape[-1], dtype=torch.float) + 2.)).to(dev)\n",
    "    discounted_gains = ground_truth * discounts\n",
    "\n",
    "    if powered_relevancies:\n",
    "        idcg = dcg(y_true, y_true, ats=[k]).permute(1, 0)\n",
    "    else:\n",
    "        idcg = dcg(y_true, y_true, ats=[k], gain_function=lambda x: x).permute(1, 0)\n",
    "\n",
    "    discounted_gains = discounted_gains[:, :, :k]\n",
    "    ndcg = discounted_gains.sum(dim=-1) / (idcg + DEFAULT_EPS)\n",
    "    idcg_mask = idcg == 0.\n",
    "    ndcg = ndcg.masked_fill(idcg_mask.repeat(ndcg.shape[0], 1), 0.)\n",
    "\n",
    "    assert (ndcg < 0.).sum() >= 0, \"every ndcg should be non-negative\"\n",
    "    if idcg_mask.all():\n",
    "        return torch.tensor(0.)\n",
    "\n",
    "    mean_ndcg = ndcg.sum() / ((~idcg_mask).sum() * ndcg.shape[0])  # type: ignore\n",
    "    return -1. * mean_ndcg  # -1 cause we want to maximize NDCG\n",
    "\n",
    "\n",
    "def neuralNDCG_transposed(y_pred, y_true, padded_value_indicator=PADDED_Y_VALUE, temperature=1.,\n",
    "                          powered_relevancies=True, k=None, stochastic=False, n_samples=32, beta=0.1, log_scores=True,\n",
    "                          max_iter=50, tol=1e-6):\n",
    "    \"\"\"\n",
    "    NeuralNDCG Transposed loss introduced in \"NeuralNDCG: Direct Optimisation of a Ranking Metric via Differentiable\n",
    "    Relaxation of Sorting\" - https://arxiv.org/abs/2102.07831. Based on the NeuralSort algorithm.\n",
    "    :param y_pred: predictions from the model, shape [batch_size, slate_length]\n",
    "    :param y_true: ground truth labels, shape [batch_size, slate_length]\n",
    "    :param padded_value_indicator: an indicator of the y_true index containing a padded item, e.g. -1\n",
    "    :param temperature: temperature for the NeuralSort algorithm\n",
    "    :param powered_relevancies: whether to apply 2^x - 1 gain function, x otherwise\n",
    "    :param k: rank at which the loss is truncated\n",
    "    :param stochastic: whether to calculate the stochastic variant\n",
    "    :param n_samples: how many stochastic samples are taken, used if stochastic == True\n",
    "    :param beta: beta parameter for NeuralSort algorithm, used if stochastic == True\n",
    "    :param log_scores: log_scores parameter for NeuralSort algorithm, used if stochastic == True\n",
    "    :param max_iter: maximum iteration count for Sinkhorn scaling\n",
    "    :param tol: tolerance for Sinkhorn scaling\n",
    "    :return: loss value, a torch.Tensor\n",
    "    \"\"\"\n",
    "    dev = get_torch_device()\n",
    "\n",
    "    if k is None:\n",
    "        k = y_true.shape[1]\n",
    "\n",
    "    mask = (y_true == padded_value_indicator)\n",
    "\n",
    "    if stochastic:\n",
    "        P_hat = stochastic_neural_sort(y_pred.unsqueeze(-1), n_samples=n_samples, tau=temperature, mask=mask,\n",
    "                                       beta=beta, log_scores=log_scores)\n",
    "    else:\n",
    "        P_hat = deterministic_neural_sort(y_pred.unsqueeze(-1), tau=temperature, mask=mask).unsqueeze(0)\n",
    "\n",
    "    # Perform sinkhorn scaling to obtain doubly stochastic permutation matrices\n",
    "    P_hat_masked = sinkhorn_scaling(P_hat.view(P_hat.shape[0] * y_pred.shape[0], y_pred.shape[1], y_pred.shape[1]),\n",
    "                                    mask.repeat_interleave(P_hat.shape[0], dim=0), tol=tol, max_iter=max_iter)\n",
    "    P_hat_masked = P_hat_masked.view(P_hat.shape[0], y_pred.shape[0], y_pred.shape[1], y_pred.shape[1])\n",
    "    discounts = (torch.tensor(1) / torch.log2(torch.arange(y_true.shape[-1], dtype=torch.float) + 2.)).to(dev)\n",
    "\n",
    "    # This takes care of the @k metric truncation - if something is @>k, it is useless and gets 0.0 discount\n",
    "    discounts[k:] = 0.\n",
    "    discounts = discounts[None, None, :, None]\n",
    "\n",
    "    # Here the discounts become expected discounts\n",
    "    discounts = torch.matmul(P_hat_masked.permute(0, 1, 3, 2), discounts).squeeze(-1)\n",
    "    if powered_relevancies:\n",
    "        gains = torch.pow(2., y_true) - 1\n",
    "        discounted_gains = gains.unsqueeze(0) * discounts\n",
    "        idcg = dcg(y_true, y_true, ats=[k]).squeeze()\n",
    "    else:\n",
    "        gains = y_true\n",
    "        discounted_gains = gains.unsqueeze(0) * discounts\n",
    "        idcg = dcg(y_true, y_true, ats=[k]).squeeze()\n",
    "\n",
    "    ndcg = discounted_gains.sum(dim=2) / (idcg + DEFAULT_EPS)\n",
    "    idcg_mask = idcg == 0.\n",
    "    ndcg = ndcg.masked_fill(idcg_mask, 0.)\n",
    "\n",
    "    assert (ndcg < 0.).sum() >= 0, \"every ndcg should be non-negative\"\n",
    "    if idcg_mask.all():\n",
    "        return torch.tensor(0.)\n",
    "\n",
    "    mean_ndcg = ndcg.sum() / ((~idcg_mask).sum() * ndcg.shape[0])  # type: ignore\n",
    "    return -1. * mean_ndcg  # -1 cause we want to maximize NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5270521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lambdaLoss(y_pred, y_true, eps=DEFAULT_EPS, padded_value_indicator=PADDED_Y_VALUE, weighing_scheme=None, k=None, sigma=1., mu=10.,\n",
    "#                reduction=\"sum\", reduction_log=\"binary\"):\n",
    "#     \"\"\"\n",
    "#     LambdaLoss framework for LTR losses implementations, introduced in \"The LambdaLoss Framework for Ranking Metric Optimization\".\n",
    "#     Contains implementations of different weighing schemes corresponding to e.g. LambdaRank or RankNet.\n",
    "#     :param y_pred: predictions from the model, shape [batch_size, slate_length]\n",
    "#     :param y_true: ground truth labels, shape [batch_size, slate_length]\n",
    "#     :param eps: epsilon value, used for numerical stability\n",
    "#     :param padded_value_indicator: an indicator of the y_true index containing a padded item, e.g. -1\n",
    "#     :param weighing_scheme: a string corresponding to a name of one of the weighing schemes\n",
    "#     :param k: rank at which the loss is truncated\n",
    "#     :param sigma: score difference weight used in the sigmoid function\n",
    "#     :param mu: optional weight used in NDCGLoss2++ weighing scheme\n",
    "#     :param reduction: losses reduction method, could be either a sum or a mean\n",
    "#     :param reduction_log: logarithm variant used prior to masking and loss reduction, either binary or natural\n",
    "#     :return: loss value, a torch.Tensor\n",
    "#     \"\"\"\n",
    "#     device = y_pred.device\n",
    "#     y_pred = y_pred.clone()\n",
    "#     y_true = y_true.clone()\n",
    "\n",
    "#     padded_mask = y_true == padded_value_indicator\n",
    "#     y_pred[padded_mask] = float(\"-inf\")\n",
    "#     y_true[padded_mask] = float(\"-inf\")\n",
    "\n",
    "#     # Here we sort the true and predicted relevancy scores.\n",
    "#     y_pred_sorted, indices_pred = y_pred.sort(descending=True, dim=-1)\n",
    "#     y_true_sorted, _ = y_true.sort(descending=True, dim=-1)\n",
    "\n",
    "#     # After sorting, we can mask out the pairs of indices (i, j) containing index of a padded element.\n",
    "#     true_sorted_by_preds = torch.gather(y_true, dim=1, index=indices_pred)\n",
    "#     true_diffs = true_sorted_by_preds[:, :, None] - true_sorted_by_preds[:, None, :]\n",
    "#     print(true_sorted_by_preds.shape)\n",
    "#     print(true_diffs.shape)\n",
    "#     padded_pairs_mask = torch.isfinite(true_diffs)\n",
    "\n",
    "#     if weighing_scheme != \"ndcgLoss1_scheme\":\n",
    "#         padded_pairs_mask = padded_pairs_mask & (true_diffs > 0)\n",
    "\n",
    "#     ndcg_at_k_mask = torch.zeros((y_pred.shape[1], y_pred.shape[1]), dtype=torch.bool, device=device)\n",
    "#     ndcg_at_k_mask[:k, :k] = 1\n",
    "\n",
    "#     # Here we clamp the -infs to get correct gains and ideal DCGs (maxDCGs)\n",
    "#     true_sorted_by_preds.clamp_(min=0.)\n",
    "#     y_true_sorted.clamp_(min=0.)\n",
    "\n",
    "#     # Here we find the gains, discounts and ideal DCGs per slate.\n",
    "#     pos_idxs = torch.arange(1, y_pred.shape[1] + 1).to(device)\n",
    "#     D = torch.log2(1. + pos_idxs.float())[None, :]\n",
    "#     maxDCGs = torch.sum(((torch.pow(2, y_true_sorted) - 1) / D)[:, :k], dim=-1).clamp(min=eps)\n",
    "#     G = (torch.pow(2, true_sorted_by_preds) - 1) / maxDCGs[:, None]\n",
    "\n",
    "#     # Here we apply appropriate weighing scheme - ndcgLoss1, ndcgLoss2, ndcgLoss2++ or no weights (=1.0)\n",
    "#     if weighing_scheme is None:\n",
    "#         weights = 1.\n",
    "#     else:\n",
    "#         weights = globals()[weighing_scheme](G, D, mu, true_sorted_by_preds)  # type: ignore\n",
    "\n",
    "#     # We are clamping the array entries to maintain correct backprop (log(0) and division by 0)\n",
    "#     scores_diffs = (y_pred_sorted[:, :, None] - y_pred_sorted[:, None, :]).clamp(min=-1e8, max=1e8)\n",
    "#     scores_diffs.masked_fill(torch.isnan(scores_diffs), 0.)\n",
    "#     weighted_probas = (torch.sigmoid(sigma * scores_diffs).clamp(min=eps) ** weights).clamp(min=eps)\n",
    "#     if reduction_log == \"natural\":\n",
    "#         losses = torch.log(weighted_probas)\n",
    "#     elif reduction_log == \"binary\":\n",
    "#         losses = torch.log2(weighted_probas)\n",
    "#     else:\n",
    "#         raise ValueError(\"Reduction logarithm base can be either natural or binary\")\n",
    "\n",
    "#     if reduction == \"sum\":\n",
    "#         loss = -torch.sum(losses[padded_pairs_mask & ndcg_at_k_mask])\n",
    "#     elif reduction == \"mean\":\n",
    "#         loss = -torch.mean(losses[padded_pairs_mask & ndcg_at_k_mask])\n",
    "#     else:\n",
    "#         raise ValueError(\"Reduction method can be either sum or mean\")\n",
    "\n",
    "#     return loss\n",
    "\n",
    "\n",
    "# def ndcgLoss1_scheme(G, D, *args):\n",
    "#     return (G / D)[:, :, None]\n",
    "\n",
    "\n",
    "# def ndcgLoss2_scheme(G, D, *args):\n",
    "#     pos_idxs = torch.arange(1, G.shape[1] + 1, device=G.device)\n",
    "#     delta_idxs = torch.abs(pos_idxs[:, None] - pos_idxs[None, :])\n",
    "#     deltas = torch.abs(torch.pow(torch.abs(D[0, delta_idxs - 1]), -1.) - torch.pow(torch.abs(D[0, delta_idxs]), -1.))\n",
    "#     deltas.diagonal().zero_()\n",
    "\n",
    "#     return deltas[None, :, :] * torch.abs(G[:, :, None] - G[:, None, :])\n",
    "\n",
    "\n",
    "# def lambdaRank_scheme(G, D, *args):\n",
    "#     return torch.abs(torch.pow(D[:, :, None], -1.) - torch.pow(D[:, None, :], -1.)) * torch.abs(G[:, :, None] - G[:, None, :])\n",
    "\n",
    "\n",
    "# def ndcgLoss2PP_scheme(G, D, *args):\n",
    "#     return args[0] * ndcgLoss2_scheme(G, D) + lambdaRank_scheme(G, D)\n",
    "\n",
    "\n",
    "# def rankNet_scheme(G, D, *args):\n",
    "#     return 1.\n",
    "\n",
    "\n",
    "# def rankNetWeightedByGTDiff_scheme(G, D, *args):\n",
    "#     return torch.abs(args[1][:, :, None] - args[1][:, None, :])\n",
    "\n",
    "\n",
    "# def rankNetWeightedByGTDiffPowed_scheme(G, D, *args):\n",
    "#     return torch.abs(torch.pow(args[1][:, :, None], 2) - torch.pow(args[1][:, None, :], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76eaf1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchmetrics.functional import retrieval_normalized_dcg\n",
    "# preds = torch.tensor([[.1, .2, .3, 4, 70], [.1, .2, .3, 4, 2]])\n",
    "# target = torch.tensor([[10, 0, 8, 1, 5], [10, 0, 0, 1, 5]])\n",
    "# # preds = torch.tensor([[.1, .2, .3, 4, 70]])\n",
    "# # target = torch.tensor([[10, 0, 0, 1, 6]])\n",
    "\n",
    "# print(retrieval_normalized_dcg(preds[0], target[0], k=3))\n",
    "# print(ndcg_score(target[0].reshape(1,-1), preds[0].reshape(1,-1), k=3))\n",
    "# print(retrieval_normalized_dcg(preds[1], target[1], k=3))\n",
    "# print(ndcg_score(target[1].reshape(1,-1), preds[1].reshape(1,-1), k=3))\n",
    "# print(retrieval_normalized_dcg(preds, target, k=3))\n",
    "# print(ndcg_score(target, preds, k=3))\n",
    "\n",
    "# ndcg = RetrievalNormalizedDCG(k=3)\n",
    "# ndcg(preds, target, indexes=torch.vstack([torch.arange(preds.shape[0]) for _ in range(preds.shape[1])]).T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
