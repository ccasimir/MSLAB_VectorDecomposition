{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "982acb8f",
   "metadata": {},
   "source": [
    "## Siamese network \n",
    "Steps:\n",
    "1. load word embeding and document embedding\n",
    "2. create pytorch dataset and dataloader\n",
    "3. Try Contrastive loss and triplet loss\n",
    "4. further improve negative sampling (e.g. hard negative or word2vec negative sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95b262e",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "Goal:\n",
    "    for dataloader\n",
    "1. Read raw file\n",
    "2. \n",
    "\n",
    "CBOW:\n",
    "    local words and center word\n",
    "DNN:\n",
    "    Document vectors and words emb\n",
    "Triplet:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67d8234",
   "metadata": {},
   "source": [
    "#### raw data\n",
    "* word embedding: glove\n",
    "* doc text: ./data/IMDB.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b007799a",
   "metadata": {},
   "source": [
    "### preprocess\n",
    "1. truncate smallest k word in IDF\n",
    "2. stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6a82ca",
   "metadata": {},
   "source": [
    "### model\n",
    "1. k highest freq words\n",
    "2. CBOW\n",
    "3. Triplet\n",
    "4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b9967b",
   "metadata": {},
   "source": [
    "### evaluation\n",
    "1. F1\n",
    "2. F1 weighted by TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e9beff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np \n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from itertools import cycle\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dc01df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_document = 10000\n",
    "min_word_freq_threshold = 20\n",
    "topk_word_freq_threshold = 0\n",
    "document_vector_agg = 'TF-IDF'\n",
    "select_topk_TFIDF = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05c1627d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58cb4e49ebb148219424a64c59584d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:400000\n"
     ]
    }
   ],
   "source": [
    "# load word embedding\n",
    "embedding_file = \"../data/glove.6B.100d.txt\"\n",
    "\n",
    "word2embedding = dict()\n",
    "word_dim = int(re.findall(r\".(\\d+)d\",embedding_file)[0])\n",
    "\n",
    "with open(embedding_file,\"r\") as f:\n",
    "    for line in tqdm(f):\n",
    "        line = line.strip().split()\n",
    "        word = line[0]\n",
    "        embedding = list(map(float,line[1:]))\n",
    "        word2embedding[word] = embedding\n",
    "\n",
    "print(\"Number of words:%d\" % len(word2embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1233053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, word2embedding, min_word_freq_threshold=0, topk_word_freq_threshold=0):\n",
    "        # The low frequency words will be assigned as <UNK> token\n",
    "        self.itos = {0: \"<UNK>\"}\n",
    "        self.stoi = {\"<UNK>\": 0}\n",
    "        self.word2embedding = word2embedding\n",
    "        self.min_word_freq_threshold = min_word_freq_threshold\n",
    "        self.topk_word_freq_threshold = topk_word_freq_threshold\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer_eng(text):\n",
    "        text = re.sub(r'[^A-Za-z0-9 ]+', '', text)\n",
    "        return text.strip().split()\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        self.word_freq_in_corpus = defaultdict(int)\n",
    "        self.doc_freq = defaultdict(int) # # of document a word appear\n",
    "        self.document_num = len(sentence_list)\n",
    "        \n",
    "        self.word_vectors = [[0]*word_dim] # init zero padding\n",
    "        self.mask_word = set()\n",
    "        idx = 1\n",
    "        \n",
    "        for sentence in tqdm(sentence_list, desc=\"Preprocessing documents\"):\n",
    "            # for doc_freq\n",
    "            document_words = set()\n",
    "            \n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                # pass unknown word\n",
    "                if word not in self.word2embedding:\n",
    "                    continue\n",
    "                    \n",
    "                # calculate word freq\n",
    "                self.word_freq_in_corpus[word] += 1\n",
    "                \n",
    "                # validate word if it is more than min_word_freq_threshold times\n",
    "                if self.word_freq_in_corpus[word] == self.min_word_freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    self.word_vectors.append(self.word2embedding[word])\n",
    "                    idx += 1\n",
    "                \n",
    "                document_words.add(word)\n",
    "                \n",
    "            for word in document_words:\n",
    "                self.doc_freq[word] += 1\n",
    "        \n",
    "        # calculate IDF\n",
    "        self.IDF = {}\n",
    "        print('doc num', self.document_num)\n",
    "        for word, freq in self.doc_freq.items():\n",
    "            self.IDF[word] = math.log(self.document_num / (freq+1))\n",
    "        \n",
    "        # eliminate smallest K IDF words\n",
    "        IDF = [(word, freq) for word, freq in self.IDF.items()]\n",
    "        IDF.sort(key=lambda x: x[1])\n",
    "        \n",
    "        print('eliminate words')\n",
    "        for i in range(self.topk_word_freq_threshold):\n",
    "            if IDF[i][0] not in self.stoi:\n",
    "                continue\n",
    "            print(IDF[i][0])\n",
    "            idx = self.stoi[IDF[i][0]]\n",
    "            del self.stoi[IDF[i][0]]\n",
    "            del self.itos[idx]\n",
    "            del self.word_freq_in_corpus[IDF[i][0]]\n",
    "                \n",
    "    def calculate_document_vector(self, sentence_list, agg, select_topk_TFIDF=None):\n",
    "        document_vectors = []\n",
    "        document_answers = []\n",
    "        \n",
    "        for sentence in tqdm(sentence_list, desc=\"calculate document vectors\"):\n",
    "            document_vector = np.zeros(len(self.word_vectors[0]))\n",
    "            select_words = []\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                # pass unknown word\n",
    "                if word not in self.stoi:\n",
    "                    continue\n",
    "                else:\n",
    "                    select_words.append(word)\n",
    "\n",
    "            # select topk TDIDF\n",
    "            if select_topk_TFIDF is not None:\n",
    "                doc_TFIDF = defaultdict(float)\n",
    "                for word in select_words:    \n",
    "                    doc_TFIDF[word] += self.IDF[word]\n",
    "\n",
    "                doc_TFIDF_l = [(word, TFIDF) for word, TFIDF in doc_TFIDF.items()]\n",
    "                doc_TFIDF_l.sort(key=lambda x:x[1], reverse=True)\n",
    "                \n",
    "                select_topk_words = set(list(map(lambda x:x[0], doc_TFIDF_l[:select_topk_TFIDF])))\n",
    "                select_words = [word for word in select_words if word in select_topk_words]\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            # aggregate to doc vectors\n",
    "            for word in select_words:\n",
    "                if agg == 'mean':\n",
    "                    document_vector += self.word2embedding[word]\n",
    "                elif agg == 'TF-IDF':\n",
    "                    document_vector += np.array(self.word2embedding[word]) * self.IDF[word]\n",
    "\n",
    "            if len(select_words) == 0:\n",
    "                print('error', sentence)\n",
    "                return -1\n",
    "            else:\n",
    "                document_vector /= len(select_words)\n",
    "            \n",
    "            document_vectors.append(document_vector)\n",
    "            document_answers.append(select_words)\n",
    "        \n",
    "        # get answers\n",
    "        document_answers_idx = []    \n",
    "        for ans in document_answers:\n",
    "            ans_idx = []\n",
    "            for token in ans:\n",
    "                if token in self.stoi:\n",
    "                    ans_idx.append(self.stoi[token])                    \n",
    "            document_answers_idx.append(ans_idx)\n",
    "            \n",
    "        return document_vectors, document_answers_idx\n",
    "        \n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c5706a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBowDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 raw_data_file_path,\n",
    "                 word2embedding,\n",
    "                 skip_header = False,\n",
    "                 n_document = None, # read first n document\n",
    "                 min_word_freq_threshold = 20, # eliminate less freq words\n",
    "                 topk_word_freq_threshold = 5, # eliminate smallest k IDF words\n",
    "                 select_topk_TFIDF = None, # select topk tf-idf as ground-truth\n",
    "                 document_vector_agg = 'mean',\n",
    "                 ):\n",
    "\n",
    "        assert document_vector_agg in ['mean', 'TF-IDF']\n",
    "        \n",
    "        # raw documents\n",
    "        self.documents = []\n",
    "        # document vectors\n",
    "        self.document_vectors = []\n",
    "        \n",
    "        with open(raw_data_file_path,'r',encoding='utf-8') as f:\n",
    "            if skip_header:\n",
    "                f.readline()\n",
    "            for line in tqdm(f, desc=\"Loading documents\"):\n",
    "                # read firt n document\n",
    "                if n_document is not None and len(self.documents) >= n_document:\n",
    "                    break    \n",
    "                self.documents.append(line.strip(\"\\n\"))\n",
    "\n",
    "        # build vocabulary\n",
    "        self.vocab = Vocabulary(word2embedding, min_word_freq_threshold, topk_word_freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.documents)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "#         self.words_tokenized = [self.vocab.numericalize(text) for text in self.documents]\n",
    "        \n",
    "        # calculate document vectors\n",
    "        self.document_vectors, self.words_tokenized = self.vocab.calculate_document_vector(self.documents, \\\n",
    "                                                                                           document_vector_agg, select_topk_TFIDF)\n",
    "        # train-test split\n",
    "        # training\n",
    "        self.train_length = int(len(self.words_tokenized)*0.8)\n",
    "        self.train_vectors = self.document_vectors[:self.train_length]\n",
    "        self.train_words = self.words_tokenized[:self.train_length]\n",
    "        self.document_ids = list(range(self.train_length))\n",
    "        self.generator = cycle(self.context_target_generator())\n",
    "        self.dataset_size = sum([len(s) for s in self.train_words])\n",
    "        \n",
    "        # testing\n",
    "        self.test_vectors = self.document_vectors[self.train_length:]\n",
    "        self.test_words = self.words_tokenized[self.train_length:]\n",
    "\n",
    "    def context_target_generator(self):\n",
    "        np.random.shuffle(self.document_ids) # inplace shuffle\n",
    "\n",
    "        # randomly select a document and create its training example\n",
    "        for document_id in self.document_ids: \n",
    "            word_list = set(self.train_words[document_id])\n",
    "            negative_sample_space = list(set(range(self.vocab_size)) - word_list)\n",
    "            negative_samples = np.random.choice(negative_sample_space,size=len(word_list),replace = False)\n",
    "            for word_id, negative_wordID in zip(word_list, negative_samples):\n",
    "                yield [document_id, word_id, negative_wordID]\n",
    "                \n",
    "    def __getitem__(self, idx):\n",
    "        doc_id, word_id, negative_wordID = next(self.generator)\n",
    "        doc_id = torch.FloatTensor(self.document_vectors[doc_id])\n",
    "        word_id = torch.FloatTensor(self.vocab.word_vectors[word_id])\n",
    "        negative_word = torch.FloatTensor(self.vocab.word_vectors[negative_wordID])\n",
    "\n",
    "        return doc_id, word_id, negative_word\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1a7186c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e103a274314e348fc1c797ec9586c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading documents: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d9ffe6eb7eb4813acdd0a21f6903965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing documents:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc num 10000\n",
      "eliminate words\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a99cddf668394b8c87313e2b6266acbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "calculate document vectors:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish building dataset!\n",
      "Number of documents:10000\n",
      "Number of words:7533\n"
     ]
    }
   ],
   "source": [
    "# load and build torch dataset\n",
    "data_file_path = '../data/IMDB.txt'\n",
    "\n",
    "print(\"Building dataset....\")\n",
    "dataset = CBowDataset(\n",
    "                    raw_data_file_path=data_file_path,\n",
    "                    word2embedding=word2embedding,\n",
    "                    skip_header=False,\n",
    "                    n_document = n_document,\n",
    "                    min_word_freq_threshold = min_word_freq_threshold,\n",
    "                    topk_word_freq_threshold = topk_word_freq_threshold,\n",
    "                    document_vector_agg = document_vector_agg,\n",
    "                    select_topk_TFIDF = select_topk_TFIDF\n",
    "                    )\n",
    "print(\"Finish building dataset!\")\n",
    "print(f\"Number of documents:{len(dataset.documents)}\")\n",
    "print(f\"Number of words:{dataset.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf6bf5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 doc_vectors,\n",
    "                 ans_words,\n",
    "                 ):\n",
    "        self.doc_vectors = doc_vectors\n",
    "        self.ans_words = ans_words\n",
    "        assert len(doc_vectors) == len(ans_words)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        doc_vec = torch.FloatTensor(self.doc_vectors[idx])\n",
    "        ans_w = torch.tensor(list(set(self.ans_words[idx])))\n",
    "        return doc_vec, ans_w\n",
    "\n",
    "    def collate_fn(self,batch):\n",
    "        # Batch: List of tuples [(batch1), (batch2)]\n",
    "        \n",
    "        doc_vec = torch.cat([item[0].unsqueeze(0) for item in batch], dim=0)\n",
    "        ans_w = [item[1] for item in batch]\n",
    "        ans_w = pad_sequence(ans_w, batch_first=True, padding_value=-1)\n",
    "        \n",
    "        return doc_vec, ans_w \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doc_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "457c5d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletNet(nn.Module):\n",
    "    def __init__(self, hdim):\n",
    "        super(TripletNet, self).__init__()\n",
    "        self.fc = nn.Sequential(nn.Linear(hdim, 256),\n",
    "                        nn.PReLU(),\n",
    "                        nn.Linear(256, 256),\n",
    "                        nn.PReLU(),\n",
    "                        nn.Linear(256, 2)\n",
    "                        )\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        output1 = self.fc(x1)\n",
    "        output2 = self.fc(x2)\n",
    "        output3 = self.fc(x3)\n",
    "        return output1, output2, output3\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59ec9a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Triplet loss\n",
    "    Takes embeddings of an anchor sample, a positive sample and a negative sample\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative, size_average=True):\n",
    "        distance_positive = (anchor - positive).pow(2).sum(1)  # .pow(.5)\n",
    "        distance_negative = (anchor - negative).pow(2).sum(1)  # .pow(.5)\n",
    "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean() if size_average else losses.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b60ca7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "margin = 1.\n",
    "BATCH_SIZE = 1024\n",
    "EPOCH = 300\n",
    "\n",
    "device = \"cuda:0\"\n",
    "model = TripletNet(word_dim).to(device)\n",
    "loss_fn = TripletLoss(margin).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22a934f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "                        dataset, \n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        num_workers=4,\n",
    "                        shuffle=True,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2aa0290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docvec = dataset.test_vectors\n",
    "test_ans = dataset.test_words\n",
    "test_dataset = TestDataset(test_docvec,test_ans)\n",
    "test_loader = DataLoader(test_dataset,                         \n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         num_workers=4,\n",
    "                         collate_fn=test_dataset.collate_fn)\n",
    "word_embedding_tensor = torch.FloatTensor(dataset.vocab.word_vectors).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05bb89ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_word_emb, loader,Ks = [50,100,150,200]):\n",
    "    avg_precision, avg_recall = [], []\n",
    "    for batch in test_loader:\n",
    "        batch = [item.to(device) for item in batch]\n",
    "        emb, ans = batch\n",
    "        emb = model.get_embedding(emb)\n",
    "        scores = torch.cdist(emb, test_word_emb)\n",
    "        ans_length = torch.sum((~ans.eq(-1)).float(), dim=-1)\n",
    "        mask = ~ans.eq(-1).unsqueeze(-1)\n",
    "        \n",
    "        # calculate precision and recall\n",
    "        tmp_pr, tmp_re = [],[]\n",
    "        for K in Ks:\n",
    "            top_indices = torch.argsort(scores,dim=1)[:,:K]\n",
    "            hit = top_indices.unsqueeze(-2) == ans.unsqueeze(-1)\n",
    "            hit = torch.sum((hit * mask).flatten(1),dim=-1)\n",
    "            precision = hit / K\n",
    "            recall = hit / ans_length\n",
    "            tmp_pr.append(precision)\n",
    "            tmp_re.append(recall)\n",
    "        tmp_pr = torch.stack(tmp_pr).T.detach().cpu().numpy().tolist()\n",
    "        tmp_re = torch.stack(tmp_re).T.detach().cpu().numpy().tolist()\n",
    "        avg_precision.extend(tmp_pr)\n",
    "        avg_recall.extend(tmp_re)\n",
    "        \n",
    "    avg_precision = np.mean(avg_precision,axis=0)\n",
    "    avg_recall = np.mean(avg_recall, axis=0)\n",
    "    for idx, kval in enumerate(Ks):\n",
    "        print(f\"[K={kval}] Precision:{avg_precision[idx]:.4f} Recall:{avg_recall[idx]:.4f}\")\n",
    "    return avg_precision, avg_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c124d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(EPOCH):\n",
    "#     avg_loss = []\n",
    "#     model.train()\n",
    "#     for batch in tqdm(train_loader):\n",
    "#         batch = [item.to(device) for item in batch]\n",
    "#         doc_id,pos_w,neg_w = batch\n",
    "#         optimizer.zero_grad()\n",
    "#         loss = loss_fn(*model(doc_id,pos_w,neg_w))\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         avg_loss.append(loss.item())\n",
    "#     avg_loss = np.mean(avg_loss)\n",
    "#     print(f\"Loss:{avg_loss:4f}\")\n",
    "    \n",
    "#     # evaluate\n",
    "#     model.eval()\n",
    "#     test_word_emb = model.get_embedding(word_embedding_tensor)\n",
    "#     res = evaluate(test_word_emb,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae277ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "def evaluate_NDCG(test_word_emb, loader, topk=None):\n",
    "    NDCGs = defaultdict(list)\n",
    "    \n",
    "    for batch in (test_loader):\n",
    "        batch = [item.to(device) for item in batch]\n",
    "        emb, ans = batch\n",
    "        TFIDF_ans = np.zeros((len(ans), test_word_emb.shape[0]))\n",
    "        for i in range(len(ans)):\n",
    "            ans_row = ans[i]\n",
    "            ans_row = ans_row[~ans_row.eq(-1)]\n",
    "            ans_row = ans_row[~ans_row.eq(0)]\n",
    "            for word_id in ans_row:\n",
    "                word_id = word_id.item()\n",
    "                word = dataset.vocab.itos[word_id]\n",
    "                TFIDF_ans[i][word_id] += dataset.vocab.IDF[word]\n",
    "             \n",
    "        emb = model.get_embedding(emb)\n",
    "        scores = -torch.cdist(emb, test_word_emb).cpu().detach().numpy()\n",
    "        true_relevance = TFIDF_ans\n",
    "\n",
    "        NDCGs['top50'].append(ndcg_score(true_relevance, scores, k=50))\n",
    "        NDCGs['top100'].append(ndcg_score(true_relevance, scores, k=100))\n",
    "        NDCGs['top200'].append(ndcg_score(true_relevance, scores, k=200))\n",
    "        NDCGs['ALL'].append(ndcg_score(true_relevance, scores, k=None))\n",
    "    \n",
    "    print('NDCG top50', np.mean(NDCGs['top50']))\n",
    "    print('NDCG top100', np.mean(NDCGs['top100']))\n",
    "    print('NDCG top200', np.mean(NDCGs['top200']))\n",
    "    print('NDCG ALL', np.mean(NDCGs['ALL']))\n",
    "    return NDCGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f96c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d9ae7d2ed0540a2a7c5442b71547186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.305506\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ed4f8e22fa64fcb972d99b068d9df09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG top50 0.0619646844187639\n",
      "NDCG top100 0.09042869692375093\n",
      "NDCG top200 0.13439641600824404\n",
      "NDCG ALL 0.4483385346415185\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb1c900b1c643a7976f81b53e011e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.301450\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62541d6dbcb46c08b34e3df42809ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG top50 0.060092025928094824\n",
      "NDCG top100 0.08953223597164092\n",
      "NDCG top200 0.13424658979267123\n",
      "NDCG ALL 0.44814630600559147\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace1cfc040904443a646ca44b2ad0b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.299519\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb02f7f054f54814b4402e4b2ca34331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG top50 0.05919581934976732\n",
      "NDCG top100 0.08887181041240963\n",
      "NDCG top200 0.13361935674899794\n",
      "NDCG ALL 0.4474013728551325\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1623ffabe5c14286a087b11fa4717c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.322570\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "436ae0ec03db4afd84347fbca0bab692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG top50 0.06611332490617108\n",
      "NDCG top100 0.09237006427755486\n",
      "NDCG top200 0.1355189191680095\n",
      "NDCG ALL 0.44934649893111206\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2911d04fb9c04481b990253375d48935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.307150\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9b791e27ca4ac4a6b9f0499ff20fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG top50 0.05833176504317743\n",
      "NDCG top100 0.08805255585460489\n",
      "NDCG top200 0.13203344641313888\n",
      "NDCG ALL 0.44572650456773655\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06359b21e07d4f9c817ba4584c791d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.301666\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98bd1971491b4f3f82764400dfdbc93e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG top50 0.06063452413919296\n",
      "NDCG top100 0.09020892077763806\n",
      "NDCG top200 0.13428031339643992\n",
      "NDCG ALL 0.4479605294614182\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a88bf6030dd48c4b45127a86a4dfe98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.300216\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1880e90842df468b8e8bd0c494b7f774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG top50 0.061386898080554766\n",
      "NDCG top100 0.09073054747771767\n",
      "NDCG top200 0.13459320114360826\n",
      "NDCG ALL 0.4482885908229965\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85cc69f060394c87b82e08c918d618ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.301776\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78bbdc60753249d68de4ce60449e2071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG top50 0.05974852430311798\n",
      "NDCG top100 0.08893008346214475\n",
      "NDCG top200 0.13291445962807688\n",
      "NDCG ALL 0.44721191322252074\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88505265ea124d7f87d846e518cf2cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.300692\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12bb086cccbd4b819193790f60d8f79b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG top50 0.0601835752013056\n",
      "NDCG top100 0.08901255560249033\n",
      "NDCG top200 0.13298843988333847\n",
      "NDCG ALL 0.44688830823006453\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9127e7cd1bb4cc6a57d5b743069b099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.301657\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85923798d1eb49adbe677b018d156550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG top50 0.05863434288621186\n",
      "NDCG top100 0.08877973766581645\n",
      "NDCG top200 0.13306602996592726\n",
      "NDCG ALL 0.4464240511497411\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd69050884a943ecaf4630f5a9ce6676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.311457\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8841b0e7fd3d4d3c990e1feb06ce4cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG top50 0.06444123130656344\n",
      "NDCG top100 0.09147243359574156\n",
      "NDCG top200 0.13592698526866542\n",
      "NDCG ALL 0.44931278059791024\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c3b0071e60f4b29a86ef86dab7eb2e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.303516\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a878efc522fe461eac2b4ce8c01778cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG top50 0.0665812042490288\n",
      "NDCG top100 0.09332327279915556\n",
      "NDCG top200 0.13786596933168632\n",
      "NDCG ALL 0.4516086085135712\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9ed7e58902e4e2e802972668c121778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.302352\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb74e2d227214cb084c76146bfd2fe8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG top50 0.06629779867969726\n",
      "NDCG top100 0.09310741451704031\n",
      "NDCG top200 0.1374176155376164\n",
      "NDCG ALL 0.45144083463494794\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fcaf2c547ac45a4a8713ed2b25f4623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.315158\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f93ed978190454cbc154849905e0e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG top50 0.06205345708528831\n",
      "NDCG top100 0.0907715157125385\n",
      "NDCG top200 0.1353515088957077\n",
      "NDCG ALL 0.4478733651040635\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8d7b3c0d434a3e9fbe7a0f3eed3846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.304113\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2dbbeac866d4c27b6c78ac1b1acef34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG top50 0.061478642220540974\n",
      "NDCG top100 0.09013230726885382\n",
      "NDCG top200 0.13477571661953947\n",
      "NDCG ALL 0.44748881208248137\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658eafbaa77c42a6bb6e40cd546bcad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.303177\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c20390027caf4be8969e9500ac3714d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG top50 0.06466840676118699\n",
      "NDCG top100 0.09250816533151904\n",
      "NDCG top200 0.13748410395592248\n",
      "NDCG ALL 0.4504635666016017\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c2f261d0a145659c7d637bc9929771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.409597\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c54667dc5c3440a80d2598029f3f8c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG top50 0.0637513130488553\n",
      "NDCG top100 0.09189657791777775\n",
      "NDCG top200 0.13647170587760613\n",
      "NDCG ALL 0.44989537380400113\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e05b77d7eb8b47a6ad8ba12ebe652ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.301388\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9229bc80a88e430697fc81f9983fb173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG top50 0.06599133904721499\n",
      "NDCG top100 0.09298280292844814\n",
      "NDCG top200 0.13730047834341846\n",
      "NDCG ALL 0.45111606904144574\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1502d2cd5d14e36b7214468be47d48d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.302236\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8d0d1d5ba7a4ad59f4318653e23cdb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG top50 0.06296529378513534\n",
      "NDCG top100 0.0916297066337324\n",
      "NDCG top200 0.13616101278684792\n",
      "NDCG ALL 0.4491981885346109\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ebe0865e5945ce9d1146b35e182329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.305805\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f1b66486324452834ffe7d2d81a2a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG top50 0.0665860563165742\n",
      "NDCG top100 0.09351552116860413\n",
      "NDCG top200 0.13766477938515082\n",
      "NDCG ALL 0.4510165976437699\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a2d70603e3b4a59b39969813b842da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.330975\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c282a58d4864d38a5733de534be28b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG top50 0.06486724766170143\n",
      "NDCG top100 0.09307983249854318\n",
      "NDCG top200 0.1377874311170486\n",
      "NDCG ALL 0.4511709782221003\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a04991aa9e04d568004a458926704fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.301357\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f7d645ee824c91a25c7e3f54b3b429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG top50 0.06549343279634964\n",
      "NDCG top100 0.09310356021191374\n",
      "NDCG top200 0.137754026915147\n",
      "NDCG ALL 0.45137356611502566\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73081636551647a9ae2a55896e3a4dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.300109\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3a6d0ef0da45e9a58c498ac550e240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG top50 0.06594222051103898\n",
      "NDCG top100 0.09340085545266037\n",
      "NDCG top200 0.13787296336535335\n",
      "NDCG ALL 0.45139767477069853\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d38fd472524da08b2e457fdca76a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.297850\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f350d569807740b5be25d7564fcc3aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG top50 0.06314956404288946\n",
      "NDCG top100 0.09182728028793283\n",
      "NDCG top200 0.13647785755446143\n",
      "NDCG ALL 0.44991551044815925\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c44747f87df434e9af4abc4fe2bc969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1684 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.298139\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a29de041a91a4a9f8257b916ec697dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG top50 0.0657564336205156\n",
      "NDCG top100 0.0939900789704376\n",
      "NDCG top200 0.13842771870368684\n",
      "NDCG ALL 0.4515650539807071\n"
     ]
    }
   ],
   "source": [
    "validation_history = []\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    avg_loss = []\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch = [item.to(device) for item in batch]\n",
    "        doc_id,pos_w,neg_w = batch\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(*model(doc_id,pos_w,neg_w))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss.append(loss.item())\n",
    "    avg_loss = np.mean(avg_loss)\n",
    "    print(f\"Loss:{avg_loss:4f}\")\n",
    "    \n",
    "    # evaluate\n",
    "    model.eval()\n",
    "    test_word_emb = model.get_embedding(word_embedding_tensor)\n",
    "    ndcg_res = evaluate_NDCG(test_word_emb,test_loader)\n",
    "    validation_history.append(ndcg_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781f1011",
   "metadata": {},
   "source": [
    "## Top K freq word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97bdd19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 137959),\n",
       " ('and', 71459),\n",
       " ('a', 66359),\n",
       " ('of', 61514),\n",
       " ('to', 52823),\n",
       " ('is', 45511),\n",
       " ('in', 39808),\n",
       " ('it', 31652),\n",
       " ('i', 29011),\n",
       " ('this', 27891)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = [(word, freq) for word, freq in dataset.vocab.word_freq_in_corpus.items()]\n",
    "word_freq.sort(key=lambda x:x[1], reverse=True)\n",
    "word_freq[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d065f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75acebea9db8491c8b043d5a70891836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 50 word\n",
      "percision 0.57312\n",
      "recall 0.25995568997152513\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4fd7926169d4b6cb478491423c1c626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 100 word\n",
      "percision 0.42140999999999995\n",
      "recall 0.3705359126710024\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625a5202010d44fb9117bea593274da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 200 word\n",
      "percision 0.2830875\n",
      "recall 0.48635047389378483\n"
     ]
    }
   ],
   "source": [
    "def topk_word_evaluation(k=50):\n",
    "    topk_word = [word for (word, freq) in word_freq[:k]]\n",
    "\n",
    "    pr, re = [], []\n",
    "    for ans in tqdm(test_ans):\n",
    "        ans = set(ans)\n",
    "        ans = [dataset.vocab.itos[a] for a in ans]\n",
    "\n",
    "        hit = []\n",
    "        for word in ans:\n",
    "            if word in topk_word:\n",
    "                hit.append(word)\n",
    "\n",
    "        precision = len(hit) / k\n",
    "        recall = len(hit) / len(ans)\n",
    "        pr.append(precision)\n",
    "        re.append(recall)\n",
    "\n",
    "    print('top {} word'.format(k))\n",
    "    print('percision', np.mean(pr))\n",
    "    print('recall', np.mean(re))\n",
    "\n",
    "topk_word_evaluation(k=50)\n",
    "topk_word_evaluation(k=100)\n",
    "topk_word_evaluation(k=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "786e8f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## test dcg\n",
    "# from sklearn.metrics import ndcg_score, dcg_score\n",
    "# k=2\n",
    "\n",
    "# true_relevance = np.asarray([[1, 2, 3, 4]])\n",
    "# scores = np.asarray([[1, 2, 3, 2.5]])\n",
    "# print('dcg',dcg_score(true_relevance, scores,k=k))\n",
    "# print('ndcg',ndcg_score(true_relevance, scores,k=k))\n",
    "\n",
    "\n",
    "# w = 1 / (np.log(np.arange(true_relevance.shape[1])[:k] + 2) / np.log(2))\n",
    "# dcg = true_relevance[0][np.argsort(scores)[0][::-1][:k]].dot(w)\n",
    "# print(dcg)\n",
    "\n",
    "# idcg = np.sort(true_relevance[0])[::-1][:k].dot(w)\n",
    "# print(dcg/idcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f17a67be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8fbd4921a5c43fda73823b706ab7334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 50 NDCG:0.08714991654386776\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c336be817de44fba76337ad8b0c6bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 100 NDCG:0.11619786532653996\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9773784af434fdd9b8757cce7671834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 200 NDCG:0.15694766452897216\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a64fb9bd6da4f2999b17941b1c417c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top None NDCG:0.43048368192931924\n"
     ]
    }
   ],
   "source": [
    "def topk_word_evaluation_NDCG(k=50):\n",
    "    freq_word =[word for (word, freq) in word_freq]\n",
    "    freq_word_idx = [dataset.vocab.stoi[word] for word in freq_word if word in dataset.vocab.stoi]\n",
    "    \n",
    "    scores = np.zeros(len(dataset.vocab.word_vectors))\n",
    "    for rank, idx in enumerate(freq_word_idx):\n",
    "        scores[idx] = len(dataset.vocab.word_vectors) - rank\n",
    "    \n",
    "    NDCGs = []\n",
    "    \n",
    "    for ans in tqdm(test_ans):\n",
    "        TFIDF_ans = np.zeros(len(dataset.vocab.word_vectors))\n",
    "        \n",
    "        for word_idx in ans:\n",
    "            if word_idx == 0:\n",
    "                continue\n",
    "            word = dataset.vocab.itos[word_idx]\n",
    "            TFIDF_ans[word_idx] += dataset.vocab.IDF[word]\n",
    "\n",
    "        NDCG_score = ndcg_score(TFIDF_ans.reshape(1,-1), scores.reshape(1,-1), k=k)\n",
    "        NDCGs.append(NDCG_score)\n",
    "\n",
    "    print('top {} NDCG:{}'.format(k, np.mean(NDCGs)))\n",
    "\n",
    "topk_word_evaluation_NDCG(k=50)\n",
    "topk_word_evaluation_NDCG(k=100)\n",
    "topk_word_evaluation_NDCG(k=200)\n",
    "topk_word_evaluation_NDCG(k=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d0ae77",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6e7575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select > 0\n",
    "def metric1(binary_x, answer, verbose=0):\n",
    "    binary_x = binary_x > 0\n",
    "    pred = np.arange(len(binary_x))[binary_x]\n",
    "    \n",
    "    hit = np.intersect1d(pred, answer)\n",
    "    hit_num = len(hit)\n",
    "    recall = hit_num / len(answer)\n",
    "    precision = hit_num / len(pred)\n",
    "    if verbose == 1:\n",
    "        print('answer:', word_list[answer])\n",
    "        print('hit:', word_list[hit])    \n",
    "    return {\"recall\": recall, \"precision\": precision}\n",
    "\n",
    "# select > 0.5\n",
    "def metric2(binary_x, answer, th=0.5, verbose=0):\n",
    "#     binary_x = np.array([int(np.round(i)) for i in binary_x])\n",
    "    pred = np.arange(len(binary_x))[binary_x>=th]\n",
    "\n",
    "    hit = np.intersect1d(pred, answer)\n",
    "    hit_num = len(hit)\n",
    "    recall = hit_num / len(answer)\n",
    "    precision = hit_num / len(pred)\n",
    "    if verbose == 1:\n",
    "        print('answer:', word_list[answer])\n",
    "        print('hit:', word_list[hit])    \n",
    "    return {\"recall\": recall, \"precision\": precision}\n",
    "\n",
    "# select #answer largest pred\n",
    "def metric3(binary_x, answer, w_idx=None, scale=1, verbose=0):\n",
    "    select_num = int(len(answer) * scale)\n",
    "    if w_idx is not None:\n",
    "        pred = w_idx[np.argsort(binary_x)[-select_num:]]\n",
    "    else:\n",
    "        pred = np.arange(len(binary_x))[np.argsort(binary_x)[-select_num:]]\n",
    "    \n",
    "    hit = np.intersect1d(pred, answer)\n",
    "    hit_num = len(hit)\n",
    "    recall = hit_num / len(answer)\n",
    "    precision = hit_num / len(pred)\n",
    "    if verbose == 1:\n",
    "        print('answer:', word_list[answer])\n",
    "        print('hit:', word_list[hit])\n",
    "    return {\"recall\": recall, \"precision\": precision}\n",
    "\n",
    "# select #answer largest pred\n",
    "def metric4(binary_x, answer, w_idx=None, topk=50, verbose=0):\n",
    "    select_num = topk\n",
    "    answer = list(set(answer))\n",
    "    \n",
    "    if w_idx is not None:\n",
    "        pred = w_idx[np.argsort(binary_x)[-select_num:]]\n",
    "    else:\n",
    "        pred = np.arange(len(binary_x))[np.argsort(binary_x)[-select_num:]]\n",
    "    \n",
    "    hit = np.intersect1d(pred, answer)\n",
    "    hit_num = len(hit)\n",
    "    recall = hit_num / len(answer)\n",
    "    precision = hit_num / len(pred)\n",
    "    if verbose == 1:\n",
    "        print('answer:', word_list[answer])\n",
    "        print('hit:', word_list[hit])\n",
    "    return {\"recall\": recall, \"precision\": precision}\n",
    "\n",
    "# select #answer largest pred\n",
    "def metric_ndcg(binary_x, answer, topk=50, verbose=0):\n",
    "\n",
    "    TFIDF_ans = np.zeros(len(binary_x))\n",
    "    for word_idx in answer:\n",
    "        if word_idx == 0:\n",
    "            continue\n",
    "        word = dataset.vocab.itos[word_idx]\n",
    "        TFIDF_ans[word_idx] += dataset.vocab.IDF[word]\n",
    "    NDCG_score = ndcg_score(TFIDF_ans.reshape(1,-1), binary_x.reshape(1,-1), k=topk)\n",
    "    \n",
    "    if verbose == 1:\n",
    "        print('NDCG_score:', NDCG_score)\n",
    "    return NDCG_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1aa22bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "class PyTorchLinearRegression:\n",
    "    ''' Class that implemnets Multiple Linear Regression with PyTorch'''\n",
    "    def __init__(self, num_of_features, lr, constraintHigh, constraintLow, total, init_type=0, L1=0, L2=0):\n",
    "        if init_type == 0:\n",
    "            self.w = torch.zeros(num_of_features, requires_grad=True)\n",
    "        elif init_type == 1:\n",
    "            self.w = torch.ones(num_of_features, requires_grad=True)\n",
    "        elif init_type == 2:  \n",
    "            self.w = torch.rand(num_of_features, requires_grad=True)\n",
    "        elif init_type == 3:\n",
    "            self.w = -torch.ones(num_of_features, requires_grad=True)\n",
    "\n",
    "        self.learning_rate = lr\n",
    "        self.high = constraintHigh\n",
    "        self.low = constraintLow\n",
    "        self.total = total\n",
    "        self.rg2 = total / num_of_features\n",
    "        self.L1 = L1\n",
    "        self.L2 = L2\n",
    "        \n",
    "    def _model(self, X):\n",
    "        return X @ self.w.t()# + self.b\n",
    "    \n",
    "    def _mse(self, pred, real):\n",
    "        difference = pred - real\n",
    "        return torch.sum(difference * difference) / difference.numel()\n",
    "    \n",
    "    def _regularization_weightdist(self):\n",
    "        difference = self.w - 1\n",
    "        return -torch.sum(difference * difference) / difference.numel()\n",
    "    \n",
    "    def _regularization_weightsum(self):\n",
    "        difference = torch.sum(self.w) - self.total\n",
    "        return difference * difference / self.w.numel()\n",
    "        \n",
    "#     def _regularization_L1(self):\n",
    "#         return self.w.norm(1)#torch.sum(torch.abs(self.w))\n",
    "    \n",
    "#     def _regularization_L2(self):\n",
    "#         return self.w.norm(2)\n",
    "    \n",
    "    def fit(self, X, y, epochs):\n",
    "        print(loss_weight)\n",
    "        X = torch.from_numpy(X).float()\n",
    "        y = torch.from_numpy(y).float()\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            predictions = self._model(X)\n",
    "            loss1 = self._mse(predictions, y)\n",
    "            loss2 = self._regularization_weightdist()\n",
    "            loss3 = self._regularization_weightsum()\n",
    "            loss = loss1 * loss_weight[0] + loss2 * loss_weight[1] + loss3 * loss_weight[2]\n",
    "#             loss = loss1\n",
    "            \n",
    "            if (i % (epochs//20)) == 0:\n",
    "                print(f'Epoch: {i} - Loss: {loss1}')\n",
    "            \n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                self.w -= (self.w.grad) * self.learning_rate + torch.sign(self.w)*self.L1 + self.w*self.L2\n",
    "                self.w.grad.zero_()\n",
    "                self.w.data.clamp_(min=self.low, max=self.high)\n",
    "\n",
    "#             x = 100\n",
    "#             if i % x == x-1:\n",
    "# #                 self.w=torch.tensor(self.low + (self.high-self.low)*(self.w - torch.min(self.w))/(torch.max(self.w) - torch.min(self.w)), requires_grad=True)\n",
    "#                 self.w.data.clamp_(min=self.low, max=self.high)\n",
    "#                 pass\n",
    "                \n",
    "    def predict(self, X):\n",
    "        X = torch.from_numpy(X).float()\n",
    "        return self._model(X)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        X = torch.from_numpy(X).float()\n",
    "        y_pred = self._model(X).detach().numpy()\n",
    "        return r2_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5befacd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7533, 100)\n",
      "(10000, 100)\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "word_embs = np.array(dataset.vocab.word_vectors)\n",
    "doc_embs = np.array(dataset.document_vectors)\n",
    "doc_answers = dataset.words_tokenized\n",
    "word_list = dataset.vocab.itos\n",
    "\n",
    "print(word_embs.shape)\n",
    "print(doc_embs.shape)\n",
    "print(len(doc_answers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8a41331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d96fec0c1f34472b1279055a9e0d995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.4937790334224701\n",
      "Epoch: 50 - Loss: 0.008251933380961418\n",
      "Epoch: 100 - Loss: 0.004072100855410099\n",
      "Epoch: 150 - Loss: 0.002774544758722186\n",
      "Epoch: 200 - Loss: 0.002185127465054393\n",
      "Epoch: 250 - Loss: 0.0018590078689157963\n",
      "Epoch: 300 - Loss: 0.0016588633880019188\n",
      "Epoch: 350 - Loss: 0.0015220518689602613\n",
      "Epoch: 400 - Loss: 0.00142097647767514\n",
      "Epoch: 450 - Loss: 0.001343129319138825\n",
      "Epoch: 500 - Loss: 0.0012788015883415937\n",
      "Epoch: 550 - Loss: 0.0012312212493270636\n",
      "Epoch: 600 - Loss: 0.0011921662371605635\n",
      "Epoch: 650 - Loss: 0.0011574241798371077\n",
      "Epoch: 700 - Loss: 0.0011261473409831524\n",
      "Epoch: 750 - Loss: 0.0010994613403454423\n",
      "Epoch: 800 - Loss: 0.0010740638244897127\n",
      "Epoch: 850 - Loss: 0.00104913127142936\n",
      "Epoch: 900 - Loss: 0.0010235417867079377\n",
      "Epoch: 950 - Loss: 0.0010017292806878686\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.3999672830104828\n",
      "Epoch: 50 - Loss: 0.0017954091308638453\n",
      "Epoch: 100 - Loss: 0.0011469563469290733\n",
      "Epoch: 150 - Loss: 0.0009346236474812031\n",
      "Epoch: 200 - Loss: 0.0008334752055816352\n",
      "Epoch: 250 - Loss: 0.0007788099464960396\n",
      "Epoch: 300 - Loss: 0.0007460271008312702\n",
      "Epoch: 350 - Loss: 0.0007233393262140453\n",
      "Epoch: 400 - Loss: 0.0007038371986709535\n",
      "Epoch: 450 - Loss: 0.0006872309022583067\n",
      "Epoch: 500 - Loss: 0.0006740298122167587\n",
      "Epoch: 550 - Loss: 0.0006634581950493157\n",
      "Epoch: 600 - Loss: 0.000653024238999933\n",
      "Epoch: 650 - Loss: 0.0006443641614168882\n",
      "Epoch: 700 - Loss: 0.0006366076995618641\n",
      "Epoch: 750 - Loss: 0.0006271200836636126\n",
      "Epoch: 800 - Loss: 0.0006227256963029504\n",
      "Epoch: 850 - Loss: 0.0006140305777080357\n",
      "Epoch: 900 - Loss: 0.0006035081460140646\n",
      "Epoch: 950 - Loss: 0.0005997817497700453\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.32694846391677856\n",
      "Epoch: 50 - Loss: 0.0019220821559429169\n",
      "Epoch: 100 - Loss: 0.0013083337107673287\n",
      "Epoch: 150 - Loss: 0.0011130205821245909\n",
      "Epoch: 200 - Loss: 0.0010199642274528742\n",
      "Epoch: 250 - Loss: 0.0009709432488307357\n",
      "Epoch: 300 - Loss: 0.000934441399294883\n",
      "Epoch: 350 - Loss: 0.000911156996153295\n",
      "Epoch: 400 - Loss: 0.0008855118067003787\n",
      "Epoch: 450 - Loss: 0.0008678033482283354\n",
      "Epoch: 500 - Loss: 0.0008509854087606072\n",
      "Epoch: 550 - Loss: 0.0008371963631361723\n",
      "Epoch: 600 - Loss: 0.0008269546087831259\n",
      "Epoch: 650 - Loss: 0.0008145280298776925\n",
      "Epoch: 700 - Loss: 0.0008037164225243032\n",
      "Epoch: 750 - Loss: 0.0007931425352580845\n",
      "Epoch: 800 - Loss: 0.000783015857450664\n",
      "Epoch: 850 - Loss: 0.0007709449273534119\n",
      "Epoch: 900 - Loss: 0.0007616740185767412\n",
      "Epoch: 950 - Loss: 0.0007552608149126172\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.29119786620140076\n",
      "Epoch: 50 - Loss: 0.0025044726207852364\n",
      "Epoch: 100 - Loss: 0.0016979598440229893\n",
      "Epoch: 150 - Loss: 0.0014450071612372994\n",
      "Epoch: 200 - Loss: 0.001330251689068973\n",
      "Epoch: 250 - Loss: 0.0012653856538236141\n",
      "Epoch: 300 - Loss: 0.0012217502808198333\n",
      "Epoch: 350 - Loss: 0.0011899886885657907\n",
      "Epoch: 400 - Loss: 0.001159856328740716\n",
      "Epoch: 450 - Loss: 0.0011336697498336434\n",
      "Epoch: 500 - Loss: 0.001111463992856443\n",
      "Epoch: 550 - Loss: 0.0010920163476839662\n",
      "Epoch: 600 - Loss: 0.0010721171274781227\n",
      "Epoch: 650 - Loss: 0.0010533018503338099\n",
      "Epoch: 700 - Loss: 0.0010397047735750675\n",
      "Epoch: 750 - Loss: 0.0010241343406960368\n",
      "Epoch: 800 - Loss: 0.0010115941986441612\n",
      "Epoch: 850 - Loss: 0.0010022259084507823\n",
      "Epoch: 900 - Loss: 0.0009973172564059496\n",
      "Epoch: 950 - Loss: 0.0009892946109175682\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.2629016637802124\n",
      "Epoch: 50 - Loss: 0.0023546568118035793\n",
      "Epoch: 100 - Loss: 0.001603343989700079\n",
      "Epoch: 150 - Loss: 0.0013683083234354854\n",
      "Epoch: 200 - Loss: 0.0012553216656669974\n",
      "Epoch: 250 - Loss: 0.0011909273453056812\n",
      "Epoch: 300 - Loss: 0.0011466736905276775\n",
      "Epoch: 350 - Loss: 0.0011115303495898843\n",
      "Epoch: 400 - Loss: 0.0010846926597878337\n",
      "Epoch: 450 - Loss: 0.0010602816473692656\n",
      "Epoch: 500 - Loss: 0.0010407508816570044\n",
      "Epoch: 550 - Loss: 0.001022562850266695\n",
      "Epoch: 600 - Loss: 0.0010077571496367455\n",
      "Epoch: 650 - Loss: 0.0009938522707670927\n",
      "Epoch: 700 - Loss: 0.0009815823286771774\n",
      "Epoch: 750 - Loss: 0.0009705182746984065\n",
      "Epoch: 800 - Loss: 0.0009621597127988935\n",
      "Epoch: 850 - Loss: 0.0009529250091873109\n",
      "Epoch: 900 - Loss: 0.0009444640018045902\n",
      "Epoch: 950 - Loss: 0.0009358999086543918\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.3583441972732544\n",
      "Epoch: 50 - Loss: 0.0026473274920135736\n",
      "Epoch: 100 - Loss: 0.0016524138627573848\n",
      "Epoch: 150 - Loss: 0.0013424914795905352\n",
      "Epoch: 200 - Loss: 0.0011985127348452806\n",
      "Epoch: 250 - Loss: 0.0011138085974380374\n",
      "Epoch: 300 - Loss: 0.001052633859217167\n",
      "Epoch: 350 - Loss: 0.0010098612401634455\n",
      "Epoch: 400 - Loss: 0.0009807220194488764\n",
      "Epoch: 450 - Loss: 0.0009532459662295878\n",
      "Epoch: 500 - Loss: 0.0009302171529270709\n",
      "Epoch: 550 - Loss: 0.0009110263781622052\n",
      "Epoch: 600 - Loss: 0.0008963398868218064\n",
      "Epoch: 650 - Loss: 0.0008836002671159804\n",
      "Epoch: 700 - Loss: 0.0008747397223487496\n",
      "Epoch: 750 - Loss: 0.0008658937294967473\n",
      "Epoch: 800 - Loss: 0.0008561526774428785\n",
      "Epoch: 850 - Loss: 0.0008438024669885635\n",
      "Epoch: 900 - Loss: 0.0008370910072699189\n",
      "Epoch: 950 - Loss: 0.0008271695696748793\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.39501309394836426\n",
      "Epoch: 50 - Loss: 0.0037351411301642656\n",
      "Epoch: 100 - Loss: 0.002259191358461976\n",
      "Epoch: 150 - Loss: 0.001778792473487556\n",
      "Epoch: 200 - Loss: 0.0015509106451645494\n",
      "Epoch: 250 - Loss: 0.0014265404315665364\n",
      "Epoch: 300 - Loss: 0.001343784504570067\n",
      "Epoch: 350 - Loss: 0.0012868389021605253\n",
      "Epoch: 400 - Loss: 0.0012443732703104615\n",
      "Epoch: 450 - Loss: 0.0012135100550949574\n",
      "Epoch: 500 - Loss: 0.0011845246190205216\n",
      "Epoch: 550 - Loss: 0.001165743451565504\n",
      "Epoch: 600 - Loss: 0.0011452980106696486\n",
      "Epoch: 650 - Loss: 0.0011281328042969108\n",
      "Epoch: 700 - Loss: 0.0011129918275400996\n",
      "Epoch: 750 - Loss: 0.0010979253565892577\n",
      "Epoch: 800 - Loss: 0.001086046569980681\n",
      "Epoch: 850 - Loss: 0.0010799013543874025\n",
      "Epoch: 900 - Loss: 0.001066313125193119\n",
      "Epoch: 950 - Loss: 0.001060068723745644\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.4594181180000305\n",
      "Epoch: 50 - Loss: 0.0022543505765497684\n",
      "Epoch: 100 - Loss: 0.0014355842722579837\n",
      "Epoch: 150 - Loss: 0.0012029609642922878\n",
      "Epoch: 200 - Loss: 0.0010998412035405636\n",
      "Epoch: 250 - Loss: 0.001047965488396585\n",
      "Epoch: 300 - Loss: 0.0010133220348507166\n",
      "Epoch: 350 - Loss: 0.0009920619195327163\n",
      "Epoch: 400 - Loss: 0.0009671918815001845\n",
      "Epoch: 450 - Loss: 0.0009455616818740964\n",
      "Epoch: 500 - Loss: 0.0009276114287786186\n",
      "Epoch: 550 - Loss: 0.0009122104966081679\n",
      "Epoch: 600 - Loss: 0.0008990173228085041\n",
      "Epoch: 650 - Loss: 0.000887976260855794\n",
      "Epoch: 700 - Loss: 0.0008816514746285975\n",
      "Epoch: 750 - Loss: 0.0008753263973630965\n",
      "Epoch: 800 - Loss: 0.0008627291535958648\n",
      "Epoch: 850 - Loss: 0.0008588109631091356\n",
      "Epoch: 900 - Loss: 0.0008545495802536607\n",
      "Epoch: 950 - Loss: 0.0008456854266114533\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.3703095614910126\n",
      "Epoch: 50 - Loss: 0.002026823116466403\n",
      "Epoch: 100 - Loss: 0.0013420215109363198\n",
      "Epoch: 150 - Loss: 0.0011559238191694021\n",
      "Epoch: 200 - Loss: 0.0010757684940472245\n",
      "Epoch: 250 - Loss: 0.0010282204020768404\n",
      "Epoch: 300 - Loss: 0.0009977244772017002\n",
      "Epoch: 350 - Loss: 0.0009748084121383727\n",
      "Epoch: 400 - Loss: 0.0009558990714140236\n",
      "Epoch: 450 - Loss: 0.0009415438980795443\n",
      "Epoch: 500 - Loss: 0.000931937072891742\n",
      "Epoch: 550 - Loss: 0.0009233906166628003\n",
      "Epoch: 600 - Loss: 0.0009139029425568879\n",
      "Epoch: 650 - Loss: 0.000905259104911238\n",
      "Epoch: 700 - Loss: 0.0008936681551858783\n",
      "Epoch: 750 - Loss: 0.0008837876375764608\n",
      "Epoch: 800 - Loss: 0.0008783271186985075\n",
      "Epoch: 850 - Loss: 0.0008734285947866738\n",
      "Epoch: 900 - Loss: 0.000863308145198971\n",
      "Epoch: 950 - Loss: 0.0008582072914578021\n",
      "[1, 0, 0]\n",
      "Epoch: 0 - Loss: 0.36652711033821106\n",
      "Epoch: 50 - Loss: 0.0013726209290325642\n",
      "Epoch: 100 - Loss: 0.0009666215046308935\n",
      "Epoch: 150 - Loss: 0.0008601221488788724\n",
      "Epoch: 200 - Loss: 0.0008135445532388985\n",
      "Epoch: 250 - Loss: 0.0007890380802564323\n",
      "Epoch: 300 - Loss: 0.0007783846813254058\n",
      "Epoch: 350 - Loss: 0.0007691761711612344\n",
      "Epoch: 400 - Loss: 0.0007616460206918418\n",
      "Epoch: 450 - Loss: 0.0007532175513915718\n",
      "Epoch: 500 - Loss: 0.0007467916584573686\n",
      "Epoch: 550 - Loss: 0.0007420955807901919\n",
      "Epoch: 600 - Loss: 0.0007330192602239549\n",
      "Epoch: 650 - Loss: 0.0007257519755512476\n",
      "Epoch: 700 - Loss: 0.0007206693408079445\n",
      "Epoch: 750 - Loss: 0.0007162233232520521\n",
      "Epoch: 800 - Loss: 0.0007099669892340899\n",
      "Epoch: 850 - Loss: 0.0007046930259093642\n",
      "Epoch: 900 - Loss: 0.0006984926876612008\n",
      "Epoch: 950 - Loss: 0.0006925419438630342\n"
     ]
    }
   ],
   "source": [
    "pr, re = [[],[],[]], [[],[],[]]\n",
    "ndcgs = defaultdict(list)\n",
    "\n",
    "lr = 0.001\n",
    "epochs = 1000\n",
    "constraintHigh=1\n",
    "constraintLow=0\n",
    "# constraintHigh=float('inf')\n",
    "# constraintLow=-float('inf')\n",
    "loss_weight = [1, 0, 0]\n",
    "L1, L2 = 1e-5, 0\n",
    "rand_type = 0\n",
    "\n",
    "total_mul = 1\n",
    "\n",
    "for uid, uemb in enumerate(tqdm(doc_embs[:10])):\n",
    "    x = word_embs.T\n",
    "    y = uemb\n",
    "    total = len(doc_answers[uid])\n",
    "\n",
    "    torch_model = PyTorchLinearRegression(x.shape[1], lr, constraintHigh, constraintLow, int(total*total_mul), rand_type, L1, L2)\n",
    "    torch_model.fit(x, y, epochs)\n",
    "    \n",
    "    m1 = metric4(torch_model.w.detach().numpy(), doc_answers[uid], w_idx=None, topk=50, verbose=0)\n",
    "    m2 = metric4(torch_model.w.detach().numpy(), doc_answers[uid], w_idx=None, topk=100, verbose=0)\n",
    "    m3 = metric4(torch_model.w.detach().numpy(), doc_answers[uid], w_idx=None, topk=200, verbose=0)\n",
    "    ndcg1 = metric_ndcg(torch_model.w.detach().numpy(), doc_answers[uid], topk=50, verbose=0)\n",
    "    ndcg2 = metric_ndcg(torch_model.w.detach().numpy(), doc_answers[uid], topk=100, verbose=0)\n",
    "    ndcg3 = metric_ndcg(torch_model.w.detach().numpy(), doc_answers[uid], topk=200, verbose=0)\n",
    "    ndcg4 = metric_ndcg(torch_model.w.detach().numpy(), doc_answers[uid], topk=None, verbose=0)\n",
    "    pr[0].append(m1[\"precision\"])\n",
    "    re[0].append(m1[\"recall\"])\n",
    "    pr[1].append(m2[\"precision\"])\n",
    "    re[1].append(m2[\"recall\"])\n",
    "    pr[2].append(m3[\"precision\"])\n",
    "    re[2].append(m3[\"recall\"])\n",
    "    \n",
    "    ndcgs[\"50\"].append(ndcg1)\n",
    "    ndcgs[\"100\"].append(ndcg2)\n",
    "    ndcgs[\"200\"].append(ndcg3)\n",
    "    ndcgs[\"-1\"].append(ndcg4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b84ee06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:0.4260 Recall:0.1797\n",
      "Precision:0.3290 Recall:0.2738\n",
      "Precision:0.2495 Recall:0.4015\n",
      "NDCG 50:0.2574\n",
      "NDCG 100:0.2772\n",
      "NDCG 200:0.3099\n",
      "NDCG all:0.5551\n"
     ]
    }
   ],
   "source": [
    "print(f\"Precision:{np.mean(pr[0]):.4f} Recall:{np.mean(re[0]):.4f}\")\n",
    "print(f\"Precision:{np.mean(pr[1]):.4f} Recall:{np.mean(re[1]):.4f}\")\n",
    "print(f\"Precision:{np.mean(pr[2]):.4f} Recall:{np.mean(re[2]):.4f}\")\n",
    "print(f\"NDCG 50:{np.mean(ndcgs['50']):.4f}\")\n",
    "print(f\"NDCG 100:{np.mean(ndcgs['100']):.4f}\")\n",
    "print(f\"NDCG 200:{np.mean(ndcgs['200']):.4f}\")\n",
    "print(f\"NDCG all:{np.mean(ndcgs['-1']):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9084693",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
