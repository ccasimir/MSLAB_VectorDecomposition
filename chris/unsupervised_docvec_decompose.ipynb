{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a67d8234",
   "metadata": {},
   "source": [
    "### raw data\n",
    "* word embedding: glove\n",
    "* doc text: ./data/IMDB.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792dad29",
   "metadata": {},
   "source": [
    "### dataset\n",
    "1. IMDB\n",
    "2. CNNNews\n",
    "3. [PubMed](https://github.com/LIAAD/KeywordExtractor-Datasets/blob/master/datasets/PubMed.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b007799a",
   "metadata": {},
   "source": [
    "### preprocess\n",
    "1. filter too frequent and less frequent words\n",
    "2. stemming\n",
    "3. document vector aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6a82ca",
   "metadata": {},
   "source": [
    "### model\n",
    "1. TopK\n",
    "2. Sklearn\n",
    "3. Our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b9967b",
   "metadata": {},
   "source": [
    "### evaluation\n",
    "1. F1\n",
    "2. NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e9beff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/chrisliu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np \n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from itertools import cycle\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Used to get the data\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "seed = 33\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2341254",
   "metadata": {},
   "source": [
    "## Preprocess config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dc01df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "config[\"dataset\"] = \"CNN\" # \"IMDB\" \"CNN\", \"PubMed\"\n",
    "config[\"n_document\"] = 100\n",
    "config[\"normalize_word_embedding\"] = False\n",
    "config[\"min_word_freq_threshold\"] = 20\n",
    "config[\"topk_word_freq_threshold\"] = 100\n",
    "config[\"document_vector_agg_weight\"] = 'IDF' # ['mean', 'IDF', 'uniform', 'gaussian', 'exponential', 'pmi']\n",
    "config[\"document_vector_weight_normalize\"] = True # weighted sum or mean, True for mean, False for sum \n",
    "config[\"select_topk_TFIDF\"] = None # ignore\n",
    "config[\"embedding_file\"] = \"../data/glove.6B.100d.txt\"\n",
    "config[\"topk\"] = [10, 30, 50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05b0b76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_notebook():\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        if 'IPKernelApp' not in get_ipython().config:  # pragma: no cover\n",
    "            return False\n",
    "    except ImportError:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fc91521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41a14888c52a4284a0a395677489826d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:400000\n"
     ]
    }
   ],
   "source": [
    "def load_word2emb(embedding_file):\n",
    "    word2embedding = dict()\n",
    "    word_dim = int(re.findall(r\".(\\d+)d\", embedding_file)[0])\n",
    "\n",
    "    with open(embedding_file, \"r\") as f:\n",
    "        for line in tqdm(f):\n",
    "            line = line.strip().split()\n",
    "            word = line[0]\n",
    "            embedding = list(map(float, line[1:]))\n",
    "            word2embedding[word] = np.array(embedding)\n",
    "\n",
    "    print(\"Number of words:%d\" % len(word2embedding))\n",
    "\n",
    "    return word2embedding\n",
    "\n",
    "word2embedding = load_word2emb(config[\"embedding_file\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47bfc175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_wordemb(word2embedding):\n",
    "    # Every word emb should have norm 1\n",
    "    \n",
    "    word_emb = []\n",
    "    word_list = []\n",
    "    for word, emb in word2embedding.items():\n",
    "        word_list.append(word)\n",
    "        word_emb.append(emb)\n",
    "\n",
    "    word_emb = np.array(word_emb)\n",
    "\n",
    "    for i in range(len(word_emb)):\n",
    "        norm = np.linalg.norm(word_emb[i])\n",
    "        word_emb[i] = word_emb[i] / norm\n",
    "\n",
    "    for word, emb in tqdm(zip(word_list, word_emb)):\n",
    "        word2embedding[word] = emb\n",
    "    return word2embedding\n",
    "\n",
    "if config[\"normalize_word_embedding\"]:\n",
    "    normalize_wordemb(word2embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1233053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, word2embedding, config):\n",
    "        # The low frequency words will be assigned as <UNK> token\n",
    "        self.itos = {0: \"<UNK>\"}\n",
    "        self.stoi = {\"<UNK>\": 0}\n",
    "        \n",
    "        self.word2embedding = word2embedding\n",
    "        self.config = config\n",
    "\n",
    "        self.word_freq_in_corpus = defaultdict(int)\n",
    "        self.IDF = {}\n",
    "        self.ps = PorterStemmer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        self.word_dim = len(word2embedding['the'])\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def tokenizer_eng(self, text):\n",
    "        text = re.sub(r'[^A-Za-z0-9 ]+', '', text)\n",
    "        text = text.strip().split()\n",
    "        \n",
    "        return [self.ps.stem(w) for w in text if w.lower() not in self.stop_words]\n",
    "    \n",
    "    def read_raw(self):        \n",
    "        if self.config[\"dataset\"] == 'IMDB':\n",
    "            data_file_path = '../data/IMDB.txt'\n",
    "        elif self.config[\"dataset\"] == 'CNN':\n",
    "            data_file_path = '../data/CNN.txt'\n",
    "        elif self.config[\"dataset\"] == 'PubMed':\n",
    "            data_file_path = '../data/PubMed.txt'\n",
    "        \n",
    "        # raw documents\n",
    "        self.raw_documents = []\n",
    "        with open(data_file_path,'r',encoding='utf-8') as f:\n",
    "            for line in tqdm(f, desc=\"Loading documents\"):\n",
    "                self.raw_documents.append(line.strip(\"\\n\"))\n",
    "                \n",
    "        return self.raw_documents\n",
    "    \n",
    "    def build_vocabulary(self):\n",
    "        sentence_list = self.raw_documents\n",
    "        \n",
    "        self.doc_freq = defaultdict(int) # # of document a word appear\n",
    "        self.document_num = len(sentence_list)\n",
    "        self.word_vectors = [[0]*self.word_dim] # unknown word emb\n",
    "        \n",
    "        for sentence in tqdm(sentence_list, desc=\"Preprocessing documents\"):\n",
    "            # for doc_freq\n",
    "            document_words = set()\n",
    "            \n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                # pass unknown word\n",
    "                if word not in self.word2embedding:\n",
    "                    continue\n",
    "                    \n",
    "                # calculate word freq\n",
    "                self.word_freq_in_corpus[word] += 1\n",
    "                document_words.add(word)\n",
    "                \n",
    "            for word in document_words:\n",
    "                self.doc_freq[word] += 1\n",
    "        \n",
    "        # calculate IDF\n",
    "        print('doc num', self.document_num)\n",
    "        for word, freq in self.doc_freq.items():\n",
    "            self.IDF[word] = math.log(self.document_num / (freq+1))\n",
    "        \n",
    "        # delete less freq words:\n",
    "        delete_words = []\n",
    "        for word, v in self.word_freq_in_corpus.items():\n",
    "            if v < self.config[\"min_word_freq_threshold\"]:\n",
    "                delete_words.append(word)     \n",
    "        for word in delete_words:\n",
    "            del self.IDF[word]    \n",
    "            del self.word_freq_in_corpus[word]    \n",
    "        \n",
    "        # delete too freq words\n",
    "        print('eliminate freq words')\n",
    "        IDF = [(word, freq) for word, freq in self.IDF.items()]\n",
    "        IDF.sort(key=lambda x: x[1])\n",
    "\n",
    "        for i in range(self.config[\"topk_word_freq_threshold\"]):\n",
    "            print(word)\n",
    "            word = IDF[i][0]\n",
    "            del self.IDF[word]\n",
    "            del self.word_freq_in_corpus[word]\n",
    "        \n",
    "        # construct word_vectors\n",
    "        idx = 1\n",
    "        for word in self.word_freq_in_corpus:\n",
    "            self.word_vectors.append(self.word2embedding[word])\n",
    "            self.stoi[word] = idx\n",
    "            self.itos[idx] = word\n",
    "            idx += 1\n",
    "            \n",
    "    def init_word_weight(self,sentence_list, agg):\n",
    "        if agg == 'mean':\n",
    "            self.word_weight = {word: 1 for word in self.IDF.keys()}\n",
    "        elif agg == 'IDF':\n",
    "            self.word_weight = self.IDF\n",
    "        elif agg == 'uniform':\n",
    "            self.word_weight = {word: np.random.uniform(low=0.0, high=1.0) for word in self.IDF.keys()}\n",
    "        elif agg == 'gaussian':\n",
    "            mu, sigma = 10, 1 # mean and standard deviation\n",
    "            self.word_weight = {word: np.random.normal(mu, sigma) for word in self.IDF.keys()}\n",
    "        elif agg == 'exponential':\n",
    "            self.word_weight = {word: np.random.exponential(scale=1.0) for word in self.IDF.keys()}\n",
    "        elif agg == 'pmi':\n",
    "            trigram_measures = BigramAssocMeasures()\n",
    "            self.word_weight = defaultdict(int)\n",
    "            corpus = []\n",
    "\n",
    "            for text in tqdm(sentence_list):\n",
    "                corpus.extend(text.split())\n",
    "\n",
    "            finder = BigramCollocationFinder.from_words(corpus)\n",
    "            for pmi_score in finder.score_ngrams(trigram_measures.pmi):\n",
    "                pair, score = pmi_score\n",
    "                self.word_weight[pair[0]] += score\n",
    "                self.word_weight[pair[1]] += score\n",
    "                \n",
    "    def calculate_document_vector(self):\n",
    "        # Return\n",
    "        # document_vectors: weighted sum of word emb\n",
    "        # document_answers_idx: doc to word index list\n",
    "        # document_answers_wsum: word weight summation, e.g. total TFIDF score of a doc\n",
    "        \n",
    "        document_vectors = [] \n",
    "        document_answers = []\n",
    "        document_answers_wsum = []\n",
    "        \n",
    "        sentence_list = self.raw_documents\n",
    "        agg = self.config[\"document_vector_agg_weight\"]\n",
    "        n_document = self.config[\"n_document\"]\n",
    "        select_topk_TFIDF = self.config[\"select_topk_TFIDF\"]\n",
    "        \n",
    "        self.init_word_weight(sentence_list, agg)\n",
    "        for sentence in tqdm(sentence_list[:min(n_document, len(sentence_list))], desc=\"calculate document vectors\"):\n",
    "            document_vector = np.zeros(len(self.word_vectors[0]))\n",
    "            select_words = []\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                # pass unknown word\n",
    "                if word not in self.stoi:\n",
    "                    continue\n",
    "                else:\n",
    "                    select_words.append(word)\n",
    "\n",
    "            # select topk TDIDF\n",
    "            if select_topk_TFIDF is not None:\n",
    "                doc_TFIDF = defaultdict(float)\n",
    "                for word in select_words:    \n",
    "                    doc_TFIDF[word] += self.IDF[word]\n",
    "\n",
    "                doc_TFIDF_l = [(word, TFIDF) for word, TFIDF in doc_TFIDF.items()]\n",
    "                doc_TFIDF_l.sort(key=lambda x:x[1], reverse=True)\n",
    "                \n",
    "                select_topk_words = set(list(map(lambda x:x[0], doc_TFIDF_l[:select_topk_TFIDF])))\n",
    "                select_words = [word for word in select_words if word in select_topk_words]\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            total_weight = 0\n",
    "            # aggregate to doc vectors\n",
    "            for word in select_words:\n",
    "                document_vector += np.array(self.word2embedding[word]) * self.word_weight[word]\n",
    "                total_weight += self.word_weight[word]\n",
    "                \n",
    "            if len(select_words) == 0:\n",
    "                print('error', sentence)\n",
    "                continue\n",
    "            else:\n",
    "                if self.config[\"document_vector_weight_normalize\"]:\n",
    "                    document_vector /= total_weight\n",
    "                    total_weight = 1\n",
    "            \n",
    "            document_vectors.append(document_vector)\n",
    "            document_answers.append(select_words)\n",
    "            document_answers_wsum.append(total_weight)\n",
    "        \n",
    "        # get answers\n",
    "        document_answers_idx = []    \n",
    "        for ans in document_answers:\n",
    "            ans_idx = []\n",
    "            for token in ans:\n",
    "                if token in self.stoi:\n",
    "                    ans_idx.append(self.stoi[token])                    \n",
    "            document_answers_idx.append(ans_idx)\n",
    "        \n",
    "        self.document_vectors = document_vectors\n",
    "        self.document_answers_idx = document_answers_idx\n",
    "        self.document_answers_wsum = document_answers_wsum\n",
    "        \n",
    "        return document_vectors, document_answers_idx, document_answers_wsum\n",
    "        \n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]\n",
    "    \n",
    "    def check_docemb(self):\n",
    "        word_vectors = np.array(self.word_vectors)\n",
    "        pred = np.zeros(word_vectors.shape[1])\n",
    "        cnt = 0\n",
    "\n",
    "        for word_idx in self.document_answers_idx[0]:\n",
    "            pred += word_vectors[word_idx] * self.word_weight[self.itos[word_idx]]\n",
    "            cnt += self.word_weight[self.itos[word_idx]]\n",
    "        \n",
    "        if self.config[\"document_vector_weight_normalize\"]:\n",
    "            pred /= cnt\n",
    "        assert np.sum(self.document_vectors[0]) - np.sum(pred) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "361a0b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1f5cd56bc94a1f9e1e953a394dd849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading documents: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a5ebe1233247aaa62d3aae21c85599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing documents:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc num 100000\n",
      "eliminate freq words\n",
      "hadrian\n",
      "film\n",
      "one\n",
      "like\n",
      "time\n",
      "make\n",
      "good\n",
      "see\n",
      "watch\n",
      "get\n",
      "even\n",
      "would\n",
      "well\n",
      "much\n",
      "look\n",
      "end\n",
      "act\n",
      "scene\n",
      "go\n",
      "also\n",
      "way\n",
      "great\n",
      "think\n",
      "dont\n",
      "first\n",
      "thing\n",
      "made\n",
      "bad\n",
      "love\n",
      "could\n",
      "play\n",
      "know\n",
      "say\n",
      "show\n",
      "seen\n",
      "plot\n",
      "seem\n",
      "come\n",
      "mani\n",
      "take\n",
      "want\n",
      "work\n",
      "never\n",
      "actor\n",
      "tri\n",
      "two\n",
      "best\n",
      "ever\n",
      "year\n",
      "give\n",
      "better\n",
      "life\n",
      "still\n",
      "find\n",
      "perform\n",
      "part\n",
      "use\n",
      "actual\n",
      "interest\n",
      "feel\n",
      "lot\n",
      "back\n",
      "man\n",
      "im\n",
      "director\n",
      "real\n",
      "cast\n",
      "doesnt\n",
      "though\n",
      "enjoy\n",
      "didnt\n",
      "noth\n",
      "start\n",
      "live\n",
      "cant\n",
      "point\n",
      "set\n",
      "guy\n",
      "role\n",
      "new\n",
      "turn\n",
      "thought\n",
      "old\n",
      "direct\n",
      "fact\n",
      "that\n",
      "quit\n",
      "star\n",
      "day\n",
      "wonder\n",
      "around\n",
      "happen\n",
      "got\n",
      "enough\n",
      "right\n",
      "effect\n",
      "world\n",
      "long\n",
      "music\n",
      "without\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ed521e146d446eb2b214050ce1fcd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "calculate document vectors:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_vocab(config, word2embedding):\n",
    "    # build vocabulary\n",
    "    vocab = Vocabulary(word2embedding, config)\n",
    "    vocab.read_raw()\n",
    "    vocab.build_vocabulary()\n",
    "    vocab_size = len(vocab)\n",
    "    # get doc emb\n",
    "    vocab.calculate_document_vector()\n",
    "    vocab.check_docemb()\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(config, word2embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7e6d657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish building dataset!\n",
      "Number of documents:100000\n",
      "Number of words:13976\n",
      "Average length of document: 65.79\n"
     ]
    }
   ],
   "source": [
    "print(\"Finish building dataset!\")\n",
    "print(f\"Number of documents:{len(vocab.raw_documents)}\")\n",
    "print(f\"Number of words:{len(vocab)}\")\n",
    "\n",
    "l = list(map(len, vocab.document_answers_idx))\n",
    "print(\"Average length of document:\", np.mean(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c161ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_vectors: (13976, 100)\n",
      "document_vectors (100, 100)\n",
      "document_answers_wsum (100, 1)\n"
     ]
    }
   ],
   "source": [
    "word_vectors = np.array(vocab.word_vectors)\n",
    "print(\"word_vectors:\", word_vectors.shape)\n",
    "\n",
    "document_vectors = np.array(vocab.document_vectors)\n",
    "print(\"document_vectors\", document_vectors.shape)\n",
    "\n",
    "document_answers_wsum = np.array(vocab.document_answers_wsum).reshape(-1, 1)\n",
    "print(\"document_answers_wsum\", document_answers_wsum.shape)\n",
    "\n",
    "# create weight_ans\n",
    "document_answers_idx = vocab.document_answers_idx\n",
    "\n",
    "# random shuffle\n",
    "shuffle_idx = list(range(len(document_vectors)))\n",
    "random.Random(seed).shuffle(shuffle_idx)\n",
    "\n",
    "document_vectors = document_vectors[shuffle_idx]\n",
    "document_answers_wsum = document_answers_wsum[shuffle_idx]\n",
    "document_answers_idx = [document_answers_idx[idx] for idx in shuffle_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe65f37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 13976)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd9778859ff2411d8024acec0b7b8e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# onthot_ans: word freq matrix\n",
    "# weight_ans: TFIDF matrix\n",
    "\n",
    "onehot_ans = np.zeros((len(document_answers_idx), word_vectors.shape[0]))\n",
    "weight_ans = np.zeros((len(document_answers_idx), word_vectors.shape[0]))\n",
    "print(weight_ans.shape)\n",
    "\n",
    "for i in tqdm(range(len(document_answers_idx))):\n",
    "    for word_idx in document_answers_idx[i]:\n",
    "        weight_ans[i, word_idx] += vocab.word_weight[vocab.itos[word_idx]]\n",
    "        onehot_ans[i, word_idx] += 1\n",
    "        \n",
    "    if config[\"document_vector_weight_normalize\"]:\n",
    "        weight_ans[i] /= np.sum(weight_ans[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bd3dd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "assert np.sum(document_vectors - np.dot(weight_ans, word_vectors) > 1e-10) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbca2e49",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a52979e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model',\n",
       " 'percision@10',\n",
       " 'percision@30',\n",
       " 'percision@50',\n",
       " 'recall@10',\n",
       " 'recall@30',\n",
       " 'recall@50',\n",
       " 'F1@10',\n",
       " 'F1@30',\n",
       " 'F1@50',\n",
       " 'ndcg@10',\n",
       " 'ndcg@30',\n",
       " 'ndcg@50',\n",
       " 'ndcg@all']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results = []\n",
    "select_columns = ['model']\n",
    "for topk in config[\"topk\"]:\n",
    "    select_columns.append('percision@{}'.format(topk))\n",
    "for topk in config[\"topk\"]:\n",
    "    select_columns.append('recall@{}'.format(topk))\n",
    "for topk in config[\"topk\"]:\n",
    "    select_columns.append('F1@{}'.format(topk))\n",
    "for topk in config[\"topk\"]:\n",
    "    select_columns.append('ndcg@{}'.format(topk))\n",
    "select_columns.append('ndcg@all')\n",
    "select_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87d5379",
   "metadata": {},
   "source": [
    "## setting training size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f9ddbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size_ratio = 1\n",
    "train_size = int(len(document_answers_idx) * train_size_ratio)\n",
    "train_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d31a21",
   "metadata": {},
   "source": [
    "## Top K freq word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c664900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "251dbcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ans = document_answers_idx[:train_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1efec49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('girl', 15618),\n",
       " ('origin', 15299),\n",
       " ('kill', 15068),\n",
       " ('us', 14729),\n",
       " ('action', 14559),\n",
       " ('horror', 14226),\n",
       " ('young', 14197),\n",
       " ('fan', 13417),\n",
       " ('bit', 13324),\n",
       " ('big', 13265)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = [(word, freq) for word, freq in vocab.word_freq_in_corpus.items()]\n",
    "word_freq.sort(key=lambda x:x[1], reverse=True)\n",
    "word_freq[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a04793e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc113172e63249dcbef644493a3b9944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 word\n",
      "percision 0.11399999999999999\n",
      "recall 0.023834712657783915\n",
      "F1 0.03942631272767297\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1dbbfd6f735465c840debeb16a2f0d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 30 word\n",
      "percision 0.09666666666666664\n",
      "recall 0.05891858643470809\n",
      "F1 0.0732134086209844\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8df4aaf6f644abbad6d3c839ee7df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 50 word\n",
      "percision 0.09140000000000001\n",
      "recall 0.09227283986795463\n",
      "F1 0.09183434600340698\n"
     ]
    }
   ],
   "source": [
    "def topk_word_evaluation(k=50):\n",
    "    topk_word = [word for (word, freq) in word_freq[:k]]\n",
    "\n",
    "    pr, re = [], []\n",
    "    for ans in tqdm(test_ans):\n",
    "        ans = set(ans)\n",
    "        ans = [vocab.itos[a] for a in ans]\n",
    "\n",
    "        hit = []\n",
    "        for word in ans:\n",
    "            if word in topk_word:\n",
    "                hit.append(word)\n",
    "\n",
    "        precision = len(hit) / k\n",
    "        recall = len(hit) / len(ans)\n",
    "        pr.append(precision)\n",
    "        re.append(recall)\n",
    "\n",
    "    pr = np.mean(pr)\n",
    "    re = np.mean(re)\n",
    "    f1 = 2 * pr * re / (pr + re) if (pr + re) != 0 else 0\n",
    "    print('top {} word'.format(k))\n",
    "    print('percision', np.mean(pr))\n",
    "    print('recall', np.mean(re))\n",
    "    print('F1', f1)\n",
    "    return f1\n",
    "\n",
    "\n",
    "for topk in config['topk']:\n",
    "    topk_results[\"F1@{}\".format(topk)] = topk_word_evaluation(k=topk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26308789",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def topk_word_evaluation_NDCG(k=50):\n",
    "    freq_word =[word for (word, freq) in word_freq]\n",
    "    freq_word_idx = [vocab.stoi[word] for word in freq_word if word in vocab.stoi]\n",
    "    \n",
    "    scores = np.zeros(len(vocab.word_vectors))\n",
    "    for rank, idx in enumerate(freq_word_idx):\n",
    "        scores[idx] = len(vocab.word_vectors) - rank\n",
    "    \n",
    "    NDCGs = []\n",
    "    \n",
    "    for ans in tqdm(test_ans):\n",
    "        weight_ans = np.zeros(len(vocab.word_vectors))\n",
    "        \n",
    "        for word_idx in ans:\n",
    "            if word_idx == 0:\n",
    "                continue\n",
    "            word = vocab.itos[word_idx]\n",
    "            weight_ans[word_idx] += vocab.IDF[word]\n",
    "\n",
    "        NDCG_score = ndcg_score(weight_ans.reshape(1,-1), scores.reshape(1,-1), k=k)\n",
    "        NDCGs.append(NDCG_score)\n",
    "\n",
    "    print('top {} NDCG:{}'.format(k, np.mean(NDCGs)))\n",
    "    \n",
    "    return np.mean(NDCGs)\n",
    "\n",
    "\n",
    "# for topk in config['topk']:\n",
    "#     topk_results[\"ndcg@{}\".format(topk)] = topk_word_evaluation_NDCG(k=topk)\n",
    "    \n",
    "# topk_results[\"ndcg@all\"] = topk_word_evaluation_NDCG(k=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a1cbb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_results[\"model\"] = \"topk\"\n",
    "final_results.append(pd.Series(topk_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731c7c3c",
   "metadata": {},
   "source": [
    "## Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a091701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "091f8b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n",
      "(100, 13976)\n",
      "(13976, 100)\n"
     ]
    }
   ],
   "source": [
    "print(document_vectors.shape)\n",
    "print(weight_ans.shape)\n",
    "print(word_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75933b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sklearn(pred, ans):\n",
    "    results = {}\n",
    "        \n",
    "    one_hot_ans = np.arange(ans.shape[0])[ans > 0]\n",
    "    \n",
    "    for topk in config[\"topk\"]:\n",
    "        one_hot_pred = np.argsort(pred)[-topk:]\n",
    "        hit = np.intersect1d(one_hot_pred, one_hot_ans)\n",
    "        percision = len(hit) / topk\n",
    "        recall = len(hit) / len(one_hot_ans)\n",
    "        f1 = 2 * percision * recall / (percision + recall) if (percision + recall) > 0 else 0\n",
    "        \n",
    "        results['percision@{}'.format(topk)] = percision\n",
    "        results['recall@{}'.format(topk)] = recall\n",
    "        results['F1@{}'.format(topk)] = f1\n",
    "        \n",
    "    ans = ans.reshape(1, -1)\n",
    "    pred = pred.reshape(1, -1)\n",
    "    for topk in config[\"topk\"]:\n",
    "        results['ndcg@{}'.format(topk)] = ndcg_score(ans, pred, k=topk)\n",
    "\n",
    "    results['ndcg@all'] = (ndcg_score(ans, pred, k=None))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbdc9ed",
   "metadata": {},
   "source": [
    "### linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4e09dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3149232df1fa42a394e996d3bce601e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for doc_id, doc_emb in enumerate(tqdm(document_vectors[:train_size])):\n",
    "    x = word_vectors.T\n",
    "    y = doc_emb\n",
    "    \n",
    "    ans = weight_ans[doc_id]\n",
    "    model = LinearRegression(fit_intercept=False).fit(x, y)\n",
    "    r2 = model.score(x, y)\n",
    "\n",
    "    res = evaluate_sklearn(model.coef_, ans)\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18befbbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "percision@10                   0.341\n",
       "recall@10                   0.100978\n",
       "F1@10                       0.140458\n",
       "percision@30                   0.213\n",
       "recall@30                   0.171015\n",
       "F1@30                       0.167621\n",
       "percision@50                  0.1674\n",
       "recall@50                   0.219625\n",
       "F1@50                       0.168138\n",
       "ndcg@10                     0.363496\n",
       "ndcg@30                     0.327512\n",
       "ndcg@50                     0.334415\n",
       "ndcg@all                    0.563008\n",
       "model           sk-linear-regression\n",
       "dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(results).mean()\n",
    "results['model'] = 'sk-linear-regression'\n",
    "final_results.append(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafc846b",
   "metadata": {},
   "source": [
    "### lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f42cf137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cdeb975b71b48e581c075ad94c234de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.977472597772946e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8655044740415708e-06, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.68099291680852e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.395983694921603e-06, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.991707259394473e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0359714824637566e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.623936267889343e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.2978982797988875e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.372652579803796e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.270528832591805e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.552318276054558e-07, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.7394975636485936e-06, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.079033177381897e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.631986316140204e-06, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.472325668237364e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.9941662795890103e-15, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.288120346636291e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.1727365534019344e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.620452292834439e-06, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.511934935388473e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.00016419750921018242, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.463861343531142e-06, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.565059824182933e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.963073993539846e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.532867930144677e-06, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.220248526039418e-07, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.217157677742598e-06, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.0265131734021257e-07, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.969333522429457e-16, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.150634926150489e-05, tolerance: 0.0\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.517016683056296e-06, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.272939879969694e-06, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8958868641364584e-06, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.402619984568971e-06, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.289360565354591e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.871493955788833e-07, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.9310475305683317e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.578897803795724e-16, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.955834407934268e-06, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.00013057598244238842, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.2081810121740708e-16, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1591188485383583e-15, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3755870791522161e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.498173491070229e-07, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.548307593387404e-06, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0225939481002397e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.978736453807027e-06, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.173982348651138e-06, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.2070366662896983e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.272895891229087e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.9932106758904897e-07, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7987124184981135e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.91966607636346e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.246247439883753e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.306357432951895e-07, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.7376615280877256e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.00189439711068e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0201484237973009e-06, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.758154833080626e-07, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.83618944329074e-05, tolerance: 0.0\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.101695660069102e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.126813887982976e-16, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.084868745757815e-06, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.2416362942265784e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.7805472398524292e-06, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.00014451165987822834, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.472898249451028e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.3673793487498135e-06, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.492122607568125e-06, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.749837687763743e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.490025905076385e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.216354183038416e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.196156233282478e-07, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.420298799866801e-06, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0486349964924994e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8039707283655195e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.043541135078852e-06, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.382302212201212e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.177088210943655e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.814385965983751e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.2496708420597e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.0951024944842913e-06, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.00010003689693607426, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.002525508974434e-07, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.286478387865665e-06, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.861334688988359e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.260679921495102e-06, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.397542543486037e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.4143153024396123e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.426957813422159e-06, tolerance: 0.0\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.21976277699759e-08, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.6163559101762675e-06, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.135439443240553e-06, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.6276568727935992e-06, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.240368264169734e-16, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.571661024149927e-06, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.191816549507432e-07, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.720828949077328e-05, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1617607976266371e-06, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.743706537329922e-06, tolerance: 0.0\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "sk_lasso_epoch = 10000\n",
    "\n",
    "for doc_id, doc_emb in enumerate(tqdm(document_vectors[:train_size])):\n",
    "    x = word_vectors.T\n",
    "    y = doc_emb\n",
    "    \n",
    "    ans = weight_ans[doc_id]\n",
    "    model = Lasso(positive=True, fit_intercept=False, alpha=0.0001, max_iter=sk_lasso_epoch, tol=0).fit(x, y)\n",
    "    r2 = model.score(x, y)\n",
    "\n",
    "    res = evaluate_sklearn(model.coef_, ans)\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07772202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "percision@10        0.53\n",
       "recall@10       0.200465\n",
       "F1@10           0.262543\n",
       "percision@30    0.279667\n",
       "recall@30       0.280765\n",
       "F1@30           0.247872\n",
       "percision@50       0.204\n",
       "recall@50       0.319288\n",
       "F1@50           0.221158\n",
       "ndcg@10         0.540316\n",
       "ndcg@30         0.468284\n",
       "ndcg@50         0.465313\n",
       "ndcg@all        0.621234\n",
       "model           sk-lasso\n",
       "dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(results).mean()\n",
    "results['model'] = 'sk-lasso'\n",
    "final_results.append(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4fcb3a",
   "metadata": {},
   "source": [
    "## Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f4f53f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "854f9586",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Lasso_Dataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 doc_vectors,\n",
    "                 doc_w_sum,\n",
    "                 weight_ans\n",
    "                 ):\n",
    "        self.doc_vectors = torch.FloatTensor(doc_vectors)\n",
    "        self.doc_w_sum = torch.FloatTensor(doc_w_sum)\n",
    "        self.weight_ans = weight_ans\n",
    "        assert len(doc_vectors) == len(doc_w_sum)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "                \n",
    "        return self.doc_vectors[idx], self.doc_w_sum[idx], idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doc_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0aa07e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR(nn.Module):\n",
    "    \"\"\"\n",
    "    Input shape: (N, 3, 64, 64)\n",
    "    Output shape: (N, )\n",
    "    \"\"\"\n",
    "    def __init__(self, num_doc, num_words):\n",
    "        super(LR, self).__init__()\n",
    "        weight = torch.zeros(num_doc, num_words).to(device)\n",
    "        self.emb = torch.nn.Embedding.from_pretrained(weight, freeze=False)\n",
    "        \n",
    "    def forward(self, doc_ids, word_vectors):\n",
    "        return self.emb(doc_ids) @ word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c135bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_Custom_Lasso(model, train_loader):\n",
    "    results = {}\n",
    "    model.eval()\n",
    "    \n",
    "    scores = np.array(model.emb.cpu().weight.data)\n",
    "    model.emb.to(device)\n",
    "    true_relevance = train_loader.dataset.weight_ans\n",
    "\n",
    "    # F1\n",
    "    F1s = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for i in range(true_relevance.shape[0]):\n",
    "        one_hot_ans = np.arange(true_relevance.shape[1])[true_relevance[i] > 0]\n",
    "        pred = scores[i]\n",
    "        \n",
    "        F1_ = []\n",
    "        percision_ = []\n",
    "        recall_ = []\n",
    "        for topk in config[\"topk\"]:\n",
    "            one_hot_pred = np.argsort(pred)[-topk:]\n",
    "            \n",
    "            hit = np.intersect1d(one_hot_pred, one_hot_ans)\n",
    "            percision = len(hit) / topk\n",
    "            recall = len(hit) / len(one_hot_ans)\n",
    "            \n",
    "            F1 = 2 * percision * recall / (percision + recall) if (percision + recall) > 0 else 0\n",
    "            F1_.append(F1)\n",
    "            percision_.append(percision)\n",
    "            recall_.append(recall)\n",
    "            \n",
    "        F1s.append(F1_)\n",
    "        precisions.append(percision_)\n",
    "        recalls.append(recall_)\n",
    "        \n",
    "    F1s = np.mean(F1s, axis=0)\n",
    "    precisions = np.mean(precisions, axis=0)\n",
    "    recalls = np.mean(recalls, axis=0)\n",
    "    \n",
    "    for i, topk in enumerate(config[\"topk\"]):\n",
    "        results['F1@{}'.format(topk)] = F1s[i]\n",
    "        results['percision@{}'.format(topk)] = precisions[i]\n",
    "        results['recall@{}'.format(topk)] = recalls[i]\n",
    "\n",
    "    # NDCG\n",
    "    for topk in config[\"topk\"]:\n",
    "        results['ndcg@{}'.format(topk)] = ndcg_score(true_relevance, scores, k=topk)\n",
    "    results['ndcg@all'] = ndcg_score(true_relevance, scores, k=None)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ac91f68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document num 100\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "print('document num', train_size)\n",
    "\n",
    "train_dataset = Custom_Lasso_Dataset(document_vectors[:train_size], document_answers_wsum[:train_size], weight_ans[:train_size])\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e533d31",
   "metadata": {},
   "source": [
    "## start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b3f76e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0993200eb6504f8dbe38ea9272dd9441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 0\n",
      "loss_mse 0.05655769631266594\n",
      "loss_w_reg 1.0\n",
      "F1@10 0.022739436255302586\n",
      "percision@10 0.051000000000000004\n",
      "recall@10 0.01664564410447777\n",
      "F1@30 0.047511352112411614\n",
      "percision@30 0.060333333333333294\n",
      "recall@30 0.04970928685243355\n",
      "F1@50 0.07689679351510892\n",
      "percision@50 0.07739999999999995\n",
      "recall@50 0.0994594691733635\n",
      "ndcg@10 0.04947000370588328\n",
      "ndcg@30 0.057901463913454514\n",
      "ndcg@50 0.07767041814788288\n",
      "ndcg@all 0.3566580724768421\n",
      "\n",
      "epoch 100\n",
      "loss_mse 8.220216841436923e-05\n",
      "loss_w_reg 0.0026517000515013933\n",
      "F1@10 0.19868552855822608\n",
      "percision@10 0.4660000000000001\n",
      "recall@10 0.14327672144879688\n",
      "F1@30 0.2224742250524924\n",
      "percision@30 0.2743333333333332\n",
      "recall@30 0.2312151506064769\n",
      "F1@50 0.21302662396518135\n",
      "percision@50 0.20819999999999989\n",
      "recall@50 0.28153576639018973\n",
      "ndcg@10 0.5214077218378206\n",
      "ndcg@30 0.4531714292296421\n",
      "ndcg@50 0.45057845844363764\n",
      "ndcg@all 0.6441837809264502\n",
      "\n",
      "epoch 200\n",
      "loss_mse 1.6649612007313408e-05\n",
      "loss_w_reg 0.002097564982250333\n",
      "F1@10 0.21961618408486486\n",
      "percision@10 0.495\n",
      "recall@10 0.16090537957030826\n",
      "F1@30 0.23715095861847604\n",
      "percision@30 0.28633333333333333\n",
      "recall@30 0.252110656416815\n",
      "F1@50 0.22553627583256586\n",
      "percision@50 0.21759999999999993\n",
      "recall@50 0.3034930384838954\n",
      "ndcg@10 0.5418942543506435\n",
      "ndcg@30 0.47142737829545467\n",
      "ndcg@50 0.4697077350654304\n",
      "ndcg@all 0.6534000041924277\n",
      "\n",
      "epoch 300\n",
      "loss_mse 8.19959313957952e-06\n",
      "loss_w_reg 0.0020556312520056963\n",
      "F1@10 0.2299540046400044\n",
      "percision@10 0.511\n",
      "recall@10 0.1696155610959603\n",
      "F1@30 0.2447202694811861\n",
      "percision@30 0.2913333333333333\n",
      "recall@30 0.26483524711336015\n",
      "F1@50 0.2317266830889556\n",
      "percision@50 0.22279999999999991\n",
      "recall@50 0.31591133369585844\n",
      "ndcg@10 0.5490705302307081\n",
      "ndcg@30 0.47881945432561446\n",
      "ndcg@50 0.47791263512247945\n",
      "ndcg@all 0.6540565812403394\n",
      "\n",
      "epoch 400\n",
      "loss_mse 4.486861143959686e-06\n",
      "loss_w_reg 0.0020499073434621096\n",
      "F1@10 0.23751629254975998\n",
      "percision@10 0.5210000000000002\n",
      "recall@10 0.17613647094453844\n",
      "F1@30 0.247525559839556\n",
      "percision@30 0.2936666666666666\n",
      "recall@30 0.2690385167829922\n",
      "F1@50 0.23457884484348\n",
      "percision@50 0.22439999999999988\n",
      "recall@50 0.3216118938547001\n",
      "ndcg@10 0.5572013274963682\n",
      "ndcg@30 0.4848801102971494\n",
      "ndcg@50 0.48438403251685536\n",
      "ndcg@all 0.6557649673462771\n",
      "\n",
      "epoch 500\n",
      "loss_mse 2.704610096770921e-06\n",
      "loss_w_reg 0.002077849116176367\n",
      "F1@10 0.2482064420631294\n",
      "percision@10 0.533\n",
      "recall@10 0.1862375109355533\n",
      "F1@30 0.25268392230830655\n",
      "percision@30 0.2993333333333334\n",
      "recall@30 0.27487897091060415\n",
      "F1@50 0.2364450686518077\n",
      "percision@50 0.22619999999999993\n",
      "recall@50 0.32538527703765313\n",
      "ndcg@10 0.5649451658451876\n",
      "ndcg@30 0.4906525751435279\n",
      "ndcg@50 0.48834157649378057\n",
      "ndcg@all 0.6577670031951993\n",
      "\n",
      "epoch 600\n",
      "loss_mse 2.8248539365449687e-06\n",
      "loss_w_reg 0.00209824088960886\n",
      "F1@10 0.2550200790876703\n",
      "percision@10 0.544\n",
      "recall@10 0.19164331136069332\n",
      "F1@30 0.2567261376867099\n",
      "percision@30 0.30300000000000005\n",
      "recall@30 0.28056100865432115\n",
      "F1@50 0.23600509484375043\n",
      "percision@50 0.2255999999999999\n",
      "recall@50 0.3257820211757416\n",
      "ndcg@10 0.5720925857549071\n",
      "ndcg@30 0.49590095760697717\n",
      "ndcg@50 0.4919533058016652\n",
      "ndcg@all 0.6605376510502614\n",
      "\n",
      "epoch 700\n",
      "loss_mse 1.7963103573492845e-06\n",
      "loss_w_reg 0.002115246606990695\n",
      "F1@10 0.259311840992744\n",
      "percision@10 0.551\n",
      "recall@10 0.19517666386390217\n",
      "F1@30 0.2607087459947291\n",
      "percision@30 0.3066666666666667\n",
      "recall@30 0.2853255951367531\n",
      "F1@50 0.2377221721215088\n",
      "percision@50 0.22699999999999998\n",
      "recall@50 0.32904357250268595\n",
      "ndcg@10 0.5751365224549864\n",
      "ndcg@30 0.49931604348792746\n",
      "ndcg@50 0.49419093112346635\n",
      "ndcg@all 0.6620252185290435\n",
      "\n",
      "epoch 800\n",
      "loss_mse 1.4510067103401525e-06\n",
      "loss_w_reg 0.0021300967782735825\n",
      "F1@10 0.2645609584521652\n",
      "percision@10 0.5579999999999999\n",
      "recall@10 0.19945815150004362\n",
      "F1@30 0.26336061975043507\n",
      "percision@30 0.30866666666666676\n",
      "recall@30 0.28918363540535874\n",
      "F1@50 0.2378204515347395\n",
      "percision@50 0.22699999999999995\n",
      "recall@50 0.32939924809850557\n",
      "ndcg@10 0.5788820407432306\n",
      "ndcg@30 0.5010318485321664\n",
      "ndcg@50 0.4946662026403959\n",
      "ndcg@all 0.6628096109553472\n",
      "\n",
      "epoch 900\n",
      "loss_mse 1.1944329116886365e-06\n",
      "loss_w_reg 0.002141522476449609\n",
      "F1@10 0.2698330806954486\n",
      "percision@10 0.564\n",
      "recall@10 0.2038570238492265\n",
      "F1@30 0.2642809575772233\n",
      "percision@30 0.30933333333333335\n",
      "recall@30 0.2904942712819828\n",
      "F1@50 0.23877568649213918\n",
      "percision@50 0.22759999999999994\n",
      "recall@50 0.3313164361732622\n",
      "ndcg@10 0.5818375517547651\n",
      "ndcg@30 0.5020526249460587\n",
      "ndcg@50 0.49587106218732674\n",
      "ndcg@all 0.6646808177820216\n",
      "\n",
      "epoch 1000\n",
      "loss_mse 9.541553254166502e-07\n",
      "loss_w_reg 0.002152470638975501\n",
      "F1@10 0.27115845881892037\n",
      "percision@10 0.565\n",
      "recall@10 0.20518830727784487\n",
      "F1@30 0.2654456824779101\n",
      "percision@30 0.31066666666666676\n",
      "recall@30 0.2920207384761293\n",
      "F1@50 0.2388487826743672\n",
      "percision@50 0.22719999999999993\n",
      "recall@50 0.3318363462171179\n",
      "ndcg@10 0.5835384423856919\n",
      "ndcg@30 0.503598398511234\n",
      "ndcg@50 0.49720794429858367\n",
      "ndcg@all 0.6664184519564286\n",
      "\n",
      "epoch 1100\n",
      "loss_mse 8.00096756847779e-07\n",
      "loss_w_reg 0.002161696320399642\n",
      "F1@10 0.2739113565121739\n",
      "percision@10 0.569\n",
      "recall@10 0.2072609698600524\n",
      "F1@30 0.2658747580698956\n",
      "percision@30 0.31100000000000005\n",
      "recall@30 0.2926589421446935\n",
      "F1@50 0.23955164416344168\n",
      "percision@50 0.2273999999999999\n",
      "recall@50 0.3337738178980544\n",
      "ndcg@10 0.5862473921251303\n",
      "ndcg@30 0.5039812440464769\n",
      "ndcg@50 0.4977762168416989\n",
      "ndcg@all 0.6678557587133516\n",
      "\n",
      "epoch 1200\n",
      "loss_mse 7.34483762698801e-07\n",
      "loss_w_reg 0.0021699138451367617\n",
      "F1@10 0.2744050908480135\n",
      "percision@10 0.5699999999999998\n",
      "recall@10 0.20762929137305478\n",
      "F1@30 0.2674029968270115\n",
      "percision@30 0.31200000000000006\n",
      "recall@30 0.2947482719878803\n",
      "F1@50 0.23946907552384733\n",
      "percision@50 0.22699999999999995\n",
      "recall@50 0.3339979488254369\n",
      "ndcg@10 0.5857639767594667\n",
      "ndcg@30 0.5043790813168019\n",
      "ndcg@50 0.49753511940834516\n",
      "ndcg@all 0.6683383183541608\n",
      "\n",
      "epoch 1300\n",
      "loss_mse 6.402593157872616e-07\n",
      "loss_w_reg 0.0021725536789745092\n",
      "F1@10 0.2734579977880614\n",
      "percision@10 0.5689999999999998\n",
      "recall@10 0.20692647012162485\n",
      "F1@30 0.2690930273585235\n",
      "percision@30 0.3143333333333333\n",
      "recall@30 0.2962889914243438\n",
      "F1@50 0.23856915302684198\n",
      "percision@50 0.22640000000000002\n",
      "recall@50 0.3327210667814917\n",
      "ndcg@10 0.5852822631408866\n",
      "ndcg@30 0.5058082778823505\n",
      "ndcg@50 0.49730242511003\n",
      "ndcg@all 0.6693557505309842\n",
      "\n",
      "epoch 1400\n",
      "loss_mse 6.08563425430475e-07\n",
      "loss_w_reg 0.002178858732804656\n",
      "F1@10 0.27371848582444075\n",
      "percision@10 0.5689999999999998\n",
      "recall@10 0.2072398566942026\n",
      "F1@30 0.2703894987014376\n",
      "percision@30 0.31633333333333336\n",
      "recall@30 0.2975890967619222\n",
      "F1@50 0.2388343692920582\n",
      "percision@50 0.22639999999999996\n",
      "recall@50 0.33354748623816327\n",
      "ndcg@10 0.5853022261274298\n",
      "ndcg@30 0.5066897199394179\n",
      "ndcg@50 0.4975353340136873\n",
      "ndcg@all 0.6709932847422013\n",
      "\n",
      "epoch 1500\n",
      "loss_mse 6.140008395050245e-07\n",
      "loss_w_reg 0.002185872057452798\n",
      "F1@10 0.27272616964556584\n",
      "percision@10 0.5659999999999998\n",
      "recall@10 0.2066018033942446\n",
      "F1@30 0.2707874832102468\n",
      "percision@30 0.31666666666666665\n",
      "recall@30 0.29803306594041995\n",
      "F1@50 0.2382760252353841\n",
      "percision@50 0.22599999999999998\n",
      "recall@50 0.33291744858920225\n",
      "ndcg@10 0.5837494623152621\n",
      "ndcg@30 0.5064690107152964\n",
      "ndcg@50 0.497149191087762\n",
      "ndcg@all 0.6711392044054121\n",
      "\n",
      "epoch 1600\n",
      "loss_mse 6.518249620057759e-07\n",
      "loss_w_reg 0.0021926185581833124\n",
      "F1@10 0.27362069213919704\n",
      "percision@10 0.567\n",
      "recall@10 0.20721852055507942\n",
      "F1@30 0.27134633771735844\n",
      "percision@30 0.3176666666666666\n",
      "recall@30 0.2984190744495772\n",
      "F1@50 0.2388235842822967\n",
      "percision@50 0.22639999999999996\n",
      "recall@50 0.3342185173713837\n",
      "ndcg@10 0.5844808825532831\n",
      "ndcg@30 0.5069404623303571\n",
      "ndcg@50 0.4977111590381355\n",
      "ndcg@all 0.6725213110148021\n",
      "\n",
      "epoch 1700\n",
      "loss_mse 6.51600316814438e-07\n",
      "loss_w_reg 0.0021938385907560587\n",
      "F1@10 0.2741470079286707\n",
      "percision@10 0.568\n",
      "recall@10 0.20757566341222225\n",
      "F1@30 0.2712003784968598\n",
      "percision@30 0.31733333333333336\n",
      "recall@30 0.2983054957464439\n",
      "F1@50 0.23804351908985794\n",
      "percision@50 0.22579999999999997\n",
      "recall@50 0.3332706005943266\n",
      "ndcg@10 0.586458467037234\n",
      "ndcg@30 0.5087008120392154\n",
      "ndcg@50 0.49885232125128903\n",
      "ndcg@all 0.6749049054164702\n",
      "\n",
      "epoch 1800\n",
      "loss_mse 6.133332703939232e-07\n",
      "loss_w_reg 0.0021990379318594933\n",
      "F1@10 0.27479216921899324\n",
      "percision@10 0.569\n",
      "recall@10 0.20805185388841274\n",
      "F1@30 0.2719742927612531\n",
      "percision@30 0.31766666666666665\n",
      "recall@30 0.2998054466627484\n",
      "F1@50 0.2391603137478868\n",
      "percision@50 0.22679999999999997\n",
      "recall@50 0.33447532226180526\n",
      "ndcg@10 0.587115042941796\n",
      "ndcg@30 0.5094214798177671\n",
      "ndcg@50 0.4995909390148095\n",
      "ndcg@all 0.6757435571295134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 1900\n",
      "loss_mse 5.879484206161578e-07\n",
      "loss_w_reg 0.0022000744938850403\n",
      "F1@10 0.2738658534295196\n",
      "percision@10 0.5670000000000001\n",
      "recall@10 0.2074447110312699\n",
      "F1@30 0.27170016234402583\n",
      "percision@30 0.3176666666666666\n",
      "recall@30 0.2993156266349564\n",
      "F1@50 0.23958560158545533\n",
      "percision@50 0.22699999999999998\n",
      "recall@50 0.3351657998126924\n",
      "ndcg@10 0.5851428672103336\n",
      "ndcg@30 0.508767677234998\n",
      "ndcg@50 0.4992563334501472\n",
      "ndcg@all 0.6755133929877316\n",
      "\n",
      "epoch 2000\n",
      "loss_mse 5.737636001867941e-07\n",
      "loss_w_reg 0.0022020807955414057\n",
      "F1@10 0.2742939298324945\n",
      "percision@10 0.568\n",
      "recall@10 0.20771055291742024\n",
      "F1@30 0.27133166240825485\n",
      "percision@30 0.31733333333333325\n",
      "recall@30 0.2989514370842617\n",
      "F1@50 0.2393424785349686\n",
      "percision@50 0.2268\n",
      "recall@50 0.33469437691644016\n",
      "ndcg@10 0.5841076293244825\n",
      "ndcg@30 0.5082131932714128\n",
      "ndcg@50 0.4987648016781867\n",
      "ndcg@all 0.6758491735465281\n",
      "\n",
      "epoch 2100\n",
      "loss_mse 5.477900231198873e-07\n",
      "loss_w_reg 0.002204054733738303\n",
      "F1@10 0.2738771701779053\n",
      "percision@10 0.5660000000000001\n",
      "recall@10 0.20755639355265904\n",
      "F1@30 0.27230628383039734\n",
      "percision@30 0.31833333333333325\n",
      "recall@30 0.29991295157723025\n",
      "F1@50 0.23953457587208246\n",
      "percision@50 0.22700000000000004\n",
      "recall@50 0.33486411064880833\n",
      "ndcg@10 0.5835179862304418\n",
      "ndcg@30 0.5090681461048022\n",
      "ndcg@50 0.4989969554482887\n",
      "ndcg@all 0.6762592238515127\n",
      "\n",
      "epoch 2200\n",
      "loss_mse 5.394115873968985e-07\n",
      "loss_w_reg 0.002205235417932272\n",
      "F1@10 0.273693226868962\n",
      "percision@10 0.564\n",
      "recall@10 0.20751417229838925\n",
      "F1@30 0.2724171358258788\n",
      "percision@30 0.31833333333333325\n",
      "recall@30 0.30006540403000526\n",
      "F1@50 0.23903970937098795\n",
      "percision@50 0.22660000000000002\n",
      "recall@50 0.33411421856558293\n",
      "ndcg@10 0.5824479562278058\n",
      "ndcg@30 0.5094279532481131\n",
      "ndcg@50 0.4988862869843168\n",
      "ndcg@all 0.67640844104462\n",
      "\n",
      "epoch 2300\n",
      "loss_mse 5.173450858819706e-07\n",
      "loss_w_reg 0.0022058305330574512\n",
      "F1@10 0.27413262914774245\n",
      "percision@10 0.565\n",
      "recall@10 0.2078299757016244\n",
      "F1@30 0.27313389491662915\n",
      "percision@30 0.3189999999999999\n",
      "recall@30 0.3008736964732402\n",
      "F1@50 0.23854701844704973\n",
      "percision@50 0.2262\n",
      "recall@50 0.33350375167182394\n",
      "ndcg@10 0.5823169594942574\n",
      "ndcg@30 0.5095597366067469\n",
      "ndcg@50 0.4983217172655231\n",
      "ndcg@all 0.6762530490321517\n",
      "\n",
      "epoch 2400\n",
      "loss_mse 4.924631298308668e-07\n",
      "loss_w_reg 0.0022083332296460867\n",
      "F1@10 0.2745887694986197\n",
      "percision@10 0.566\n",
      "recall@10 0.20814652026816893\n",
      "F1@30 0.2735026821096557\n",
      "percision@30 0.3196666666666666\n",
      "recall@30 0.3011304770863039\n",
      "F1@50 0.23887085769693112\n",
      "percision@50 0.22640000000000005\n",
      "recall@50 0.3338606783786497\n",
      "ndcg@10 0.5822734929324307\n",
      "ndcg@30 0.5096523204457462\n",
      "ndcg@50 0.49848527902178175\n",
      "ndcg@all 0.6763916553992854\n",
      "\n",
      "epoch 2500\n",
      "loss_mse 4.737288463729783e-07\n",
      "loss_w_reg 0.002207521814852953\n",
      "F1@10 0.27365195032650647\n",
      "percision@10 0.564\n",
      "recall@10 0.2074495893218774\n",
      "F1@30 0.2746183923970209\n",
      "percision@30 0.32066666666666654\n",
      "recall@30 0.30250450430251186\n",
      "F1@50 0.23865306464075886\n",
      "percision@50 0.22580000000000006\n",
      "recall@50 0.33419822662526627\n",
      "ndcg@10 0.5814423898567364\n",
      "ndcg@30 0.5101822002759248\n",
      "ndcg@50 0.498444795397959\n",
      "ndcg@all 0.6768306546515541\n",
      "\n",
      "epoch 2600\n",
      "loss_mse 4.737326833037514e-07\n",
      "loss_w_reg 0.0022111537400633097\n",
      "F1@10 0.2741856401660786\n",
      "percision@10 0.565\n",
      "recall@10 0.20782213834148522\n",
      "F1@30 0.27518435648757517\n",
      "percision@30 0.32133333333333325\n",
      "recall@30 0.30301227013285836\n",
      "F1@50 0.23956309979951373\n",
      "percision@50 0.22660000000000002\n",
      "recall@50 0.33543052300228665\n",
      "ndcg@10 0.5814206030528362\n",
      "ndcg@30 0.5102585465589374\n",
      "ndcg@50 0.49890106338190826\n",
      "ndcg@all 0.6771223388816382\n",
      "\n",
      "epoch 2700\n",
      "loss_mse 4.805086177839257e-07\n",
      "loss_w_reg 0.002212350955232978\n",
      "F1@10 0.2746627717301667\n",
      "percision@10 0.567\n",
      "recall@10 0.2080933432923407\n",
      "F1@30 0.27539427691954305\n",
      "percision@30 0.32166666666666655\n",
      "recall@30 0.30320898887305536\n",
      "F1@50 0.23897112163256792\n",
      "percision@50 0.22600000000000006\n",
      "recall@50 0.3347481310215075\n",
      "ndcg@10 0.5818304180872138\n",
      "ndcg@30 0.5105097761889025\n",
      "ndcg@50 0.4984896704248424\n",
      "ndcg@all 0.6779921860851644\n",
      "\n",
      "epoch 2800\n",
      "loss_mse 4.7534535951854195e-07\n",
      "loss_w_reg 0.0022103004157543182\n",
      "F1@10 0.27521832728572226\n",
      "percision@10 0.568\n",
      "recall@10 0.2084779586769561\n",
      "F1@30 0.27460079960147554\n",
      "percision@30 0.32066666666666666\n",
      "recall@30 0.3024141860056719\n",
      "F1@50 0.2389739015882123\n",
      "percision@50 0.2260000000000001\n",
      "recall@50 0.33486972216125727\n",
      "ndcg@10 0.5813324781248803\n",
      "ndcg@30 0.5097413450276356\n",
      "ndcg@50 0.4981617494748811\n",
      "ndcg@all 0.6775732486039255\n",
      "\n",
      "epoch 2900\n",
      "loss_mse 4.6790898977633333e-07\n",
      "loss_w_reg 0.0022113672457635403\n",
      "F1@10 0.2757897558571508\n",
      "percision@10 0.569\n",
      "recall@10 0.20887795867695608\n",
      "F1@30 0.27480939720542197\n",
      "percision@30 0.32066666666666654\n",
      "recall@30 0.3027131759046618\n",
      "F1@50 0.2389739015882123\n",
      "percision@50 0.2260000000000001\n",
      "recall@50 0.33486972216125727\n",
      "ndcg@10 0.5818372873038454\n",
      "ndcg@30 0.5099044697739172\n",
      "ndcg@50 0.4983188506747372\n",
      "ndcg@all 0.6775691307086474\n",
      "\n",
      "epoch 3000\n",
      "loss_mse 4.671751412388403e-07\n",
      "loss_w_reg 0.002213398227468133\n",
      "F1@10 0.2761123365023121\n",
      "percision@10 0.57\n",
      "recall@10 0.20907026636926374\n",
      "F1@30 0.27500224350907615\n",
      "percision@30 0.32066666666666654\n",
      "recall@30 0.30297486463505446\n",
      "F1@50 0.23893195402266926\n",
      "percision@50 0.22600000000000006\n",
      "recall@50 0.3347261324176676\n",
      "ndcg@10 0.5809070956672515\n",
      "ndcg@30 0.5096507689803685\n",
      "ndcg@50 0.49760657464602126\n",
      "ndcg@all 0.6775816300562036\n",
      "\n",
      "epoch 3100\n",
      "loss_mse 4.6080339188847574e-07\n",
      "loss_w_reg 0.002212757943198085\n",
      "F1@10 0.2761123365023121\n",
      "percision@10 0.57\n",
      "recall@10 0.20907026636926374\n",
      "F1@30 0.27536900512798906\n",
      "percision@30 0.32066666666666654\n",
      "recall@30 0.3034085371910536\n",
      "F1@50 0.2393591737353986\n",
      "percision@50 0.22640000000000002\n",
      "recall@50 0.33518904801869065\n",
      "ndcg@10 0.5806871654132536\n",
      "ndcg@30 0.5094741108631227\n",
      "ndcg@50 0.49774116641759714\n",
      "ndcg@all 0.6775220852269996\n",
      "\n",
      "epoch 3200\n",
      "loss_mse 4.648329081646807e-07\n",
      "loss_w_reg 0.0022151798475533724\n",
      "F1@10 0.2761123365023121\n",
      "percision@10 0.57\n",
      "recall@10 0.20907026636926374\n",
      "F1@30 0.27492257655656055\n",
      "percision@30 0.31999999999999984\n",
      "recall@30 0.3030718368543533\n",
      "F1@50 0.24007420748960762\n",
      "percision@50 0.22680000000000003\n",
      "recall@50 0.33641127833243706\n",
      "ndcg@10 0.5805772719285727\n",
      "ndcg@30 0.5092910973785529\n",
      "ndcg@50 0.49835335437965766\n",
      "ndcg@all 0.6770994935750559\n",
      "\n",
      "epoch 3300\n",
      "loss_mse 4.700291924564226e-07\n",
      "loss_w_reg 0.002213440602645278\n",
      "F1@10 0.2765668819568576\n",
      "percision@10 0.571\n",
      "recall@10 0.2093643840163226\n",
      "F1@30 0.27413812182696184\n",
      "percision@30 0.3189999999999999\n",
      "recall@30 0.3023763485305653\n",
      "F1@50 0.23978952064513653\n",
      "percision@50 0.2266\n",
      "recall@50 0.3360654285300655\n",
      "ndcg@10 0.5808209462522125\n",
      "ndcg@30 0.5091885920794711\n",
      "ndcg@50 0.49804347473643334\n",
      "ndcg@all 0.6783141223355643\n",
      "\n",
      "epoch 3400\n",
      "loss_mse 4.718589536878426e-07\n",
      "loss_w_reg 0.0022160366643220186\n",
      "F1@10 0.2758261412161168\n",
      "percision@10 0.57\n",
      "recall@10 0.20877614872220496\n",
      "F1@30 0.27473365476611905\n",
      "percision@30 0.3199999999999999\n",
      "recall@30 0.3028330944956777\n",
      "F1@50 0.2400755959071057\n",
      "percision@50 0.2268\n",
      "recall@50 0.3365063811470806\n",
      "ndcg@10 0.5803305113724238\n",
      "ndcg@30 0.5093333488300034\n",
      "ndcg@50 0.4982808159187546\n",
      "ndcg@all 0.6778591857053519\n",
      "\n",
      "epoch 3500\n",
      "loss_mse 4.731649880795885e-07\n",
      "loss_w_reg 0.0022170906886458397\n",
      "F1@10 0.27576422171147286\n",
      "percision@10 0.5699999999999998\n",
      "recall@10 0.20871662491268111\n",
      "F1@30 0.2745865959425896\n",
      "percision@30 0.31966666666666654\n",
      "recall@30 0.30273875487303614\n",
      "F1@50 0.24070317377960937\n",
      "percision@50 0.22719999999999999\n",
      "recall@50 0.3375612582519576\n",
      "ndcg@10 0.5797237341955179\n",
      "ndcg@30 0.5091093175653302\n",
      "ndcg@50 0.4987694809392518\n",
      "ndcg@all 0.6783368489822763\n",
      "\n",
      "epoch 3600\n",
      "loss_mse 4.732300453724747e-07\n",
      "loss_w_reg 0.0022160112857818604\n",
      "F1@10 0.2757747925359972\n",
      "percision@10 0.5699999999999998\n",
      "recall@10 0.2087255375686526\n",
      "F1@30 0.2740466319879953\n",
      "percision@30 0.3189999999999999\n",
      "recall@30 0.3022339955478543\n",
      "F1@50 0.24073446955558594\n",
      "percision@50 0.22719999999999999\n",
      "recall@50 0.33759298095159074\n",
      "ndcg@10 0.5794590237862578\n",
      "ndcg@30 0.5088317079789636\n",
      "ndcg@50 0.4984745007730935\n",
      "ndcg@all 0.6783840800855628\n",
      "\n",
      "epoch 3700\n",
      "loss_mse 4.755922589083639e-07\n",
      "loss_w_reg 0.002216432010754943\n",
      "F1@10 0.2757747925359972\n",
      "percision@10 0.5699999999999998\n",
      "recall@10 0.2087255375686526\n",
      "F1@30 0.27435432429568757\n",
      "percision@30 0.31933333333333325\n",
      "recall@30 0.3025197098335686\n",
      "F1@50 0.24103204228526326\n",
      "percision@50 0.2274\n",
      "recall@50 0.3381013479916019\n",
      "ndcg@10 0.5795528459262879\n",
      "ndcg@30 0.5090561193455719\n",
      "ndcg@50 0.4984655428286613\n",
      "ndcg@all 0.6778338844362167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 3800\n",
      "loss_mse 4.7457814389417763e-07\n",
      "loss_w_reg 0.0022167530842125416\n",
      "F1@10 0.2762747925359972\n",
      "percision@10 0.5709999999999998\n",
      "recall@10 0.20905887090198594\n",
      "F1@30 0.27377499173009895\n",
      "percision@30 0.3189999999999999\n",
      "recall@30 0.3017605343685108\n",
      "F1@50 0.24083903956391187\n",
      "percision@50 0.22719999999999999\n",
      "recall@50 0.3377970054347175\n",
      "ndcg@10 0.5799159903406654\n",
      "ndcg@30 0.5085773770828247\n",
      "ndcg@50 0.4983262567632096\n",
      "ndcg@all 0.678281503091502\n",
      "\n",
      "epoch 3900\n",
      "loss_mse 4.764117988997896e-07\n",
      "loss_w_reg 0.0022159439977258444\n",
      "F1@10 0.2762747925359972\n",
      "percision@10 0.5709999999999998\n",
      "recall@10 0.20905887090198594\n",
      "F1@30 0.2740784008541955\n",
      "percision@30 0.31933333333333325\n",
      "recall@30 0.30202893433677347\n",
      "F1@50 0.24096927908486993\n",
      "percision@50 0.22719999999999999\n",
      "recall@50 0.3380448686825808\n",
      "ndcg@10 0.5800935734169371\n",
      "ndcg@30 0.5088453400767861\n",
      "ndcg@50 0.49856617363878103\n",
      "ndcg@all 0.6785726657896259\n",
      "\n",
      "epoch 4000\n",
      "loss_mse 4.796611960955488e-07\n",
      "loss_w_reg 0.0022178262006491423\n",
      "F1@10 0.27667479253599714\n",
      "percision@10 0.5719999999999998\n",
      "recall@10 0.2093088709019859\n",
      "F1@30 0.2739264294611176\n",
      "percision@30 0.3196666666666666\n",
      "recall@30 0.3018164802963283\n",
      "F1@50 0.24144227025234616\n",
      "percision@50 0.22799999999999998\n",
      "recall@50 0.3384105343659183\n",
      "ndcg@10 0.5803202616968863\n",
      "ndcg@30 0.5086048780733557\n",
      "ndcg@50 0.498749192378057\n",
      "ndcg@all 0.6784524282585932\n",
      "\n",
      "epoch 4100\n",
      "loss_mse 4.783366875926731e-07\n",
      "loss_w_reg 0.0022169649600982666\n",
      "F1@10 0.276907350675532\n",
      "percision@10 0.5729999999999998\n",
      "recall@10 0.20944044984935434\n",
      "F1@30 0.27482193116381815\n",
      "percision@30 0.32033333333333325\n",
      "recall@30 0.3026985015009301\n",
      "F1@50 0.24131617505582842\n",
      "percision@50 0.22799999999999998\n",
      "recall@50 0.338115790864653\n",
      "ndcg@10 0.5806818361828383\n",
      "ndcg@30 0.5091897548162512\n",
      "ndcg@50 0.4986409265051941\n",
      "ndcg@all 0.6784975773228662\n",
      "\n",
      "epoch 4200\n",
      "loss_mse 4.751981919071113e-07\n",
      "loss_w_reg 0.0022174683399498463\n",
      "F1@10 0.2774478912160726\n",
      "percision@10 0.5739999999999998\n",
      "recall@10 0.20981082021972472\n",
      "F1@30 0.27534000903016354\n",
      "percision@30 0.3209999999999999\n",
      "recall@30 0.30308801780998407\n",
      "F1@50 0.24134774377850934\n",
      "percision@50 0.22799999999999998\n",
      "recall@50 0.33815861036399425\n",
      "ndcg@10 0.5810932422200218\n",
      "ndcg@30 0.5094378943076665\n",
      "ndcg@50 0.49865451921921805\n",
      "ndcg@all 0.6788562743513886\n",
      "\n",
      "epoch 4300\n",
      "loss_mse 4.6717360646653106e-07\n",
      "loss_w_reg 0.002217298373579979\n",
      "F1@10 0.27790243667061804\n",
      "percision@10 0.5749999999999997\n",
      "recall@10 0.21010493786678353\n",
      "F1@30 0.2750996462183721\n",
      "percision@30 0.32099999999999995\n",
      "recall@30 0.30256840996684675\n",
      "F1@50 0.2417718485064116\n",
      "percision@50 0.22839999999999996\n",
      "recall@50 0.3386526916815445\n",
      "ndcg@10 0.581790858314828\n",
      "ndcg@30 0.5093240747783243\n",
      "ndcg@50 0.4992167233750419\n",
      "ndcg@all 0.6778890470239461\n",
      "\n",
      "epoch 4400\n",
      "loss_mse 4.662096841911989e-07\n",
      "loss_w_reg 0.0022174320183694363\n",
      "F1@10 0.2784713469270283\n",
      "percision@10 0.5769999999999998\n",
      "recall@10 0.21043718187549815\n",
      "F1@30 0.2755251781332657\n",
      "percision@30 0.32133333333333325\n",
      "recall@30 0.3031566452609644\n",
      "F1@50 0.24123134238891605\n",
      "percision@50 0.22779999999999997\n",
      "recall@50 0.33818252230593393\n",
      "ndcg@10 0.58436307453728\n",
      "ndcg@30 0.5103453638421652\n",
      "ndcg@50 0.49962311224003836\n",
      "ndcg@all 0.6790927580550407\n",
      "\n",
      "epoch 4500\n",
      "loss_mse 4.671859983318427e-07\n",
      "loss_w_reg 0.0022185416892170906\n",
      "F1@10 0.2789713469270283\n",
      "percision@10 0.5779999999999998\n",
      "recall@10 0.2107705152088315\n",
      "F1@30 0.2752524877481892\n",
      "percision@30 0.32066666666666654\n",
      "recall@30 0.30311469372480754\n",
      "F1@50 0.2410547763351427\n",
      "percision@50 0.2274\n",
      "recall@50 0.3381378915666678\n",
      "ndcg@10 0.5847158452871728\n",
      "ndcg@30 0.5103942478487583\n",
      "ndcg@50 0.4997190365279633\n",
      "ndcg@all 0.6799967307402278\n",
      "\n",
      "epoch 4600\n",
      "loss_mse 4.654790188851621e-07\n",
      "loss_w_reg 0.0022197661455720663\n",
      "F1@10 0.27908197833555504\n",
      "percision@10 0.5779999999999998\n",
      "recall@10 0.2108419189009248\n",
      "F1@30 0.27510671452858504\n",
      "percision@30 0.32066666666666654\n",
      "recall@30 0.3029514618185416\n",
      "F1@50 0.241405812820576\n",
      "percision@50 0.22799999999999998\n",
      "recall@50 0.33839299232127695\n",
      "ndcg@10 0.5841443160520999\n",
      "ndcg@30 0.5097738063855948\n",
      "ndcg@50 0.4992040635245939\n",
      "ndcg@all 0.6796244704207495\n",
      "\n",
      "epoch 4700\n",
      "loss_mse 4.640317854409659e-07\n",
      "loss_w_reg 0.002216990338638425\n",
      "F1@10 0.27897835139254984\n",
      "percision@10 0.5769999999999998\n",
      "recall@10 0.2107872740921816\n",
      "F1@30 0.2748250243877399\n",
      "percision@30 0.32033333333333325\n",
      "recall@30 0.3027075593795172\n",
      "F1@50 0.24175186575753801\n",
      "percision@50 0.22839999999999994\n",
      "recall@50 0.33869675912604985\n",
      "ndcg@10 0.583831171640284\n",
      "ndcg@30 0.5095520733251386\n",
      "ndcg@50 0.4993325835682091\n",
      "ndcg@all 0.6789491317021443\n",
      "\n",
      "epoch 4800\n",
      "loss_mse 4.653958569633687e-07\n",
      "loss_w_reg 0.002218761947005987\n",
      "F1@10 0.27897835139254984\n",
      "percision@10 0.5769999999999998\n",
      "recall@10 0.2107872740921816\n",
      "F1@30 0.27420730046281716\n",
      "percision@30 0.31966666666666654\n",
      "recall@30 0.30212856847794567\n",
      "F1@50 0.2419000139056862\n",
      "percision@50 0.22859999999999991\n",
      "recall@50 0.3388144061848734\n",
      "ndcg@10 0.5838085276914513\n",
      "ndcg@30 0.5092311563762327\n",
      "ndcg@50 0.4993446152088947\n",
      "ndcg@all 0.6791943183502991\n",
      "\n",
      "epoch 4900\n",
      "loss_mse 4.642166118173918e-07\n",
      "loss_w_reg 0.002216324908658862\n",
      "F1@10 0.27913835139254983\n",
      "percision@10 0.5779999999999998\n",
      "recall@10 0.21087423061392074\n",
      "F1@30 0.2744559707174359\n",
      "percision@30 0.3199999999999999\n",
      "recall@30 0.3023260803195253\n",
      "F1@50 0.2415350981799457\n",
      "percision@50 0.2281999999999999\n",
      "recall@50 0.3384700612800582\n",
      "ndcg@10 0.5839150628151715\n",
      "ndcg@30 0.5093613159398004\n",
      "ndcg@50 0.4992264016641286\n",
      "ndcg@all 0.6792609990588221\n",
      "\n",
      "epoch 5000\n",
      "loss_mse 4.6288249677672866e-07\n",
      "loss_w_reg 0.002221441827714443\n",
      "F1@10 0.27913835139254983\n",
      "percision@10 0.5779999999999998\n",
      "recall@10 0.21087423061392074\n",
      "F1@30 0.2743672904879106\n",
      "percision@30 0.31999999999999995\n",
      "recall@30 0.302153316091883\n",
      "F1@50 0.24123993879387717\n",
      "percision@50 0.2277999999999999\n",
      "recall@50 0.33823306236348183\n",
      "ndcg@10 0.5837962013335232\n",
      "ndcg@30 0.5091849011315992\n",
      "ndcg@50 0.49902238923053666\n",
      "ndcg@all 0.6790535272032389\n",
      "\n",
      "epoch 5100\n",
      "loss_mse 4.6391684804802935e-07\n",
      "loss_w_reg 0.0022202860563993454\n",
      "F1@10 0.2788208910750895\n",
      "percision@10 0.5769999999999998\n",
      "recall@10 0.21068555136863767\n",
      "F1@30 0.27434779731052267\n",
      "percision@30 0.3199999999999999\n",
      "recall@30 0.3021070197955867\n",
      "F1@50 0.2413607968623651\n",
      "percision@50 0.2279999999999999\n",
      "recall@50 0.33840166728618504\n",
      "ndcg@10 0.5830970032647127\n",
      "ndcg@30 0.5092060637174239\n",
      "ndcg@50 0.4991243992803748\n",
      "ndcg@all 0.6796383320394286\n",
      "\n",
      "epoch 5200\n",
      "loss_mse 4.6517294549630606e-07\n",
      "loss_w_reg 0.0022210285533219576\n",
      "F1@10 0.2781066053608038\n",
      "percision@10 0.5759999999999998\n",
      "recall@10 0.21012999581308214\n",
      "F1@30 0.27469867450350516\n",
      "percision@30 0.32033333333333325\n",
      "recall@30 0.3024773901659571\n",
      "F1@50 0.24137161937318757\n",
      "percision@50 0.22799999999999987\n",
      "recall@50 0.3384326270385071\n",
      "ndcg@10 0.5832828634167837\n",
      "ndcg@30 0.5097931057787743\n",
      "ndcg@50 0.499582012378887\n",
      "ndcg@all 0.6798364707386607\n",
      "\n",
      "epoch 5300\n",
      "loss_mse 4.6523669539055845e-07\n",
      "loss_w_reg 0.0022197915241122246\n",
      "F1@10 0.27766216091635937\n",
      "percision@10 0.575\n",
      "recall@10 0.20984428152736787\n",
      "F1@30 0.27475043433787366\n",
      "percision@30 0.32033333333333325\n",
      "recall@30 0.30254861523718213\n",
      "F1@50 0.24150529529242476\n",
      "percision@50 0.2281999999999999\n",
      "recall@50 0.3385309135595822\n",
      "ndcg@10 0.5828750555879372\n",
      "ndcg@30 0.5098768315833049\n",
      "ndcg@50 0.4995645567102949\n",
      "ndcg@all 0.6794399304107701\n",
      "\n",
      "epoch 5400\n",
      "loss_mse 4.652931124837778e-07\n",
      "loss_w_reg 0.0022177842911332846\n",
      "F1@10 0.27766216091635937\n",
      "percision@10 0.575\n",
      "recall@10 0.20984428152736787\n",
      "F1@30 0.274043212282135\n",
      "percision@30 0.3196666666666666\n",
      "recall@30 0.3017164775040401\n",
      "F1@50 0.2423323865159561\n",
      "percision@50 0.22899999999999987\n",
      "recall@50 0.3394369102921317\n",
      "ndcg@10 0.5829313284316157\n",
      "ndcg@30 0.5096306000412626\n",
      "ndcg@50 0.49999342406957353\n",
      "ndcg@all 0.6796278340640073\n",
      "\n",
      "epoch 5500\n",
      "loss_mse 4.6426634980889503e-07\n",
      "loss_w_reg 0.002218545414507389\n",
      "F1@10 0.27766216091635937\n",
      "percision@10 0.575\n",
      "recall@10 0.20984428152736787\n",
      "F1@30 0.27449636128367105\n",
      "percision@30 0.32033333333333325\n",
      "recall@30 0.3020603928479555\n",
      "F1@50 0.24223327977829356\n",
      "percision@50 0.22899999999999987\n",
      "recall@50 0.3393386237710566\n",
      "ndcg@10 0.5828045284856226\n",
      "ndcg@30 0.5097050572844598\n",
      "ndcg@50 0.499889803362149\n",
      "ndcg@all 0.6794654139026101\n",
      "\n",
      "epoch 5600\n",
      "loss_mse 4.6640275286335964e-07\n",
      "loss_w_reg 0.0022191654425114393\n",
      "F1@10 0.27806845366601735\n",
      "percision@10 0.5749999999999998\n",
      "recall@10 0.2101170088000951\n",
      "F1@30 0.2741515336974642\n",
      "percision@30 0.31999999999999995\n",
      "recall@30 0.3017032499908126\n",
      "F1@50 0.2420608659851901\n",
      "percision@50 0.2287999999999999\n",
      "recall@50 0.3391871086195415\n",
      "ndcg@10 0.5845162293608984\n",
      "ndcg@30 0.5107533914701474\n",
      "ndcg@50 0.5010330768729754\n",
      "ndcg@all 0.6802497795648376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 5700\n",
      "loss_mse 4.6463776470773155e-07\n",
      "loss_w_reg 0.0022198979277163744\n",
      "F1@10 0.27806845366601735\n",
      "percision@10 0.5749999999999998\n",
      "recall@10 0.2101170088000951\n",
      "F1@30 0.2743065724571541\n",
      "percision@30 0.32033333333333325\n",
      "recall@30 0.3018042600918227\n",
      "F1@50 0.24198498948074218\n",
      "percision@50 0.2287999999999999\n",
      "recall@50 0.33907311490884967\n",
      "ndcg@10 0.5832826542341473\n",
      "ndcg@30 0.510048306300283\n",
      "ndcg@50 0.5000949585661901\n",
      "ndcg@all 0.6800302280849216\n",
      "\n",
      "epoch 5800\n",
      "loss_mse 4.634248966794985e-07\n",
      "loss_w_reg 0.002219131449237466\n",
      "F1@10 0.27806845366601735\n",
      "percision@10 0.5749999999999998\n",
      "recall@10 0.2101170088000951\n",
      "F1@30 0.2744584964951351\n",
      "percision@30 0.32033333333333325\n",
      "recall@30 0.30197371382649385\n",
      "F1@50 0.24219775543818897\n",
      "percision@50 0.22899999999999987\n",
      "recall@50 0.33930038763612236\n",
      "ndcg@10 0.583432104387863\n",
      "ndcg@30 0.5101450859005059\n",
      "ndcg@50 0.5003023515764339\n",
      "ndcg@all 0.6804755517846899\n",
      "\n",
      "epoch 5900\n",
      "loss_mse 4.644358853056474e-07\n",
      "loss_w_reg 0.0022192730102688074\n",
      "F1@10 0.27841933085899984\n",
      "percision@10 0.5759999999999998\n",
      "recall@10 0.21032977475754192\n",
      "F1@30 0.2738503087339091\n",
      "percision@30 0.3193333333333333\n",
      "recall@30 0.30152878838156844\n",
      "F1@50 0.24248565933214092\n",
      "percision@50 0.2293999999999999\n",
      "recall@50 0.3395253623580638\n",
      "ndcg@10 0.5836514601622569\n",
      "ndcg@30 0.5098165397582958\n",
      "ndcg@50 0.5002989584422756\n",
      "ndcg@all 0.6802504135050412\n",
      "\n",
      "epoch 6000\n",
      "loss_mse 4.6408649723161943e-07\n",
      "loss_w_reg 0.0022207587026059628\n",
      "F1@10 0.2786518889985347\n",
      "percision@10 0.5769999999999998\n",
      "recall@10 0.21046135370491034\n",
      "F1@30 0.27362303600663634\n",
      "percision@30 0.319\n",
      "recall@30 0.301356374588465\n",
      "F1@50 0.24218608410298825\n",
      "percision@50 0.22899999999999987\n",
      "recall@50 0.3392850877585214\n",
      "ndcg@10 0.5837692486242981\n",
      "ndcg@30 0.5096625650534655\n",
      "ndcg@50 0.5001075073096402\n",
      "ndcg@all 0.6799478236720278\n",
      "\n",
      "epoch 6100\n",
      "loss_mse 4.6338894321706903e-07\n",
      "loss_w_reg 0.0022192050237208605\n",
      "F1@10 0.2786518889985347\n",
      "percision@10 0.5769999999999998\n",
      "recall@10 0.21046135370491034\n",
      "F1@30 0.27385191748551785\n",
      "percision@30 0.31933333333333336\n",
      "recall@30 0.3015302939521797\n",
      "F1@50 0.24202612704448043\n",
      "percision@50 0.22899999999999987\n",
      "recall@50 0.3390180626689157\n",
      "ndcg@10 0.5837534893260785\n",
      "ndcg@30 0.5096831622746947\n",
      "ndcg@50 0.5000284251104506\n",
      "ndcg@all 0.6803058327989362\n",
      "\n",
      "epoch 6200\n",
      "loss_mse 4.645650619750086e-07\n",
      "loss_w_reg 0.0022211407776921988\n",
      "F1@10 0.2786518889985347\n",
      "percision@10 0.5769999999999998\n",
      "recall@10 0.21046135370491034\n",
      "F1@30 0.2734263855706242\n",
      "percision@30 0.319\n",
      "recall@30 0.300942058658062\n",
      "F1@50 0.2419943810127344\n",
      "percision@50 0.22899999999999987\n",
      "recall@50 0.3389678234344659\n",
      "ndcg@10 0.5837848892208808\n",
      "ndcg@30 0.5094988147245642\n",
      "ndcg@50 0.5000213873029221\n",
      "ndcg@all 0.6794191985029812\n",
      "\n",
      "epoch 6300\n",
      "loss_mse 4.6382382379306364e-07\n",
      "loss_w_reg 0.002219105837866664\n",
      "F1@10 0.2786518889985347\n",
      "percision@10 0.5769999999999998\n",
      "recall@10 0.21046135370491034\n",
      "F1@30 0.2734263855706242\n",
      "percision@30 0.319\n",
      "recall@30 0.300942058658062\n",
      "F1@50 0.24195287801117807\n",
      "percision@50 0.22899999999999984\n",
      "recall@50 0.33890260097930636\n",
      "ndcg@10 0.583659636562877\n",
      "ndcg@30 0.5094113381571156\n",
      "ndcg@50 0.49993450412199464\n",
      "ndcg@all 0.679825918761863\n",
      "\n",
      "epoch 6400\n",
      "loss_mse 4.648369156257104e-07\n",
      "loss_w_reg 0.0022199172526597977\n",
      "F1@10 0.278860222331868\n",
      "percision@10 0.5779999999999998\n",
      "recall@10 0.21057763277467778\n",
      "F1@30 0.27355429588968133\n",
      "percision@30 0.3193333333333333\n",
      "recall@30 0.30103482982846097\n",
      "F1@50 0.24214335420165425\n",
      "percision@50 0.22919999999999985\n",
      "recall@50 0.33908441916112453\n",
      "ndcg@10 0.5843215072903332\n",
      "ndcg@30 0.509679060485064\n",
      "ndcg@50 0.5001428497303977\n",
      "ndcg@all 0.6794711283799708\n",
      "\n",
      "epoch 6500\n",
      "loss_mse 4.6510098172802827e-07\n",
      "loss_w_reg 0.002220234600827098\n",
      "F1@10 0.278860222331868\n",
      "percision@10 0.5779999999999998\n",
      "recall@10 0.21057763277467778\n",
      "F1@30 0.2742100335945994\n",
      "percision@30 0.32\n",
      "recall@30 0.3016799911187835\n",
      "F1@50 0.24214335420165425\n",
      "percision@50 0.22919999999999985\n",
      "recall@50 0.33908441916112453\n",
      "ndcg@10 0.584626961787814\n",
      "ndcg@30 0.5101144151503395\n",
      "ndcg@50 0.5002885930867077\n",
      "ndcg@all 0.6802739952726617\n",
      "\n",
      "epoch 6600\n",
      "loss_mse 4.6593711999776133e-07\n",
      "loss_w_reg 0.0022216772194951773\n",
      "F1@10 0.278860222331868\n",
      "percision@10 0.5779999999999998\n",
      "recall@10 0.21057763277467778\n",
      "F1@30 0.2743052716898375\n",
      "percision@30 0.3203333333333333\n",
      "recall@30 0.30173554667433905\n",
      "F1@50 0.24249534825846758\n",
      "percision@50 0.22959999999999983\n",
      "recall@50 0.33948239430228155\n",
      "ndcg@10 0.5846551158360048\n",
      "ndcg@30 0.5103674000117363\n",
      "ndcg@50 0.5004232122840331\n",
      "ndcg@all 0.6809602126505608\n",
      "\n",
      "epoch 6700\n",
      "loss_mse 4.6550368892894767e-07\n",
      "loss_w_reg 0.002220648340880871\n",
      "F1@10 0.2790177026468287\n",
      "percision@10 0.579\n",
      "recall@10 0.21066310286014786\n",
      "F1@30 0.2743052716898375\n",
      "percision@30 0.3203333333333333\n",
      "recall@30 0.30173554667433905\n",
      "F1@50 0.24244571713841392\n",
      "percision@50 0.22959999999999986\n",
      "recall@50 0.3394092717726373\n",
      "ndcg@10 0.5840972034994126\n",
      "ndcg@30 0.5099538245310956\n",
      "ndcg@50 0.49996474502864563\n",
      "ndcg@all 0.6801220710820131\n",
      "\n",
      "epoch 6800\n",
      "loss_mse 4.6526304231520044e-07\n",
      "loss_w_reg 0.0022228851448744535\n",
      "F1@10 0.2787637343928604\n",
      "percision@10 0.579\n",
      "recall@10 0.21045178210543086\n",
      "F1@30 0.27411629172626734\n",
      "percision@30 0.32\n",
      "recall@30 0.3015940451126959\n",
      "F1@50 0.2424215854199989\n",
      "percision@50 0.22959999999999983\n",
      "recall@50 0.33933034511308063\n",
      "ndcg@10 0.5839822757312138\n",
      "ndcg@30 0.5099018652757031\n",
      "ndcg@50 0.4999769823072742\n",
      "ndcg@all 0.679942587316738\n",
      "\n",
      "epoch 6900\n",
      "loss_mse 4.64644926978508e-07\n",
      "loss_w_reg 0.0022186082787811756\n",
      "F1@10 0.2785311762533255\n",
      "percision@10 0.5779999999999998\n",
      "recall@10 0.21032020315806244\n",
      "F1@30 0.27387819648817213\n",
      "percision@30 0.3196666666666667\n",
      "recall@30 0.3014088599275107\n",
      "F1@50 0.2427751964478682\n",
      "percision@50 0.22999999999999987\n",
      "recall@50 0.3396663134925273\n",
      "ndcg@10 0.5836539952698994\n",
      "ndcg@30 0.5098292439079333\n",
      "ndcg@50 0.50016230242084\n",
      "ndcg@all 0.6796638477729998\n",
      "\n",
      "epoch 7000\n",
      "loss_mse 4.6425432742580597e-07\n",
      "loss_w_reg 0.0022221512626856565\n",
      "F1@10 0.2785311762533255\n",
      "percision@10 0.5779999999999998\n",
      "recall@10 0.21032020315806244\n",
      "F1@30 0.27413841187803345\n",
      "percision@30 0.32\n",
      "recall@30 0.30164587727665493\n",
      "F1@50 0.2428734315461033\n",
      "percision@50 0.22999999999999987\n",
      "recall@50 0.3398529042578656\n",
      "ndcg@10 0.5839696732033288\n",
      "ndcg@30 0.5101799404621338\n",
      "ndcg@50 0.5004158145112908\n",
      "ndcg@all 0.6801976494989371\n",
      "\n",
      "epoch 7100\n",
      "loss_mse 4.6511627260770183e-07\n",
      "loss_w_reg 0.0022218271624296904\n",
      "F1@10 0.2785311762533255\n",
      "percision@10 0.5779999999999998\n",
      "recall@10 0.21032020315806244\n",
      "F1@30 0.2742413631477658\n",
      "percision@30 0.32\n",
      "recall@30 0.30192161257077266\n",
      "F1@50 0.2428388380200346\n",
      "percision@50 0.2299999999999999\n",
      "recall@50 0.3398053499100396\n",
      "ndcg@10 0.5840215177404018\n",
      "ndcg@30 0.5099170369856829\n",
      "ndcg@50 0.5003532586306323\n",
      "ndcg@all 0.6805501059114409\n",
      "\n",
      "epoch 7200\n",
      "loss_mse 4.644556383937015e-07\n",
      "loss_w_reg 0.0022225824650377035\n",
      "F1@10 0.2785311762533255\n",
      "percision@10 0.5779999999999998\n",
      "recall@10 0.21032020315806244\n",
      "F1@30 0.2742413631477658\n",
      "percision@30 0.32\n",
      "recall@30 0.30192161257077266\n",
      "F1@50 0.24252850142418833\n",
      "percision@50 0.22959999999999986\n",
      "recall@50 0.33954959543433616\n",
      "ndcg@10 0.5841259129984437\n",
      "ndcg@30 0.5099777718133415\n",
      "ndcg@50 0.5003367149574026\n",
      "ndcg@all 0.6801453972948478\n",
      "\n",
      "epoch 7300\n",
      "loss_mse 4.641764803636761e-07\n",
      "loss_w_reg 0.0022225291468203068\n",
      "F1@10 0.2781090271305185\n",
      "percision@10 0.5769999999999998\n",
      "recall@10 0.21003196641633737\n",
      "F1@30 0.27400039929234415\n",
      "percision@30 0.3196666666666667\n",
      "recall@30 0.3017329333254896\n",
      "F1@50 0.24263166637757988\n",
      "percision@50 0.22959999999999986\n",
      "recall@50 0.33966883570347856\n",
      "ndcg@10 0.5836692337668191\n",
      "ndcg@30 0.5099909949253568\n",
      "ndcg@50 0.5005727430861668\n",
      "ndcg@all 0.6802179880823581\n",
      "\n",
      "epoch 7400\n",
      "loss_mse 4.642515705199912e-07\n",
      "loss_w_reg 0.002221910282969475\n",
      "F1@10 0.2781090271305185\n",
      "percision@10 0.5769999999999998\n",
      "recall@10 0.21003196641633737\n",
      "F1@30 0.27418612021094835\n",
      "percision@30 0.32000000000000006\n",
      "recall@30 0.3019011527871874\n",
      "F1@50 0.24285283273314295\n",
      "percision@50 0.2299999999999999\n",
      "recall@50 0.33982778261193386\n",
      "ndcg@10 0.5837135464630169\n",
      "ndcg@30 0.5102791356124579\n",
      "ndcg@50 0.5006695554075522\n",
      "ndcg@all 0.6798959821177105\n",
      "\n",
      "epoch 7500\n",
      "loss_mse 4.62666321254801e-07\n",
      "loss_w_reg 0.002221137285232544\n",
      "F1@10 0.2781090271305185\n",
      "percision@10 0.5769999999999998\n",
      "recall@10 0.21003196641633737\n",
      "F1@30 0.27403906138741896\n",
      "percision@30 0.3196666666666667\n",
      "recall@30 0.3018068131645459\n",
      "F1@50 0.24257903479863252\n",
      "percision@50 0.22939999999999988\n",
      "recall@50 0.33963853267317545\n",
      "ndcg@10 0.5836599952683216\n",
      "ndcg@30 0.5101787620871578\n",
      "ndcg@50 0.5005884918120025\n",
      "ndcg@all 0.6806955778769228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 7600\n",
      "loss_mse 4.6443071255453106e-07\n",
      "loss_w_reg 0.002221336355432868\n",
      "F1@10 0.2779490271305185\n",
      "percision@10 0.576\n",
      "recall@10 0.20994500989459822\n",
      "F1@30 0.27432853888054387\n",
      "percision@30 0.31966666666666677\n",
      "recall@30 0.30230957837319344\n",
      "F1@50 0.24257903479863252\n",
      "percision@50 0.22939999999999988\n",
      "recall@50 0.33963853267317545\n",
      "ndcg@10 0.5836802142937101\n",
      "ndcg@30 0.5104434475393468\n",
      "ndcg@50 0.5006683590476818\n",
      "ndcg@all 0.6807289469590616\n",
      "\n",
      "epoch 7700\n",
      "loss_mse 4.6335523506968457e-07\n",
      "loss_w_reg 0.0022212478797882795\n",
      "F1@10 0.27759620661769796\n",
      "percision@10 0.576\n",
      "recall@10 0.20968713883013046\n",
      "F1@30 0.27414996745197245\n",
      "percision@30 0.3193333333333334\n",
      "recall@30 0.3021876271536812\n",
      "F1@50 0.24281713003672775\n",
      "percision@50 0.22959999999999986\n",
      "recall@50 0.3399326503202343\n",
      "ndcg@10 0.5833643291367355\n",
      "ndcg@30 0.5103659433606872\n",
      "ndcg@50 0.5007457980343226\n",
      "ndcg@all 0.6805122801305672\n",
      "\n",
      "epoch 7800\n",
      "loss_mse 4.6315187773871003e-07\n",
      "loss_w_reg 0.0022213971242308617\n",
      "F1@10 0.27827427541606753\n",
      "percision@10 0.5769999999999998\n",
      "recall@10 0.21015363312015414\n",
      "F1@30 0.27443165759281757\n",
      "percision@30 0.31966666666666677\n",
      "recall@30 0.3024315295927056\n",
      "F1@50 0.2424445521556728\n",
      "percision@50 0.22919999999999988\n",
      "recall@50 0.33958501410628683\n",
      "ndcg@10 0.5835680508158818\n",
      "ndcg@30 0.510427567201539\n",
      "ndcg@50 0.5006693514249926\n",
      "ndcg@all 0.679769028453368\n",
      "\n",
      "epoch 7900\n",
      "loss_mse 4.6347270199476043e-07\n",
      "loss_w_reg 0.0022221950348466635\n",
      "F1@10 0.27847035384744007\n",
      "percision@10 0.5779999999999998\n",
      "recall@10 0.21026232877232803\n",
      "F1@30 0.27428313151572\n",
      "percision@30 0.31966666666666677\n",
      "recall@30 0.30231477379947924\n",
      "F1@50 0.24254278725390788\n",
      "percision@50 0.22919999999999988\n",
      "recall@50 0.3397716048716252\n",
      "ndcg@10 0.5836437296679838\n",
      "ndcg@30 0.5100462626998525\n",
      "ndcg@50 0.500472076599185\n",
      "ndcg@all 0.6796910729185416\n",
      "\n",
      "epoch 8000\n",
      "loss_mse 4.6289446231639886e-07\n",
      "loss_w_reg 0.002221986185759306\n",
      "F1@10 0.27847035384744007\n",
      "percision@10 0.5779999999999998\n",
      "recall@10 0.21026232877232803\n",
      "F1@30 0.2740267212593097\n",
      "percision@30 0.31933333333333347\n",
      "recall@30 0.30210644046614593\n",
      "F1@50 0.242629743775647\n",
      "percision@50 0.22939999999999988\n",
      "recall@50 0.33982716042718075\n",
      "ndcg@10 0.5831873003617037\n",
      "ndcg@30 0.5095304106941141\n",
      "ndcg@50 0.5002828310017012\n",
      "ndcg@all 0.6800223546651746\n",
      "\n",
      "epoch 8100\n",
      "loss_mse 4.637332153833995e-07\n",
      "loss_w_reg 0.0022233384661376476\n",
      "F1@10 0.27847035384744007\n",
      "percision@10 0.5779999999999998\n",
      "recall@10 0.21026232877232803\n",
      "F1@30 0.27433771450906863\n",
      "percision@30 0.3200000000000001\n",
      "recall@30 0.30230947574096134\n",
      "F1@50 0.24288615403205724\n",
      "percision@50 0.22959999999999992\n",
      "recall@50 0.3401843032843236\n",
      "ndcg@10 0.5831540217267066\n",
      "ndcg@30 0.5097164264947848\n",
      "ndcg@50 0.5003942624045026\n",
      "ndcg@all 0.6802421208209853\n",
      "\n",
      "epoch 8200\n",
      "loss_mse 4.639883286472468e-07\n",
      "loss_w_reg 0.0022208825685083866\n",
      "F1@10 0.2787075079976377\n",
      "percision@10 0.5779999999999998\n",
      "recall@10 0.21043449519987464\n",
      "F1@30 0.2738090328428396\n",
      "percision@30 0.3196666666666667\n",
      "recall@30 0.3015681513271061\n",
      "F1@50 0.24263615403205727\n",
      "percision@50 0.2293999999999999\n",
      "recall@50 0.3398509699509903\n",
      "ndcg@10 0.5820692057874579\n",
      "ndcg@30 0.5094406984708189\n",
      "ndcg@50 0.5002094018139374\n",
      "ndcg@all 0.6796303721236435\n",
      "\n",
      "epoch 8300\n",
      "loss_mse 4.6442536927315814e-07\n",
      "loss_w_reg 0.002221050439402461\n",
      "F1@10 0.27818119220816406\n",
      "percision@10 0.577\n",
      "recall@10 0.21007735234273184\n",
      "F1@30 0.2738090328428396\n",
      "percision@30 0.3196666666666667\n",
      "recall@30 0.3015681513271061\n",
      "F1@50 0.2428559342518375\n",
      "percision@50 0.22959999999999992\n",
      "recall@50 0.34009487239001474\n",
      "ndcg@10 0.5818460189015346\n",
      "ndcg@30 0.5094871232044541\n",
      "ndcg@50 0.500299281212494\n",
      "ndcg@all 0.6793285490040804\n",
      "\n",
      "epoch 8400\n",
      "loss_mse 4.637359154457954e-07\n",
      "loss_w_reg 0.0022215251810848713\n",
      "F1@10 0.27795841659906456\n",
      "percision@10 0.576\n",
      "recall@10 0.20995068031664935\n",
      "F1@30 0.2738090328428396\n",
      "percision@30 0.3196666666666667\n",
      "recall@30 0.3015681513271061\n",
      "F1@50 0.24309122836948455\n",
      "percision@50 0.2297999999999999\n",
      "recall@50 0.34038058667572896\n",
      "ndcg@10 0.5828961265069222\n",
      "ndcg@30 0.5103100258183995\n",
      "ndcg@50 0.501542210882305\n",
      "ndcg@all 0.6808857964669435\n",
      "\n",
      "epoch 8500\n",
      "loss_mse 4.6388245777961856e-07\n",
      "loss_w_reg 0.002221992937847972\n",
      "F1@10 0.277417876058524\n",
      "percision@10 0.575\n",
      "recall@10 0.20958030994627896\n",
      "F1@30 0.27408036577919714\n",
      "percision@30 0.32000000000000006\n",
      "recall@30 0.30182305662852305\n",
      "F1@50 0.24328730680085708\n",
      "percision@50 0.2299999999999999\n",
      "recall@50 0.3405728943680366\n",
      "ndcg@10 0.5825800416379242\n",
      "ndcg@30 0.5105814633343438\n",
      "ndcg@50 0.5016867523562425\n",
      "ndcg@all 0.6805823057054416\n",
      "\n",
      "epoch 8600\n",
      "loss_mse 4.6359761540770705e-07\n",
      "loss_w_reg 0.0022201526444405317\n",
      "F1@10 0.277417876058524\n",
      "percision@10 0.575\n",
      "recall@10 0.20958030994627896\n",
      "F1@30 0.27385564667807355\n",
      "percision@30 0.31966666666666677\n",
      "recall@30 0.3016535651030993\n",
      "F1@50 0.24304039322061016\n",
      "percision@50 0.2297999999999999\n",
      "recall@50 0.34025031372287534\n",
      "ndcg@10 0.5826595713620574\n",
      "ndcg@30 0.51053783991207\n",
      "ndcg@50 0.501596315106862\n",
      "ndcg@all 0.6797584235793486\n",
      "\n",
      "epoch 8700\n",
      "loss_mse 4.65238912283894e-07\n",
      "loss_w_reg 0.002223091432824731\n",
      "F1@10 0.2771709624782771\n",
      "percision@10 0.5740000000000001\n",
      "recall@10 0.20943946487585646\n",
      "F1@30 0.27440624191616875\n",
      "percision@30 0.32033333333333336\n",
      "recall@30 0.3021328679353433\n",
      "F1@50 0.2430140427594771\n",
      "percision@50 0.22959999999999986\n",
      "recall@50 0.3402804225029841\n",
      "ndcg@10 0.5822935923912803\n",
      "ndcg@30 0.5106965149704211\n",
      "ndcg@50 0.5015903591583509\n",
      "ndcg@all 0.6802410158647934\n",
      "\n",
      "epoch 8800\n",
      "loss_mse 4.6420561261584226e-07\n",
      "loss_w_reg 0.0022193891927599907\n",
      "F1@10 0.2771709624782771\n",
      "percision@10 0.5740000000000001\n",
      "recall@10 0.20943946487585646\n",
      "F1@30 0.27440624191616875\n",
      "percision@30 0.32033333333333336\n",
      "recall@30 0.3021328679353433\n",
      "F1@50 0.2425404480557206\n",
      "percision@50 0.22899999999999987\n",
      "recall@50 0.33987794814702144\n",
      "ndcg@10 0.5820092277730111\n",
      "ndcg@30 0.510652891641504\n",
      "ndcg@50 0.5013918955179113\n",
      "ndcg@all 0.6806966953601936\n",
      "\n",
      "epoch 8900\n",
      "loss_mse 4.647541800295585e-07\n",
      "loss_w_reg 0.0022219400852918625\n",
      "F1@10 0.27799139591480965\n",
      "percision@10 0.5760000000000001\n",
      "recall@10 0.20996902152610278\n",
      "F1@30 0.27440624191616875\n",
      "percision@30 0.32033333333333336\n",
      "recall@30 0.3021328679353433\n",
      "F1@50 0.2425404480557206\n",
      "percision@50 0.22899999999999987\n",
      "recall@50 0.33987794814702144\n",
      "ndcg@10 0.5829136301991024\n",
      "ndcg@30 0.5107662055509108\n",
      "ndcg@50 0.501480456486147\n",
      "ndcg@all 0.6812972775998268\n",
      "\n",
      "epoch 9000\n",
      "loss_mse 4.6377664375540917e-07\n",
      "loss_w_reg 0.0022218432277441025\n",
      "F1@10 0.27799139591480965\n",
      "percision@10 0.5760000000000001\n",
      "recall@10 0.20996902152610278\n",
      "F1@30 0.27440624191616875\n",
      "percision@30 0.32033333333333336\n",
      "recall@30 0.3021328679353433\n",
      "F1@50 0.24242792269139668\n",
      "percision@50 0.22899999999999987\n",
      "recall@50 0.33963316484044037\n",
      "ndcg@10 0.5828069844539656\n",
      "ndcg@30 0.5107205285911394\n",
      "ndcg@50 0.5014009222253011\n",
      "ndcg@all 0.6817149496969731\n",
      "\n",
      "epoch 9100\n",
      "loss_mse 4.64347834849832e-07\n",
      "loss_w_reg 0.0022216862998902798\n",
      "F1@10 0.27799139591480965\n",
      "percision@10 0.5760000000000001\n",
      "recall@10 0.20996902152610278\n",
      "F1@30 0.27402725372649844\n",
      "percision@30 0.3196666666666667\n",
      "recall@30 0.3018654421244392\n",
      "F1@50 0.24242792269139668\n",
      "percision@50 0.22899999999999987\n",
      "recall@50 0.33963316484044037\n",
      "ndcg@10 0.5827183637186966\n",
      "ndcg@30 0.5105062977021272\n",
      "ndcg@50 0.5013438716279105\n",
      "ndcg@all 0.6804387863530793\n",
      "\n",
      "epoch 9200\n",
      "loss_mse 4.649102152143314e-07\n",
      "loss_w_reg 0.0022221498657017946\n",
      "F1@10 0.27799139591480965\n",
      "percision@10 0.5760000000000001\n",
      "recall@10 0.20996902152610278\n",
      "F1@30 0.27402725372649844\n",
      "percision@30 0.3196666666666667\n",
      "recall@30 0.3018654421244392\n",
      "F1@50 0.24266601792949188\n",
      "percision@50 0.22919999999999988\n",
      "recall@50 0.33992728248749915\n",
      "ndcg@10 0.5828148601626565\n",
      "ndcg@30 0.5105746332956507\n",
      "ndcg@50 0.5015603664364795\n",
      "ndcg@all 0.6813901904594794\n",
      "\n",
      "epoch 9300\n",
      "loss_mse 4.642950273137103e-07\n",
      "loss_w_reg 0.0022219365928322077\n",
      "F1@10 0.27750359103676087\n",
      "percision@10 0.5750000000000001\n",
      "recall@10 0.20964644088094148\n",
      "F1@30 0.27402725372649844\n",
      "percision@30 0.3196666666666667\n",
      "recall@30 0.3018654421244392\n",
      "F1@50 0.24261338635054452\n",
      "percision@50 0.22899999999999987\n",
      "recall@50 0.3398969794571961\n",
      "ndcg@10 0.5825254712254249\n",
      "ndcg@30 0.5105469231205162\n",
      "ndcg@50 0.5014990005007472\n",
      "ndcg@all 0.6817133372899015\n",
      "\n",
      "epoch 9400\n",
      "loss_mse 4.644398359232582e-07\n",
      "loss_w_reg 0.0022217032965272665\n",
      "F1@10 0.27769776579404243\n",
      "percision@10 0.5760000000000001\n",
      "recall@10 0.20975396776266192\n",
      "F1@30 0.2746999501651643\n",
      "percision@30 0.32033333333333336\n",
      "recall@30 0.3025451656267434\n",
      "F1@50 0.24268553642269464\n",
      "percision@50 0.22899999999999993\n",
      "recall@50 0.34000490724369536\n",
      "ndcg@10 0.5835667686595208\n",
      "ndcg@30 0.5110443129863846\n",
      "ndcg@50 0.5017397275428728\n",
      "ndcg@all 0.682033688477267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch 9500\n",
      "loss_mse 4.637664972051425e-07\n",
      "loss_w_reg 0.0022203372791409492\n",
      "F1@10 0.27769776579404243\n",
      "percision@10 0.5760000000000001\n",
      "recall@10 0.20975396776266192\n",
      "F1@30 0.2749898052376281\n",
      "percision@30 0.3206666666666667\n",
      "recall@30 0.30280157588315365\n",
      "F1@50 0.2429976247965542\n",
      "percision@50 0.2291999999999999\n",
      "recall@50 0.3404328300153681\n",
      "ndcg@10 0.5837227731664335\n",
      "ndcg@30 0.5113307825040025\n",
      "ndcg@50 0.5020117374572217\n",
      "ndcg@all 0.6818659254594569\n",
      "\n",
      "epoch 9600\n",
      "loss_mse 4.654537804071879e-07\n",
      "loss_w_reg 0.002221371280029416\n",
      "F1@10 0.27732177680195436\n",
      "percision@10 0.5750000000000001\n",
      "recall@10 0.20951012539812988\n",
      "F1@30 0.27449377443664374\n",
      "percision@30 0.32\n",
      "recall@30 0.3023157018689777\n",
      "F1@50 0.24305406218632492\n",
      "percision@50 0.2291999999999999\n",
      "recall@50 0.3405735924787112\n",
      "ndcg@10 0.5832735411727799\n",
      "ndcg@30 0.5109200111182657\n",
      "ndcg@50 0.5019454304175515\n",
      "ndcg@all 0.6806168962926098\n",
      "\n",
      "epoch 9700\n",
      "loss_mse 4.6256249675025174e-07\n",
      "loss_w_reg 0.0022204697597771883\n",
      "F1@10 0.27732177680195436\n",
      "percision@10 0.5750000000000001\n",
      "recall@10 0.20951012539812988\n",
      "F1@30 0.2739767901781516\n",
      "percision@30 0.31933333333333336\n",
      "recall@30 0.30188998124813515\n",
      "F1@50 0.2428412962288781\n",
      "percision@50 0.22899999999999993\n",
      "recall@50 0.34034631975143853\n",
      "ndcg@10 0.5829651451197875\n",
      "ndcg@30 0.5102786522971381\n",
      "ndcg@50 0.5015618892797715\n",
      "ndcg@all 0.680500149438162\n",
      "\n",
      "epoch 9800\n",
      "loss_mse 4.6366548644982686e-07\n",
      "loss_w_reg 0.0022219230886548758\n",
      "F1@10 0.27755422232253546\n",
      "percision@10 0.5750000000000001\n",
      "recall@10 0.20970604376547686\n",
      "F1@30 0.2739767901781516\n",
      "percision@30 0.31933333333333336\n",
      "recall@30 0.30188998124813515\n",
      "F1@50 0.2426032009907829\n",
      "percision@50 0.22879999999999992\n",
      "recall@50 0.3400522021043797\n",
      "ndcg@10 0.5830214460189334\n",
      "ndcg@30 0.5102145894228018\n",
      "ndcg@50 0.5014386622229892\n",
      "ndcg@all 0.6801587621608758\n",
      "\n",
      "epoch 9900\n",
      "loss_mse 4.6423133426287677e-07\n",
      "loss_w_reg 0.002222334500402212\n",
      "F1@10 0.27735199785337467\n",
      "percision@10 0.5750000000000001\n",
      "recall@10 0.20955984493506752\n",
      "F1@30 0.2739767901781516\n",
      "percision@30 0.31933333333333336\n",
      "recall@30 0.30188998124813515\n",
      "F1@50 0.24210869549627742\n",
      "percision@50 0.22839999999999988\n",
      "recall@50 0.339400941600178\n",
      "ndcg@10 0.5830365031505131\n",
      "ndcg@30 0.5102232417032583\n",
      "ndcg@50 0.5011908306440326\n",
      "ndcg@all 0.6815147479950645\n"
     ]
    }
   ],
   "source": [
    "# setting\n",
    "lr = 0.1\n",
    "momentum = 0.999\n",
    "weight_decay = 0\n",
    "nesterov = False # True\n",
    "\n",
    "n_epoch = 10000\n",
    "\n",
    "w_sum_reg = 1e-2\n",
    "w_sum_reg_mul = 1\n",
    "w_clip_value = 0\n",
    "\n",
    "L1 = 1e-5\n",
    "\n",
    "verbose = True\n",
    "valid_epoch = 100\n",
    "\n",
    "model = LR(num_doc=train_size, num_words=word_vectors.shape[0]).to(device)\n",
    "model.train()\n",
    "\n",
    "word_vectors_tensor = torch.FloatTensor(word_vectors).to(device)\n",
    "    \n",
    "opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay, nesterov=nesterov)\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "results = []\n",
    "step = 0\n",
    "for epoch in tqdm(range(n_epoch)):    \n",
    "    loss_mse_his = []\n",
    "    loss_w_reg_his = []\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:\n",
    "        doc_embs, doc_w_sum, doc_ids = data\n",
    "        \n",
    "        doc_embs = doc_embs.to(device)\n",
    "        doc_w_sum = doc_w_sum.to(device)\n",
    "        doc_ids = doc_ids.to(device)\n",
    "        \n",
    "        w_reg = doc_w_sum * w_sum_reg_mul\n",
    "        # w_reg = (torch.ones(doc_embs.size(0), 1) * w_sum_reg_mul).to(device)\n",
    "        \n",
    "        # MSE loss\n",
    "        pred_doc_embs = model(doc_ids, word_vectors_tensor)     \n",
    "        loss_mse = criterion(pred_doc_embs, doc_embs)\n",
    "\n",
    "        pred_w_sum = torch.sum(model.emb(doc_ids), axis=1).view(-1, 1)\n",
    "        loss_w_reg = criterion(pred_w_sum, w_reg)\n",
    "        \n",
    "        loss_l1 = torch.sum(torch.abs(model.emb(doc_ids)))\n",
    "        loss = loss_mse + loss_w_reg * w_sum_reg + loss_l1 * L1\n",
    "        \n",
    "        # Model backwarding\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        loss_mse_his.append(loss_mse.item())\n",
    "        loss_w_reg_his.append(loss_w_reg.item())\n",
    "\n",
    "        for p in model.parameters():\n",
    "            p.data.clamp_(w_clip_value, float('inf'))\n",
    "\n",
    "        \n",
    "    if epoch % valid_epoch == 0:\n",
    "        res = {}\n",
    "        res['epoch'] = epoch\n",
    "        res['loss_mse'] = np.mean(loss_mse_his)\n",
    "        res['loss_w_reg'] = np.mean(loss_w_reg_his)\n",
    "        \n",
    "        res_ndcg = evaluate_Custom_Lasso(model, train_loader)\n",
    "        res.update(res_ndcg)\n",
    "        results.append(res)\n",
    "        \n",
    "        if verbose:\n",
    "            print()\n",
    "            for k, v in res.items():\n",
    "                print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49290d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss_mse</th>\n",
       "      <th>loss_w_reg</th>\n",
       "      <th>F1@10</th>\n",
       "      <th>percision@10</th>\n",
       "      <th>recall@10</th>\n",
       "      <th>F1@30</th>\n",
       "      <th>percision@30</th>\n",
       "      <th>recall@30</th>\n",
       "      <th>F1@50</th>\n",
       "      <th>percision@50</th>\n",
       "      <th>recall@50</th>\n",
       "      <th>ndcg@10</th>\n",
       "      <th>ndcg@30</th>\n",
       "      <th>ndcg@50</th>\n",
       "      <th>ndcg@all</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.655770e-02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.022739</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.016646</td>\n",
       "      <td>0.047511</td>\n",
       "      <td>0.060333</td>\n",
       "      <td>0.049709</td>\n",
       "      <td>0.076897</td>\n",
       "      <td>0.0774</td>\n",
       "      <td>0.099459</td>\n",
       "      <td>0.049470</td>\n",
       "      <td>0.057901</td>\n",
       "      <td>0.077670</td>\n",
       "      <td>0.356658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>8.220217e-05</td>\n",
       "      <td>0.002652</td>\n",
       "      <td>0.198686</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.143277</td>\n",
       "      <td>0.222474</td>\n",
       "      <td>0.274333</td>\n",
       "      <td>0.231215</td>\n",
       "      <td>0.213027</td>\n",
       "      <td>0.2082</td>\n",
       "      <td>0.281536</td>\n",
       "      <td>0.521408</td>\n",
       "      <td>0.453171</td>\n",
       "      <td>0.450578</td>\n",
       "      <td>0.644184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>1.664961e-05</td>\n",
       "      <td>0.002098</td>\n",
       "      <td>0.219616</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.160905</td>\n",
       "      <td>0.237151</td>\n",
       "      <td>0.286333</td>\n",
       "      <td>0.252111</td>\n",
       "      <td>0.225536</td>\n",
       "      <td>0.2176</td>\n",
       "      <td>0.303493</td>\n",
       "      <td>0.541894</td>\n",
       "      <td>0.471427</td>\n",
       "      <td>0.469708</td>\n",
       "      <td>0.653400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>8.199593e-06</td>\n",
       "      <td>0.002056</td>\n",
       "      <td>0.229954</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.169616</td>\n",
       "      <td>0.244720</td>\n",
       "      <td>0.291333</td>\n",
       "      <td>0.264835</td>\n",
       "      <td>0.231727</td>\n",
       "      <td>0.2228</td>\n",
       "      <td>0.315911</td>\n",
       "      <td>0.549071</td>\n",
       "      <td>0.478819</td>\n",
       "      <td>0.477913</td>\n",
       "      <td>0.654057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>4.486861e-06</td>\n",
       "      <td>0.002050</td>\n",
       "      <td>0.237516</td>\n",
       "      <td>0.521</td>\n",
       "      <td>0.176136</td>\n",
       "      <td>0.247526</td>\n",
       "      <td>0.293667</td>\n",
       "      <td>0.269039</td>\n",
       "      <td>0.234579</td>\n",
       "      <td>0.2244</td>\n",
       "      <td>0.321612</td>\n",
       "      <td>0.557201</td>\n",
       "      <td>0.484880</td>\n",
       "      <td>0.484384</td>\n",
       "      <td>0.655765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>2.704610e-06</td>\n",
       "      <td>0.002078</td>\n",
       "      <td>0.248206</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.186238</td>\n",
       "      <td>0.252684</td>\n",
       "      <td>0.299333</td>\n",
       "      <td>0.274879</td>\n",
       "      <td>0.236445</td>\n",
       "      <td>0.2262</td>\n",
       "      <td>0.325385</td>\n",
       "      <td>0.564945</td>\n",
       "      <td>0.490653</td>\n",
       "      <td>0.488342</td>\n",
       "      <td>0.657767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>2.824854e-06</td>\n",
       "      <td>0.002098</td>\n",
       "      <td>0.255020</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.191643</td>\n",
       "      <td>0.256726</td>\n",
       "      <td>0.303000</td>\n",
       "      <td>0.280561</td>\n",
       "      <td>0.236005</td>\n",
       "      <td>0.2256</td>\n",
       "      <td>0.325782</td>\n",
       "      <td>0.572093</td>\n",
       "      <td>0.495901</td>\n",
       "      <td>0.491953</td>\n",
       "      <td>0.660538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>1.796310e-06</td>\n",
       "      <td>0.002115</td>\n",
       "      <td>0.259312</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.195177</td>\n",
       "      <td>0.260709</td>\n",
       "      <td>0.306667</td>\n",
       "      <td>0.285326</td>\n",
       "      <td>0.237722</td>\n",
       "      <td>0.2270</td>\n",
       "      <td>0.329044</td>\n",
       "      <td>0.575137</td>\n",
       "      <td>0.499316</td>\n",
       "      <td>0.494191</td>\n",
       "      <td>0.662025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>1.451007e-06</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>0.264561</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.199458</td>\n",
       "      <td>0.263361</td>\n",
       "      <td>0.308667</td>\n",
       "      <td>0.289184</td>\n",
       "      <td>0.237820</td>\n",
       "      <td>0.2270</td>\n",
       "      <td>0.329399</td>\n",
       "      <td>0.578882</td>\n",
       "      <td>0.501032</td>\n",
       "      <td>0.494666</td>\n",
       "      <td>0.662810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>1.194433e-06</td>\n",
       "      <td>0.002142</td>\n",
       "      <td>0.269833</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.203857</td>\n",
       "      <td>0.264281</td>\n",
       "      <td>0.309333</td>\n",
       "      <td>0.290494</td>\n",
       "      <td>0.238776</td>\n",
       "      <td>0.2276</td>\n",
       "      <td>0.331316</td>\n",
       "      <td>0.581838</td>\n",
       "      <td>0.502053</td>\n",
       "      <td>0.495871</td>\n",
       "      <td>0.664681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>9.541553e-07</td>\n",
       "      <td>0.002152</td>\n",
       "      <td>0.271158</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.205188</td>\n",
       "      <td>0.265446</td>\n",
       "      <td>0.310667</td>\n",
       "      <td>0.292021</td>\n",
       "      <td>0.238849</td>\n",
       "      <td>0.2272</td>\n",
       "      <td>0.331836</td>\n",
       "      <td>0.583538</td>\n",
       "      <td>0.503598</td>\n",
       "      <td>0.497208</td>\n",
       "      <td>0.666418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>8.000968e-07</td>\n",
       "      <td>0.002162</td>\n",
       "      <td>0.273911</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.207261</td>\n",
       "      <td>0.265875</td>\n",
       "      <td>0.311000</td>\n",
       "      <td>0.292659</td>\n",
       "      <td>0.239552</td>\n",
       "      <td>0.2274</td>\n",
       "      <td>0.333774</td>\n",
       "      <td>0.586247</td>\n",
       "      <td>0.503981</td>\n",
       "      <td>0.497776</td>\n",
       "      <td>0.667856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>7.344838e-07</td>\n",
       "      <td>0.002170</td>\n",
       "      <td>0.274405</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.207629</td>\n",
       "      <td>0.267403</td>\n",
       "      <td>0.312000</td>\n",
       "      <td>0.294748</td>\n",
       "      <td>0.239469</td>\n",
       "      <td>0.2270</td>\n",
       "      <td>0.333998</td>\n",
       "      <td>0.585764</td>\n",
       "      <td>0.504379</td>\n",
       "      <td>0.497535</td>\n",
       "      <td>0.668338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>6.402593e-07</td>\n",
       "      <td>0.002173</td>\n",
       "      <td>0.273458</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.206926</td>\n",
       "      <td>0.269093</td>\n",
       "      <td>0.314333</td>\n",
       "      <td>0.296289</td>\n",
       "      <td>0.238569</td>\n",
       "      <td>0.2264</td>\n",
       "      <td>0.332721</td>\n",
       "      <td>0.585282</td>\n",
       "      <td>0.505808</td>\n",
       "      <td>0.497302</td>\n",
       "      <td>0.669356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>6.085634e-07</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>0.273718</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.207240</td>\n",
       "      <td>0.270389</td>\n",
       "      <td>0.316333</td>\n",
       "      <td>0.297589</td>\n",
       "      <td>0.238834</td>\n",
       "      <td>0.2264</td>\n",
       "      <td>0.333547</td>\n",
       "      <td>0.585302</td>\n",
       "      <td>0.506690</td>\n",
       "      <td>0.497535</td>\n",
       "      <td>0.670993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>6.140008e-07</td>\n",
       "      <td>0.002186</td>\n",
       "      <td>0.272726</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.206602</td>\n",
       "      <td>0.270787</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.298033</td>\n",
       "      <td>0.238276</td>\n",
       "      <td>0.2260</td>\n",
       "      <td>0.332917</td>\n",
       "      <td>0.583749</td>\n",
       "      <td>0.506469</td>\n",
       "      <td>0.497149</td>\n",
       "      <td>0.671139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>6.518250e-07</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.273621</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.207219</td>\n",
       "      <td>0.271346</td>\n",
       "      <td>0.317667</td>\n",
       "      <td>0.298419</td>\n",
       "      <td>0.238824</td>\n",
       "      <td>0.2264</td>\n",
       "      <td>0.334219</td>\n",
       "      <td>0.584481</td>\n",
       "      <td>0.506940</td>\n",
       "      <td>0.497711</td>\n",
       "      <td>0.672521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700</th>\n",
       "      <td>6.516003e-07</td>\n",
       "      <td>0.002194</td>\n",
       "      <td>0.274147</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.207576</td>\n",
       "      <td>0.271200</td>\n",
       "      <td>0.317333</td>\n",
       "      <td>0.298305</td>\n",
       "      <td>0.238044</td>\n",
       "      <td>0.2258</td>\n",
       "      <td>0.333271</td>\n",
       "      <td>0.586458</td>\n",
       "      <td>0.508701</td>\n",
       "      <td>0.498852</td>\n",
       "      <td>0.674905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>6.133333e-07</td>\n",
       "      <td>0.002199</td>\n",
       "      <td>0.274792</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.208052</td>\n",
       "      <td>0.271974</td>\n",
       "      <td>0.317667</td>\n",
       "      <td>0.299805</td>\n",
       "      <td>0.239160</td>\n",
       "      <td>0.2268</td>\n",
       "      <td>0.334475</td>\n",
       "      <td>0.587115</td>\n",
       "      <td>0.509421</td>\n",
       "      <td>0.499591</td>\n",
       "      <td>0.675744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>5.879484e-07</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.273866</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.207445</td>\n",
       "      <td>0.271700</td>\n",
       "      <td>0.317667</td>\n",
       "      <td>0.299316</td>\n",
       "      <td>0.239586</td>\n",
       "      <td>0.2270</td>\n",
       "      <td>0.335166</td>\n",
       "      <td>0.585143</td>\n",
       "      <td>0.508768</td>\n",
       "      <td>0.499256</td>\n",
       "      <td>0.675513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>5.737636e-07</td>\n",
       "      <td>0.002202</td>\n",
       "      <td>0.274294</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.207711</td>\n",
       "      <td>0.271332</td>\n",
       "      <td>0.317333</td>\n",
       "      <td>0.298951</td>\n",
       "      <td>0.239342</td>\n",
       "      <td>0.2268</td>\n",
       "      <td>0.334694</td>\n",
       "      <td>0.584108</td>\n",
       "      <td>0.508213</td>\n",
       "      <td>0.498765</td>\n",
       "      <td>0.675849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2100</th>\n",
       "      <td>5.477900e-07</td>\n",
       "      <td>0.002204</td>\n",
       "      <td>0.273877</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.207556</td>\n",
       "      <td>0.272306</td>\n",
       "      <td>0.318333</td>\n",
       "      <td>0.299913</td>\n",
       "      <td>0.239535</td>\n",
       "      <td>0.2270</td>\n",
       "      <td>0.334864</td>\n",
       "      <td>0.583518</td>\n",
       "      <td>0.509068</td>\n",
       "      <td>0.498997</td>\n",
       "      <td>0.676259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>5.394116e-07</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>0.273693</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.207514</td>\n",
       "      <td>0.272417</td>\n",
       "      <td>0.318333</td>\n",
       "      <td>0.300065</td>\n",
       "      <td>0.239040</td>\n",
       "      <td>0.2266</td>\n",
       "      <td>0.334114</td>\n",
       "      <td>0.582448</td>\n",
       "      <td>0.509428</td>\n",
       "      <td>0.498886</td>\n",
       "      <td>0.676408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2300</th>\n",
       "      <td>5.173451e-07</td>\n",
       "      <td>0.002206</td>\n",
       "      <td>0.274133</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.207830</td>\n",
       "      <td>0.273134</td>\n",
       "      <td>0.319000</td>\n",
       "      <td>0.300874</td>\n",
       "      <td>0.238547</td>\n",
       "      <td>0.2262</td>\n",
       "      <td>0.333504</td>\n",
       "      <td>0.582317</td>\n",
       "      <td>0.509560</td>\n",
       "      <td>0.498322</td>\n",
       "      <td>0.676253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2400</th>\n",
       "      <td>4.924631e-07</td>\n",
       "      <td>0.002208</td>\n",
       "      <td>0.274589</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.208147</td>\n",
       "      <td>0.273503</td>\n",
       "      <td>0.319667</td>\n",
       "      <td>0.301130</td>\n",
       "      <td>0.238871</td>\n",
       "      <td>0.2264</td>\n",
       "      <td>0.333861</td>\n",
       "      <td>0.582273</td>\n",
       "      <td>0.509652</td>\n",
       "      <td>0.498485</td>\n",
       "      <td>0.676392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2500</th>\n",
       "      <td>4.737288e-07</td>\n",
       "      <td>0.002208</td>\n",
       "      <td>0.273652</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.207450</td>\n",
       "      <td>0.274618</td>\n",
       "      <td>0.320667</td>\n",
       "      <td>0.302505</td>\n",
       "      <td>0.238653</td>\n",
       "      <td>0.2258</td>\n",
       "      <td>0.334198</td>\n",
       "      <td>0.581442</td>\n",
       "      <td>0.510182</td>\n",
       "      <td>0.498445</td>\n",
       "      <td>0.676831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2600</th>\n",
       "      <td>4.737327e-07</td>\n",
       "      <td>0.002211</td>\n",
       "      <td>0.274186</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.207822</td>\n",
       "      <td>0.275184</td>\n",
       "      <td>0.321333</td>\n",
       "      <td>0.303012</td>\n",
       "      <td>0.239563</td>\n",
       "      <td>0.2266</td>\n",
       "      <td>0.335431</td>\n",
       "      <td>0.581421</td>\n",
       "      <td>0.510259</td>\n",
       "      <td>0.498901</td>\n",
       "      <td>0.677122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2700</th>\n",
       "      <td>4.805086e-07</td>\n",
       "      <td>0.002212</td>\n",
       "      <td>0.274663</td>\n",
       "      <td>0.567</td>\n",
       "      <td>0.208093</td>\n",
       "      <td>0.275394</td>\n",
       "      <td>0.321667</td>\n",
       "      <td>0.303209</td>\n",
       "      <td>0.238971</td>\n",
       "      <td>0.2260</td>\n",
       "      <td>0.334748</td>\n",
       "      <td>0.581830</td>\n",
       "      <td>0.510510</td>\n",
       "      <td>0.498490</td>\n",
       "      <td>0.677992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2800</th>\n",
       "      <td>4.753454e-07</td>\n",
       "      <td>0.002210</td>\n",
       "      <td>0.275218</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.208478</td>\n",
       "      <td>0.274601</td>\n",
       "      <td>0.320667</td>\n",
       "      <td>0.302414</td>\n",
       "      <td>0.238974</td>\n",
       "      <td>0.2260</td>\n",
       "      <td>0.334870</td>\n",
       "      <td>0.581332</td>\n",
       "      <td>0.509741</td>\n",
       "      <td>0.498162</td>\n",
       "      <td>0.677573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2900</th>\n",
       "      <td>4.679090e-07</td>\n",
       "      <td>0.002211</td>\n",
       "      <td>0.275790</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.208878</td>\n",
       "      <td>0.274809</td>\n",
       "      <td>0.320667</td>\n",
       "      <td>0.302713</td>\n",
       "      <td>0.238974</td>\n",
       "      <td>0.2260</td>\n",
       "      <td>0.334870</td>\n",
       "      <td>0.581837</td>\n",
       "      <td>0.509904</td>\n",
       "      <td>0.498319</td>\n",
       "      <td>0.677569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>4.671751e-07</td>\n",
       "      <td>0.002213</td>\n",
       "      <td>0.276112</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.209070</td>\n",
       "      <td>0.275002</td>\n",
       "      <td>0.320667</td>\n",
       "      <td>0.302975</td>\n",
       "      <td>0.238932</td>\n",
       "      <td>0.2260</td>\n",
       "      <td>0.334726</td>\n",
       "      <td>0.580907</td>\n",
       "      <td>0.509651</td>\n",
       "      <td>0.497607</td>\n",
       "      <td>0.677582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3100</th>\n",
       "      <td>4.608034e-07</td>\n",
       "      <td>0.002213</td>\n",
       "      <td>0.276112</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.209070</td>\n",
       "      <td>0.275369</td>\n",
       "      <td>0.320667</td>\n",
       "      <td>0.303409</td>\n",
       "      <td>0.239359</td>\n",
       "      <td>0.2264</td>\n",
       "      <td>0.335189</td>\n",
       "      <td>0.580687</td>\n",
       "      <td>0.509474</td>\n",
       "      <td>0.497741</td>\n",
       "      <td>0.677522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3200</th>\n",
       "      <td>4.648329e-07</td>\n",
       "      <td>0.002215</td>\n",
       "      <td>0.276112</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.209070</td>\n",
       "      <td>0.274923</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.303072</td>\n",
       "      <td>0.240074</td>\n",
       "      <td>0.2268</td>\n",
       "      <td>0.336411</td>\n",
       "      <td>0.580577</td>\n",
       "      <td>0.509291</td>\n",
       "      <td>0.498353</td>\n",
       "      <td>0.677099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3300</th>\n",
       "      <td>4.700292e-07</td>\n",
       "      <td>0.002213</td>\n",
       "      <td>0.276567</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.209364</td>\n",
       "      <td>0.274138</td>\n",
       "      <td>0.319000</td>\n",
       "      <td>0.302376</td>\n",
       "      <td>0.239790</td>\n",
       "      <td>0.2266</td>\n",
       "      <td>0.336065</td>\n",
       "      <td>0.580821</td>\n",
       "      <td>0.509189</td>\n",
       "      <td>0.498043</td>\n",
       "      <td>0.678314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3400</th>\n",
       "      <td>4.718590e-07</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>0.275826</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.208776</td>\n",
       "      <td>0.274734</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.302833</td>\n",
       "      <td>0.240076</td>\n",
       "      <td>0.2268</td>\n",
       "      <td>0.336506</td>\n",
       "      <td>0.580331</td>\n",
       "      <td>0.509333</td>\n",
       "      <td>0.498281</td>\n",
       "      <td>0.677859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3500</th>\n",
       "      <td>4.731650e-07</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.275764</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.208717</td>\n",
       "      <td>0.274587</td>\n",
       "      <td>0.319667</td>\n",
       "      <td>0.302739</td>\n",
       "      <td>0.240703</td>\n",
       "      <td>0.2272</td>\n",
       "      <td>0.337561</td>\n",
       "      <td>0.579724</td>\n",
       "      <td>0.509109</td>\n",
       "      <td>0.498769</td>\n",
       "      <td>0.678337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3600</th>\n",
       "      <td>4.732300e-07</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>0.275775</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.208726</td>\n",
       "      <td>0.274047</td>\n",
       "      <td>0.319000</td>\n",
       "      <td>0.302234</td>\n",
       "      <td>0.240734</td>\n",
       "      <td>0.2272</td>\n",
       "      <td>0.337593</td>\n",
       "      <td>0.579459</td>\n",
       "      <td>0.508832</td>\n",
       "      <td>0.498475</td>\n",
       "      <td>0.678384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3700</th>\n",
       "      <td>4.755923e-07</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>0.275775</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.208726</td>\n",
       "      <td>0.274354</td>\n",
       "      <td>0.319333</td>\n",
       "      <td>0.302520</td>\n",
       "      <td>0.241032</td>\n",
       "      <td>0.2274</td>\n",
       "      <td>0.338101</td>\n",
       "      <td>0.579553</td>\n",
       "      <td>0.509056</td>\n",
       "      <td>0.498466</td>\n",
       "      <td>0.677834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3800</th>\n",
       "      <td>4.745781e-07</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.276275</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.209059</td>\n",
       "      <td>0.273775</td>\n",
       "      <td>0.319000</td>\n",
       "      <td>0.301761</td>\n",
       "      <td>0.240839</td>\n",
       "      <td>0.2272</td>\n",
       "      <td>0.337797</td>\n",
       "      <td>0.579916</td>\n",
       "      <td>0.508577</td>\n",
       "      <td>0.498326</td>\n",
       "      <td>0.678282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3900</th>\n",
       "      <td>4.764118e-07</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>0.276275</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.209059</td>\n",
       "      <td>0.274078</td>\n",
       "      <td>0.319333</td>\n",
       "      <td>0.302029</td>\n",
       "      <td>0.240969</td>\n",
       "      <td>0.2272</td>\n",
       "      <td>0.338045</td>\n",
       "      <td>0.580094</td>\n",
       "      <td>0.508845</td>\n",
       "      <td>0.498566</td>\n",
       "      <td>0.678573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>4.796612e-07</td>\n",
       "      <td>0.002218</td>\n",
       "      <td>0.276675</td>\n",
       "      <td>0.572</td>\n",
       "      <td>0.209309</td>\n",
       "      <td>0.273926</td>\n",
       "      <td>0.319667</td>\n",
       "      <td>0.301816</td>\n",
       "      <td>0.241442</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.338411</td>\n",
       "      <td>0.580320</td>\n",
       "      <td>0.508605</td>\n",
       "      <td>0.498749</td>\n",
       "      <td>0.678452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4100</th>\n",
       "      <td>4.783367e-07</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.276907</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.209440</td>\n",
       "      <td>0.274822</td>\n",
       "      <td>0.320333</td>\n",
       "      <td>0.302699</td>\n",
       "      <td>0.241316</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.338116</td>\n",
       "      <td>0.580682</td>\n",
       "      <td>0.509190</td>\n",
       "      <td>0.498641</td>\n",
       "      <td>0.678498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4200</th>\n",
       "      <td>4.751982e-07</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.277448</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.209811</td>\n",
       "      <td>0.275340</td>\n",
       "      <td>0.321000</td>\n",
       "      <td>0.303088</td>\n",
       "      <td>0.241348</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.338159</td>\n",
       "      <td>0.581093</td>\n",
       "      <td>0.509438</td>\n",
       "      <td>0.498655</td>\n",
       "      <td>0.678856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4300</th>\n",
       "      <td>4.671736e-07</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.277902</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.210105</td>\n",
       "      <td>0.275100</td>\n",
       "      <td>0.321000</td>\n",
       "      <td>0.302568</td>\n",
       "      <td>0.241772</td>\n",
       "      <td>0.2284</td>\n",
       "      <td>0.338653</td>\n",
       "      <td>0.581791</td>\n",
       "      <td>0.509324</td>\n",
       "      <td>0.499217</td>\n",
       "      <td>0.677889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4400</th>\n",
       "      <td>4.662097e-07</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.278471</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.210437</td>\n",
       "      <td>0.275525</td>\n",
       "      <td>0.321333</td>\n",
       "      <td>0.303157</td>\n",
       "      <td>0.241231</td>\n",
       "      <td>0.2278</td>\n",
       "      <td>0.338183</td>\n",
       "      <td>0.584363</td>\n",
       "      <td>0.510345</td>\n",
       "      <td>0.499623</td>\n",
       "      <td>0.679093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4500</th>\n",
       "      <td>4.671860e-07</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.278971</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.210771</td>\n",
       "      <td>0.275252</td>\n",
       "      <td>0.320667</td>\n",
       "      <td>0.303115</td>\n",
       "      <td>0.241055</td>\n",
       "      <td>0.2274</td>\n",
       "      <td>0.338138</td>\n",
       "      <td>0.584716</td>\n",
       "      <td>0.510394</td>\n",
       "      <td>0.499719</td>\n",
       "      <td>0.679997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>4.654790e-07</td>\n",
       "      <td>0.002220</td>\n",
       "      <td>0.279082</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.210842</td>\n",
       "      <td>0.275107</td>\n",
       "      <td>0.320667</td>\n",
       "      <td>0.302951</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.338393</td>\n",
       "      <td>0.584144</td>\n",
       "      <td>0.509774</td>\n",
       "      <td>0.499204</td>\n",
       "      <td>0.679624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4700</th>\n",
       "      <td>4.640318e-07</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.278978</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.210787</td>\n",
       "      <td>0.274825</td>\n",
       "      <td>0.320333</td>\n",
       "      <td>0.302708</td>\n",
       "      <td>0.241752</td>\n",
       "      <td>0.2284</td>\n",
       "      <td>0.338697</td>\n",
       "      <td>0.583831</td>\n",
       "      <td>0.509552</td>\n",
       "      <td>0.499333</td>\n",
       "      <td>0.678949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4800</th>\n",
       "      <td>4.653959e-07</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.278978</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.210787</td>\n",
       "      <td>0.274207</td>\n",
       "      <td>0.319667</td>\n",
       "      <td>0.302129</td>\n",
       "      <td>0.241900</td>\n",
       "      <td>0.2286</td>\n",
       "      <td>0.338814</td>\n",
       "      <td>0.583809</td>\n",
       "      <td>0.509231</td>\n",
       "      <td>0.499345</td>\n",
       "      <td>0.679194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4900</th>\n",
       "      <td>4.642166e-07</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>0.279138</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.210874</td>\n",
       "      <td>0.274456</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>0.241535</td>\n",
       "      <td>0.2282</td>\n",
       "      <td>0.338470</td>\n",
       "      <td>0.583915</td>\n",
       "      <td>0.509361</td>\n",
       "      <td>0.499226</td>\n",
       "      <td>0.679261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>4.628825e-07</td>\n",
       "      <td>0.002221</td>\n",
       "      <td>0.279138</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.210874</td>\n",
       "      <td>0.274367</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.302153</td>\n",
       "      <td>0.241240</td>\n",
       "      <td>0.2278</td>\n",
       "      <td>0.338233</td>\n",
       "      <td>0.583796</td>\n",
       "      <td>0.509185</td>\n",
       "      <td>0.499022</td>\n",
       "      <td>0.679054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5100</th>\n",
       "      <td>4.639168e-07</td>\n",
       "      <td>0.002220</td>\n",
       "      <td>0.278821</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.210686</td>\n",
       "      <td>0.274348</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.302107</td>\n",
       "      <td>0.241361</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.338402</td>\n",
       "      <td>0.583097</td>\n",
       "      <td>0.509206</td>\n",
       "      <td>0.499124</td>\n",
       "      <td>0.679638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5200</th>\n",
       "      <td>4.651729e-07</td>\n",
       "      <td>0.002221</td>\n",
       "      <td>0.278107</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.210130</td>\n",
       "      <td>0.274699</td>\n",
       "      <td>0.320333</td>\n",
       "      <td>0.302477</td>\n",
       "      <td>0.241372</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.338433</td>\n",
       "      <td>0.583283</td>\n",
       "      <td>0.509793</td>\n",
       "      <td>0.499582</td>\n",
       "      <td>0.679836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5300</th>\n",
       "      <td>4.652367e-07</td>\n",
       "      <td>0.002220</td>\n",
       "      <td>0.277662</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.209844</td>\n",
       "      <td>0.274750</td>\n",
       "      <td>0.320333</td>\n",
       "      <td>0.302549</td>\n",
       "      <td>0.241505</td>\n",
       "      <td>0.2282</td>\n",
       "      <td>0.338531</td>\n",
       "      <td>0.582875</td>\n",
       "      <td>0.509877</td>\n",
       "      <td>0.499565</td>\n",
       "      <td>0.679440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5400</th>\n",
       "      <td>4.652931e-07</td>\n",
       "      <td>0.002218</td>\n",
       "      <td>0.277662</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.209844</td>\n",
       "      <td>0.274043</td>\n",
       "      <td>0.319667</td>\n",
       "      <td>0.301716</td>\n",
       "      <td>0.242332</td>\n",
       "      <td>0.2290</td>\n",
       "      <td>0.339437</td>\n",
       "      <td>0.582931</td>\n",
       "      <td>0.509631</td>\n",
       "      <td>0.499993</td>\n",
       "      <td>0.679628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5500</th>\n",
       "      <td>4.642663e-07</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.277662</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.209844</td>\n",
       "      <td>0.274496</td>\n",
       "      <td>0.320333</td>\n",
       "      <td>0.302060</td>\n",
       "      <td>0.242233</td>\n",
       "      <td>0.2290</td>\n",
       "      <td>0.339339</td>\n",
       "      <td>0.582805</td>\n",
       "      <td>0.509705</td>\n",
       "      <td>0.499890</td>\n",
       "      <td>0.679465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5600</th>\n",
       "      <td>4.664028e-07</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.278068</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.210117</td>\n",
       "      <td>0.274152</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.301703</td>\n",
       "      <td>0.242061</td>\n",
       "      <td>0.2288</td>\n",
       "      <td>0.339187</td>\n",
       "      <td>0.584516</td>\n",
       "      <td>0.510753</td>\n",
       "      <td>0.501033</td>\n",
       "      <td>0.680250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5700</th>\n",
       "      <td>4.646378e-07</td>\n",
       "      <td>0.002220</td>\n",
       "      <td>0.278068</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.210117</td>\n",
       "      <td>0.274307</td>\n",
       "      <td>0.320333</td>\n",
       "      <td>0.301804</td>\n",
       "      <td>0.241985</td>\n",
       "      <td>0.2288</td>\n",
       "      <td>0.339073</td>\n",
       "      <td>0.583283</td>\n",
       "      <td>0.510048</td>\n",
       "      <td>0.500095</td>\n",
       "      <td>0.680030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5800</th>\n",
       "      <td>4.634249e-07</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.278068</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.210117</td>\n",
       "      <td>0.274458</td>\n",
       "      <td>0.320333</td>\n",
       "      <td>0.301974</td>\n",
       "      <td>0.242198</td>\n",
       "      <td>0.2290</td>\n",
       "      <td>0.339300</td>\n",
       "      <td>0.583432</td>\n",
       "      <td>0.510145</td>\n",
       "      <td>0.500302</td>\n",
       "      <td>0.680476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5900</th>\n",
       "      <td>4.644359e-07</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.278419</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.210330</td>\n",
       "      <td>0.273850</td>\n",
       "      <td>0.319333</td>\n",
       "      <td>0.301529</td>\n",
       "      <td>0.242486</td>\n",
       "      <td>0.2294</td>\n",
       "      <td>0.339525</td>\n",
       "      <td>0.583651</td>\n",
       "      <td>0.509817</td>\n",
       "      <td>0.500299</td>\n",
       "      <td>0.680250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6000</th>\n",
       "      <td>4.640865e-07</td>\n",
       "      <td>0.002221</td>\n",
       "      <td>0.278652</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.210461</td>\n",
       "      <td>0.273623</td>\n",
       "      <td>0.319000</td>\n",
       "      <td>0.301356</td>\n",
       "      <td>0.242186</td>\n",
       "      <td>0.2290</td>\n",
       "      <td>0.339285</td>\n",
       "      <td>0.583769</td>\n",
       "      <td>0.509663</td>\n",
       "      <td>0.500108</td>\n",
       "      <td>0.679948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6100</th>\n",
       "      <td>4.633889e-07</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.278652</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.210461</td>\n",
       "      <td>0.273852</td>\n",
       "      <td>0.319333</td>\n",
       "      <td>0.301530</td>\n",
       "      <td>0.242026</td>\n",
       "      <td>0.2290</td>\n",
       "      <td>0.339018</td>\n",
       "      <td>0.583753</td>\n",
       "      <td>0.509683</td>\n",
       "      <td>0.500028</td>\n",
       "      <td>0.680306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6200</th>\n",
       "      <td>4.645651e-07</td>\n",
       "      <td>0.002221</td>\n",
       "      <td>0.278652</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.210461</td>\n",
       "      <td>0.273426</td>\n",
       "      <td>0.319000</td>\n",
       "      <td>0.300942</td>\n",
       "      <td>0.241994</td>\n",
       "      <td>0.2290</td>\n",
       "      <td>0.338968</td>\n",
       "      <td>0.583785</td>\n",
       "      <td>0.509499</td>\n",
       "      <td>0.500021</td>\n",
       "      <td>0.679419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6300</th>\n",
       "      <td>4.638238e-07</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.278652</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.210461</td>\n",
       "      <td>0.273426</td>\n",
       "      <td>0.319000</td>\n",
       "      <td>0.300942</td>\n",
       "      <td>0.241953</td>\n",
       "      <td>0.2290</td>\n",
       "      <td>0.338903</td>\n",
       "      <td>0.583660</td>\n",
       "      <td>0.509411</td>\n",
       "      <td>0.499935</td>\n",
       "      <td>0.679826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6400</th>\n",
       "      <td>4.648369e-07</td>\n",
       "      <td>0.002220</td>\n",
       "      <td>0.278860</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.210578</td>\n",
       "      <td>0.273554</td>\n",
       "      <td>0.319333</td>\n",
       "      <td>0.301035</td>\n",
       "      <td>0.242143</td>\n",
       "      <td>0.2292</td>\n",
       "      <td>0.339084</td>\n",
       "      <td>0.584322</td>\n",
       "      <td>0.509679</td>\n",
       "      <td>0.500143</td>\n",
       "      <td>0.679471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6500</th>\n",
       "      <td>4.651010e-07</td>\n",
       "      <td>0.002220</td>\n",
       "      <td>0.278860</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.210578</td>\n",
       "      <td>0.274210</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.301680</td>\n",
       "      <td>0.242143</td>\n",
       "      <td>0.2292</td>\n",
       "      <td>0.339084</td>\n",
       "      <td>0.584627</td>\n",
       "      <td>0.510114</td>\n",
       "      <td>0.500289</td>\n",
       "      <td>0.680274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6600</th>\n",
       "      <td>4.659371e-07</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.278860</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.210578</td>\n",
       "      <td>0.274305</td>\n",
       "      <td>0.320333</td>\n",
       "      <td>0.301736</td>\n",
       "      <td>0.242495</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.339482</td>\n",
       "      <td>0.584655</td>\n",
       "      <td>0.510367</td>\n",
       "      <td>0.500423</td>\n",
       "      <td>0.680960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6700</th>\n",
       "      <td>4.655037e-07</td>\n",
       "      <td>0.002221</td>\n",
       "      <td>0.279018</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.210663</td>\n",
       "      <td>0.274305</td>\n",
       "      <td>0.320333</td>\n",
       "      <td>0.301736</td>\n",
       "      <td>0.242446</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.339409</td>\n",
       "      <td>0.584097</td>\n",
       "      <td>0.509954</td>\n",
       "      <td>0.499965</td>\n",
       "      <td>0.680122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6800</th>\n",
       "      <td>4.652630e-07</td>\n",
       "      <td>0.002223</td>\n",
       "      <td>0.278764</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.210452</td>\n",
       "      <td>0.274116</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.301594</td>\n",
       "      <td>0.242422</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.339330</td>\n",
       "      <td>0.583982</td>\n",
       "      <td>0.509902</td>\n",
       "      <td>0.499977</td>\n",
       "      <td>0.679943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6900</th>\n",
       "      <td>4.646449e-07</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.278531</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.210320</td>\n",
       "      <td>0.273878</td>\n",
       "      <td>0.319667</td>\n",
       "      <td>0.301409</td>\n",
       "      <td>0.242775</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>0.339666</td>\n",
       "      <td>0.583654</td>\n",
       "      <td>0.509829</td>\n",
       "      <td>0.500162</td>\n",
       "      <td>0.679664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7000</th>\n",
       "      <td>4.642543e-07</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.278531</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.210320</td>\n",
       "      <td>0.274138</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.301646</td>\n",
       "      <td>0.242873</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>0.339853</td>\n",
       "      <td>0.583970</td>\n",
       "      <td>0.510180</td>\n",
       "      <td>0.500416</td>\n",
       "      <td>0.680198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7100</th>\n",
       "      <td>4.651163e-07</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.278531</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.210320</td>\n",
       "      <td>0.274241</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.301922</td>\n",
       "      <td>0.242839</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>0.339805</td>\n",
       "      <td>0.584022</td>\n",
       "      <td>0.509917</td>\n",
       "      <td>0.500353</td>\n",
       "      <td>0.680550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7200</th>\n",
       "      <td>4.644556e-07</td>\n",
       "      <td>0.002223</td>\n",
       "      <td>0.278531</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.210320</td>\n",
       "      <td>0.274241</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.301922</td>\n",
       "      <td>0.242529</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.339550</td>\n",
       "      <td>0.584126</td>\n",
       "      <td>0.509978</td>\n",
       "      <td>0.500337</td>\n",
       "      <td>0.680145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7300</th>\n",
       "      <td>4.641765e-07</td>\n",
       "      <td>0.002223</td>\n",
       "      <td>0.278109</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.210032</td>\n",
       "      <td>0.274000</td>\n",
       "      <td>0.319667</td>\n",
       "      <td>0.301733</td>\n",
       "      <td>0.242632</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.339669</td>\n",
       "      <td>0.583669</td>\n",
       "      <td>0.509991</td>\n",
       "      <td>0.500573</td>\n",
       "      <td>0.680218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7400</th>\n",
       "      <td>4.642516e-07</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.278109</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.210032</td>\n",
       "      <td>0.274186</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.301901</td>\n",
       "      <td>0.242853</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>0.339828</td>\n",
       "      <td>0.583714</td>\n",
       "      <td>0.510279</td>\n",
       "      <td>0.500670</td>\n",
       "      <td>0.679896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7500</th>\n",
       "      <td>4.626663e-07</td>\n",
       "      <td>0.002221</td>\n",
       "      <td>0.278109</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.210032</td>\n",
       "      <td>0.274039</td>\n",
       "      <td>0.319667</td>\n",
       "      <td>0.301807</td>\n",
       "      <td>0.242579</td>\n",
       "      <td>0.2294</td>\n",
       "      <td>0.339639</td>\n",
       "      <td>0.583660</td>\n",
       "      <td>0.510179</td>\n",
       "      <td>0.500588</td>\n",
       "      <td>0.680696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7600</th>\n",
       "      <td>4.644307e-07</td>\n",
       "      <td>0.002221</td>\n",
       "      <td>0.277949</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.209945</td>\n",
       "      <td>0.274329</td>\n",
       "      <td>0.319667</td>\n",
       "      <td>0.302310</td>\n",
       "      <td>0.242579</td>\n",
       "      <td>0.2294</td>\n",
       "      <td>0.339639</td>\n",
       "      <td>0.583680</td>\n",
       "      <td>0.510443</td>\n",
       "      <td>0.500668</td>\n",
       "      <td>0.680729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7700</th>\n",
       "      <td>4.633552e-07</td>\n",
       "      <td>0.002221</td>\n",
       "      <td>0.277596</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.209687</td>\n",
       "      <td>0.274150</td>\n",
       "      <td>0.319333</td>\n",
       "      <td>0.302188</td>\n",
       "      <td>0.242817</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.339933</td>\n",
       "      <td>0.583364</td>\n",
       "      <td>0.510366</td>\n",
       "      <td>0.500746</td>\n",
       "      <td>0.680512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7800</th>\n",
       "      <td>4.631519e-07</td>\n",
       "      <td>0.002221</td>\n",
       "      <td>0.278274</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.210154</td>\n",
       "      <td>0.274432</td>\n",
       "      <td>0.319667</td>\n",
       "      <td>0.302432</td>\n",
       "      <td>0.242445</td>\n",
       "      <td>0.2292</td>\n",
       "      <td>0.339585</td>\n",
       "      <td>0.583568</td>\n",
       "      <td>0.510428</td>\n",
       "      <td>0.500669</td>\n",
       "      <td>0.679769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7900</th>\n",
       "      <td>4.634727e-07</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.278470</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.210262</td>\n",
       "      <td>0.274283</td>\n",
       "      <td>0.319667</td>\n",
       "      <td>0.302315</td>\n",
       "      <td>0.242543</td>\n",
       "      <td>0.2292</td>\n",
       "      <td>0.339772</td>\n",
       "      <td>0.583644</td>\n",
       "      <td>0.510046</td>\n",
       "      <td>0.500472</td>\n",
       "      <td>0.679691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8000</th>\n",
       "      <td>4.628945e-07</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.278470</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.210262</td>\n",
       "      <td>0.274027</td>\n",
       "      <td>0.319333</td>\n",
       "      <td>0.302106</td>\n",
       "      <td>0.242630</td>\n",
       "      <td>0.2294</td>\n",
       "      <td>0.339827</td>\n",
       "      <td>0.583187</td>\n",
       "      <td>0.509530</td>\n",
       "      <td>0.500283</td>\n",
       "      <td>0.680022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8100</th>\n",
       "      <td>4.637332e-07</td>\n",
       "      <td>0.002223</td>\n",
       "      <td>0.278470</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.210262</td>\n",
       "      <td>0.274338</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.302309</td>\n",
       "      <td>0.242886</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.340184</td>\n",
       "      <td>0.583154</td>\n",
       "      <td>0.509716</td>\n",
       "      <td>0.500394</td>\n",
       "      <td>0.680242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8200</th>\n",
       "      <td>4.639883e-07</td>\n",
       "      <td>0.002221</td>\n",
       "      <td>0.278708</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.210434</td>\n",
       "      <td>0.273809</td>\n",
       "      <td>0.319667</td>\n",
       "      <td>0.301568</td>\n",
       "      <td>0.242636</td>\n",
       "      <td>0.2294</td>\n",
       "      <td>0.339851</td>\n",
       "      <td>0.582069</td>\n",
       "      <td>0.509441</td>\n",
       "      <td>0.500209</td>\n",
       "      <td>0.679630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8300</th>\n",
       "      <td>4.644254e-07</td>\n",
       "      <td>0.002221</td>\n",
       "      <td>0.278181</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.210077</td>\n",
       "      <td>0.273809</td>\n",
       "      <td>0.319667</td>\n",
       "      <td>0.301568</td>\n",
       "      <td>0.242856</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.340095</td>\n",
       "      <td>0.581846</td>\n",
       "      <td>0.509487</td>\n",
       "      <td>0.500299</td>\n",
       "      <td>0.679329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8400</th>\n",
       "      <td>4.637359e-07</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.277958</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.209951</td>\n",
       "      <td>0.273809</td>\n",
       "      <td>0.319667</td>\n",
       "      <td>0.301568</td>\n",
       "      <td>0.243091</td>\n",
       "      <td>0.2298</td>\n",
       "      <td>0.340381</td>\n",
       "      <td>0.582896</td>\n",
       "      <td>0.510310</td>\n",
       "      <td>0.501542</td>\n",
       "      <td>0.680886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8500</th>\n",
       "      <td>4.638825e-07</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.277418</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.209580</td>\n",
       "      <td>0.274080</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.301823</td>\n",
       "      <td>0.243287</td>\n",
       "      <td>0.2300</td>\n",
       "      <td>0.340573</td>\n",
       "      <td>0.582580</td>\n",
       "      <td>0.510581</td>\n",
       "      <td>0.501687</td>\n",
       "      <td>0.680582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8600</th>\n",
       "      <td>4.635976e-07</td>\n",
       "      <td>0.002220</td>\n",
       "      <td>0.277418</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.209580</td>\n",
       "      <td>0.273856</td>\n",
       "      <td>0.319667</td>\n",
       "      <td>0.301654</td>\n",
       "      <td>0.243040</td>\n",
       "      <td>0.2298</td>\n",
       "      <td>0.340250</td>\n",
       "      <td>0.582660</td>\n",
       "      <td>0.510538</td>\n",
       "      <td>0.501596</td>\n",
       "      <td>0.679758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8700</th>\n",
       "      <td>4.652389e-07</td>\n",
       "      <td>0.002223</td>\n",
       "      <td>0.277171</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.209439</td>\n",
       "      <td>0.274406</td>\n",
       "      <td>0.320333</td>\n",
       "      <td>0.302133</td>\n",
       "      <td>0.243014</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.340280</td>\n",
       "      <td>0.582294</td>\n",
       "      <td>0.510697</td>\n",
       "      <td>0.501590</td>\n",
       "      <td>0.680241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8800</th>\n",
       "      <td>4.642056e-07</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>0.277171</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.209439</td>\n",
       "      <td>0.274406</td>\n",
       "      <td>0.320333</td>\n",
       "      <td>0.302133</td>\n",
       "      <td>0.242540</td>\n",
       "      <td>0.2290</td>\n",
       "      <td>0.339878</td>\n",
       "      <td>0.582009</td>\n",
       "      <td>0.510653</td>\n",
       "      <td>0.501392</td>\n",
       "      <td>0.680697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8900</th>\n",
       "      <td>4.647542e-07</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.277991</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.209969</td>\n",
       "      <td>0.274406</td>\n",
       "      <td>0.320333</td>\n",
       "      <td>0.302133</td>\n",
       "      <td>0.242540</td>\n",
       "      <td>0.2290</td>\n",
       "      <td>0.339878</td>\n",
       "      <td>0.582914</td>\n",
       "      <td>0.510766</td>\n",
       "      <td>0.501480</td>\n",
       "      <td>0.681297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9000</th>\n",
       "      <td>4.637766e-07</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.277991</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.209969</td>\n",
       "      <td>0.274406</td>\n",
       "      <td>0.320333</td>\n",
       "      <td>0.302133</td>\n",
       "      <td>0.242428</td>\n",
       "      <td>0.2290</td>\n",
       "      <td>0.339633</td>\n",
       "      <td>0.582807</td>\n",
       "      <td>0.510721</td>\n",
       "      <td>0.501401</td>\n",
       "      <td>0.681715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9100</th>\n",
       "      <td>4.643478e-07</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.277991</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.209969</td>\n",
       "      <td>0.274027</td>\n",
       "      <td>0.319667</td>\n",
       "      <td>0.301865</td>\n",
       "      <td>0.242428</td>\n",
       "      <td>0.2290</td>\n",
       "      <td>0.339633</td>\n",
       "      <td>0.582718</td>\n",
       "      <td>0.510506</td>\n",
       "      <td>0.501344</td>\n",
       "      <td>0.680439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9200</th>\n",
       "      <td>4.649102e-07</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.277991</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.209969</td>\n",
       "      <td>0.274027</td>\n",
       "      <td>0.319667</td>\n",
       "      <td>0.301865</td>\n",
       "      <td>0.242666</td>\n",
       "      <td>0.2292</td>\n",
       "      <td>0.339927</td>\n",
       "      <td>0.582815</td>\n",
       "      <td>0.510575</td>\n",
       "      <td>0.501560</td>\n",
       "      <td>0.681390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9300</th>\n",
       "      <td>4.642950e-07</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.277504</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.209646</td>\n",
       "      <td>0.274027</td>\n",
       "      <td>0.319667</td>\n",
       "      <td>0.301865</td>\n",
       "      <td>0.242613</td>\n",
       "      <td>0.2290</td>\n",
       "      <td>0.339897</td>\n",
       "      <td>0.582525</td>\n",
       "      <td>0.510547</td>\n",
       "      <td>0.501499</td>\n",
       "      <td>0.681713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9400</th>\n",
       "      <td>4.644398e-07</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.277698</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.209754</td>\n",
       "      <td>0.274700</td>\n",
       "      <td>0.320333</td>\n",
       "      <td>0.302545</td>\n",
       "      <td>0.242686</td>\n",
       "      <td>0.2290</td>\n",
       "      <td>0.340005</td>\n",
       "      <td>0.583567</td>\n",
       "      <td>0.511044</td>\n",
       "      <td>0.501740</td>\n",
       "      <td>0.682034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9500</th>\n",
       "      <td>4.637665e-07</td>\n",
       "      <td>0.002220</td>\n",
       "      <td>0.277698</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.209754</td>\n",
       "      <td>0.274990</td>\n",
       "      <td>0.320667</td>\n",
       "      <td>0.302802</td>\n",
       "      <td>0.242998</td>\n",
       "      <td>0.2292</td>\n",
       "      <td>0.340433</td>\n",
       "      <td>0.583723</td>\n",
       "      <td>0.511331</td>\n",
       "      <td>0.502012</td>\n",
       "      <td>0.681866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9600</th>\n",
       "      <td>4.654538e-07</td>\n",
       "      <td>0.002221</td>\n",
       "      <td>0.277322</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.209510</td>\n",
       "      <td>0.274494</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.302316</td>\n",
       "      <td>0.243054</td>\n",
       "      <td>0.2292</td>\n",
       "      <td>0.340574</td>\n",
       "      <td>0.583274</td>\n",
       "      <td>0.510920</td>\n",
       "      <td>0.501945</td>\n",
       "      <td>0.680617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9700</th>\n",
       "      <td>4.625625e-07</td>\n",
       "      <td>0.002220</td>\n",
       "      <td>0.277322</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.209510</td>\n",
       "      <td>0.273977</td>\n",
       "      <td>0.319333</td>\n",
       "      <td>0.301890</td>\n",
       "      <td>0.242841</td>\n",
       "      <td>0.2290</td>\n",
       "      <td>0.340346</td>\n",
       "      <td>0.582965</td>\n",
       "      <td>0.510279</td>\n",
       "      <td>0.501562</td>\n",
       "      <td>0.680500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9800</th>\n",
       "      <td>4.636655e-07</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.277554</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.209706</td>\n",
       "      <td>0.273977</td>\n",
       "      <td>0.319333</td>\n",
       "      <td>0.301890</td>\n",
       "      <td>0.242603</td>\n",
       "      <td>0.2288</td>\n",
       "      <td>0.340052</td>\n",
       "      <td>0.583021</td>\n",
       "      <td>0.510215</td>\n",
       "      <td>0.501439</td>\n",
       "      <td>0.680159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9900</th>\n",
       "      <td>4.642313e-07</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.277352</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.209560</td>\n",
       "      <td>0.273977</td>\n",
       "      <td>0.319333</td>\n",
       "      <td>0.301890</td>\n",
       "      <td>0.242109</td>\n",
       "      <td>0.2284</td>\n",
       "      <td>0.339401</td>\n",
       "      <td>0.583037</td>\n",
       "      <td>0.510223</td>\n",
       "      <td>0.501191</td>\n",
       "      <td>0.681515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           loss_mse  loss_w_reg     F1@10  percision@10  recall@10     F1@30  \\\n",
       "epoch                                                                          \n",
       "0      5.655770e-02    1.000000  0.022739         0.051   0.016646  0.047511   \n",
       "100    8.220217e-05    0.002652  0.198686         0.466   0.143277  0.222474   \n",
       "200    1.664961e-05    0.002098  0.219616         0.495   0.160905  0.237151   \n",
       "300    8.199593e-06    0.002056  0.229954         0.511   0.169616  0.244720   \n",
       "400    4.486861e-06    0.002050  0.237516         0.521   0.176136  0.247526   \n",
       "500    2.704610e-06    0.002078  0.248206         0.533   0.186238  0.252684   \n",
       "600    2.824854e-06    0.002098  0.255020         0.544   0.191643  0.256726   \n",
       "700    1.796310e-06    0.002115  0.259312         0.551   0.195177  0.260709   \n",
       "800    1.451007e-06    0.002130  0.264561         0.558   0.199458  0.263361   \n",
       "900    1.194433e-06    0.002142  0.269833         0.564   0.203857  0.264281   \n",
       "1000   9.541553e-07    0.002152  0.271158         0.565   0.205188  0.265446   \n",
       "1100   8.000968e-07    0.002162  0.273911         0.569   0.207261  0.265875   \n",
       "1200   7.344838e-07    0.002170  0.274405         0.570   0.207629  0.267403   \n",
       "1300   6.402593e-07    0.002173  0.273458         0.569   0.206926  0.269093   \n",
       "1400   6.085634e-07    0.002179  0.273718         0.569   0.207240  0.270389   \n",
       "1500   6.140008e-07    0.002186  0.272726         0.566   0.206602  0.270787   \n",
       "1600   6.518250e-07    0.002193  0.273621         0.567   0.207219  0.271346   \n",
       "1700   6.516003e-07    0.002194  0.274147         0.568   0.207576  0.271200   \n",
       "1800   6.133333e-07    0.002199  0.274792         0.569   0.208052  0.271974   \n",
       "1900   5.879484e-07    0.002200  0.273866         0.567   0.207445  0.271700   \n",
       "2000   5.737636e-07    0.002202  0.274294         0.568   0.207711  0.271332   \n",
       "2100   5.477900e-07    0.002204  0.273877         0.566   0.207556  0.272306   \n",
       "2200   5.394116e-07    0.002205  0.273693         0.564   0.207514  0.272417   \n",
       "2300   5.173451e-07    0.002206  0.274133         0.565   0.207830  0.273134   \n",
       "2400   4.924631e-07    0.002208  0.274589         0.566   0.208147  0.273503   \n",
       "2500   4.737288e-07    0.002208  0.273652         0.564   0.207450  0.274618   \n",
       "2600   4.737327e-07    0.002211  0.274186         0.565   0.207822  0.275184   \n",
       "2700   4.805086e-07    0.002212  0.274663         0.567   0.208093  0.275394   \n",
       "2800   4.753454e-07    0.002210  0.275218         0.568   0.208478  0.274601   \n",
       "2900   4.679090e-07    0.002211  0.275790         0.569   0.208878  0.274809   \n",
       "3000   4.671751e-07    0.002213  0.276112         0.570   0.209070  0.275002   \n",
       "3100   4.608034e-07    0.002213  0.276112         0.570   0.209070  0.275369   \n",
       "3200   4.648329e-07    0.002215  0.276112         0.570   0.209070  0.274923   \n",
       "3300   4.700292e-07    0.002213  0.276567         0.571   0.209364  0.274138   \n",
       "3400   4.718590e-07    0.002216  0.275826         0.570   0.208776  0.274734   \n",
       "3500   4.731650e-07    0.002217  0.275764         0.570   0.208717  0.274587   \n",
       "3600   4.732300e-07    0.002216  0.275775         0.570   0.208726  0.274047   \n",
       "3700   4.755923e-07    0.002216  0.275775         0.570   0.208726  0.274354   \n",
       "3800   4.745781e-07    0.002217  0.276275         0.571   0.209059  0.273775   \n",
       "3900   4.764118e-07    0.002216  0.276275         0.571   0.209059  0.274078   \n",
       "4000   4.796612e-07    0.002218  0.276675         0.572   0.209309  0.273926   \n",
       "4100   4.783367e-07    0.002217  0.276907         0.573   0.209440  0.274822   \n",
       "4200   4.751982e-07    0.002217  0.277448         0.574   0.209811  0.275340   \n",
       "4300   4.671736e-07    0.002217  0.277902         0.575   0.210105  0.275100   \n",
       "4400   4.662097e-07    0.002217  0.278471         0.577   0.210437  0.275525   \n",
       "4500   4.671860e-07    0.002219  0.278971         0.578   0.210771  0.275252   \n",
       "4600   4.654790e-07    0.002220  0.279082         0.578   0.210842  0.275107   \n",
       "4700   4.640318e-07    0.002217  0.278978         0.577   0.210787  0.274825   \n",
       "4800   4.653959e-07    0.002219  0.278978         0.577   0.210787  0.274207   \n",
       "4900   4.642166e-07    0.002216  0.279138         0.578   0.210874  0.274456   \n",
       "5000   4.628825e-07    0.002221  0.279138         0.578   0.210874  0.274367   \n",
       "5100   4.639168e-07    0.002220  0.278821         0.577   0.210686  0.274348   \n",
       "5200   4.651729e-07    0.002221  0.278107         0.576   0.210130  0.274699   \n",
       "5300   4.652367e-07    0.002220  0.277662         0.575   0.209844  0.274750   \n",
       "5400   4.652931e-07    0.002218  0.277662         0.575   0.209844  0.274043   \n",
       "5500   4.642663e-07    0.002219  0.277662         0.575   0.209844  0.274496   \n",
       "5600   4.664028e-07    0.002219  0.278068         0.575   0.210117  0.274152   \n",
       "5700   4.646378e-07    0.002220  0.278068         0.575   0.210117  0.274307   \n",
       "5800   4.634249e-07    0.002219  0.278068         0.575   0.210117  0.274458   \n",
       "5900   4.644359e-07    0.002219  0.278419         0.576   0.210330  0.273850   \n",
       "6000   4.640865e-07    0.002221  0.278652         0.577   0.210461  0.273623   \n",
       "6100   4.633889e-07    0.002219  0.278652         0.577   0.210461  0.273852   \n",
       "6200   4.645651e-07    0.002221  0.278652         0.577   0.210461  0.273426   \n",
       "6300   4.638238e-07    0.002219  0.278652         0.577   0.210461  0.273426   \n",
       "6400   4.648369e-07    0.002220  0.278860         0.578   0.210578  0.273554   \n",
       "6500   4.651010e-07    0.002220  0.278860         0.578   0.210578  0.274210   \n",
       "6600   4.659371e-07    0.002222  0.278860         0.578   0.210578  0.274305   \n",
       "6700   4.655037e-07    0.002221  0.279018         0.579   0.210663  0.274305   \n",
       "6800   4.652630e-07    0.002223  0.278764         0.579   0.210452  0.274116   \n",
       "6900   4.646449e-07    0.002219  0.278531         0.578   0.210320  0.273878   \n",
       "7000   4.642543e-07    0.002222  0.278531         0.578   0.210320  0.274138   \n",
       "7100   4.651163e-07    0.002222  0.278531         0.578   0.210320  0.274241   \n",
       "7200   4.644556e-07    0.002223  0.278531         0.578   0.210320  0.274241   \n",
       "7300   4.641765e-07    0.002223  0.278109         0.577   0.210032  0.274000   \n",
       "7400   4.642516e-07    0.002222  0.278109         0.577   0.210032  0.274186   \n",
       "7500   4.626663e-07    0.002221  0.278109         0.577   0.210032  0.274039   \n",
       "7600   4.644307e-07    0.002221  0.277949         0.576   0.209945  0.274329   \n",
       "7700   4.633552e-07    0.002221  0.277596         0.576   0.209687  0.274150   \n",
       "7800   4.631519e-07    0.002221  0.278274         0.577   0.210154  0.274432   \n",
       "7900   4.634727e-07    0.002222  0.278470         0.578   0.210262  0.274283   \n",
       "8000   4.628945e-07    0.002222  0.278470         0.578   0.210262  0.274027   \n",
       "8100   4.637332e-07    0.002223  0.278470         0.578   0.210262  0.274338   \n",
       "8200   4.639883e-07    0.002221  0.278708         0.578   0.210434  0.273809   \n",
       "8300   4.644254e-07    0.002221  0.278181         0.577   0.210077  0.273809   \n",
       "8400   4.637359e-07    0.002222  0.277958         0.576   0.209951  0.273809   \n",
       "8500   4.638825e-07    0.002222  0.277418         0.575   0.209580  0.274080   \n",
       "8600   4.635976e-07    0.002220  0.277418         0.575   0.209580  0.273856   \n",
       "8700   4.652389e-07    0.002223  0.277171         0.574   0.209439  0.274406   \n",
       "8800   4.642056e-07    0.002219  0.277171         0.574   0.209439  0.274406   \n",
       "8900   4.647542e-07    0.002222  0.277991         0.576   0.209969  0.274406   \n",
       "9000   4.637766e-07    0.002222  0.277991         0.576   0.209969  0.274406   \n",
       "9100   4.643478e-07    0.002222  0.277991         0.576   0.209969  0.274027   \n",
       "9200   4.649102e-07    0.002222  0.277991         0.576   0.209969  0.274027   \n",
       "9300   4.642950e-07    0.002222  0.277504         0.575   0.209646  0.274027   \n",
       "9400   4.644398e-07    0.002222  0.277698         0.576   0.209754  0.274700   \n",
       "9500   4.637665e-07    0.002220  0.277698         0.576   0.209754  0.274990   \n",
       "9600   4.654538e-07    0.002221  0.277322         0.575   0.209510  0.274494   \n",
       "9700   4.625625e-07    0.002220  0.277322         0.575   0.209510  0.273977   \n",
       "9800   4.636655e-07    0.002222  0.277554         0.575   0.209706  0.273977   \n",
       "9900   4.642313e-07    0.002222  0.277352         0.575   0.209560  0.273977   \n",
       "\n",
       "       percision@30  recall@30     F1@50  percision@50  recall@50   ndcg@10  \\\n",
       "epoch                                                                         \n",
       "0          0.060333   0.049709  0.076897        0.0774   0.099459  0.049470   \n",
       "100        0.274333   0.231215  0.213027        0.2082   0.281536  0.521408   \n",
       "200        0.286333   0.252111  0.225536        0.2176   0.303493  0.541894   \n",
       "300        0.291333   0.264835  0.231727        0.2228   0.315911  0.549071   \n",
       "400        0.293667   0.269039  0.234579        0.2244   0.321612  0.557201   \n",
       "500        0.299333   0.274879  0.236445        0.2262   0.325385  0.564945   \n",
       "600        0.303000   0.280561  0.236005        0.2256   0.325782  0.572093   \n",
       "700        0.306667   0.285326  0.237722        0.2270   0.329044  0.575137   \n",
       "800        0.308667   0.289184  0.237820        0.2270   0.329399  0.578882   \n",
       "900        0.309333   0.290494  0.238776        0.2276   0.331316  0.581838   \n",
       "1000       0.310667   0.292021  0.238849        0.2272   0.331836  0.583538   \n",
       "1100       0.311000   0.292659  0.239552        0.2274   0.333774  0.586247   \n",
       "1200       0.312000   0.294748  0.239469        0.2270   0.333998  0.585764   \n",
       "1300       0.314333   0.296289  0.238569        0.2264   0.332721  0.585282   \n",
       "1400       0.316333   0.297589  0.238834        0.2264   0.333547  0.585302   \n",
       "1500       0.316667   0.298033  0.238276        0.2260   0.332917  0.583749   \n",
       "1600       0.317667   0.298419  0.238824        0.2264   0.334219  0.584481   \n",
       "1700       0.317333   0.298305  0.238044        0.2258   0.333271  0.586458   \n",
       "1800       0.317667   0.299805  0.239160        0.2268   0.334475  0.587115   \n",
       "1900       0.317667   0.299316  0.239586        0.2270   0.335166  0.585143   \n",
       "2000       0.317333   0.298951  0.239342        0.2268   0.334694  0.584108   \n",
       "2100       0.318333   0.299913  0.239535        0.2270   0.334864  0.583518   \n",
       "2200       0.318333   0.300065  0.239040        0.2266   0.334114  0.582448   \n",
       "2300       0.319000   0.300874  0.238547        0.2262   0.333504  0.582317   \n",
       "2400       0.319667   0.301130  0.238871        0.2264   0.333861  0.582273   \n",
       "2500       0.320667   0.302505  0.238653        0.2258   0.334198  0.581442   \n",
       "2600       0.321333   0.303012  0.239563        0.2266   0.335431  0.581421   \n",
       "2700       0.321667   0.303209  0.238971        0.2260   0.334748  0.581830   \n",
       "2800       0.320667   0.302414  0.238974        0.2260   0.334870  0.581332   \n",
       "2900       0.320667   0.302713  0.238974        0.2260   0.334870  0.581837   \n",
       "3000       0.320667   0.302975  0.238932        0.2260   0.334726  0.580907   \n",
       "3100       0.320667   0.303409  0.239359        0.2264   0.335189  0.580687   \n",
       "3200       0.320000   0.303072  0.240074        0.2268   0.336411  0.580577   \n",
       "3300       0.319000   0.302376  0.239790        0.2266   0.336065  0.580821   \n",
       "3400       0.320000   0.302833  0.240076        0.2268   0.336506  0.580331   \n",
       "3500       0.319667   0.302739  0.240703        0.2272   0.337561  0.579724   \n",
       "3600       0.319000   0.302234  0.240734        0.2272   0.337593  0.579459   \n",
       "3700       0.319333   0.302520  0.241032        0.2274   0.338101  0.579553   \n",
       "3800       0.319000   0.301761  0.240839        0.2272   0.337797  0.579916   \n",
       "3900       0.319333   0.302029  0.240969        0.2272   0.338045  0.580094   \n",
       "4000       0.319667   0.301816  0.241442        0.2280   0.338411  0.580320   \n",
       "4100       0.320333   0.302699  0.241316        0.2280   0.338116  0.580682   \n",
       "4200       0.321000   0.303088  0.241348        0.2280   0.338159  0.581093   \n",
       "4300       0.321000   0.302568  0.241772        0.2284   0.338653  0.581791   \n",
       "4400       0.321333   0.303157  0.241231        0.2278   0.338183  0.584363   \n",
       "4500       0.320667   0.303115  0.241055        0.2274   0.338138  0.584716   \n",
       "4600       0.320667   0.302951  0.241406        0.2280   0.338393  0.584144   \n",
       "4700       0.320333   0.302708  0.241752        0.2284   0.338697  0.583831   \n",
       "4800       0.319667   0.302129  0.241900        0.2286   0.338814  0.583809   \n",
       "4900       0.320000   0.302326  0.241535        0.2282   0.338470  0.583915   \n",
       "5000       0.320000   0.302153  0.241240        0.2278   0.338233  0.583796   \n",
       "5100       0.320000   0.302107  0.241361        0.2280   0.338402  0.583097   \n",
       "5200       0.320333   0.302477  0.241372        0.2280   0.338433  0.583283   \n",
       "5300       0.320333   0.302549  0.241505        0.2282   0.338531  0.582875   \n",
       "5400       0.319667   0.301716  0.242332        0.2290   0.339437  0.582931   \n",
       "5500       0.320333   0.302060  0.242233        0.2290   0.339339  0.582805   \n",
       "5600       0.320000   0.301703  0.242061        0.2288   0.339187  0.584516   \n",
       "5700       0.320333   0.301804  0.241985        0.2288   0.339073  0.583283   \n",
       "5800       0.320333   0.301974  0.242198        0.2290   0.339300  0.583432   \n",
       "5900       0.319333   0.301529  0.242486        0.2294   0.339525  0.583651   \n",
       "6000       0.319000   0.301356  0.242186        0.2290   0.339285  0.583769   \n",
       "6100       0.319333   0.301530  0.242026        0.2290   0.339018  0.583753   \n",
       "6200       0.319000   0.300942  0.241994        0.2290   0.338968  0.583785   \n",
       "6300       0.319000   0.300942  0.241953        0.2290   0.338903  0.583660   \n",
       "6400       0.319333   0.301035  0.242143        0.2292   0.339084  0.584322   \n",
       "6500       0.320000   0.301680  0.242143        0.2292   0.339084  0.584627   \n",
       "6600       0.320333   0.301736  0.242495        0.2296   0.339482  0.584655   \n",
       "6700       0.320333   0.301736  0.242446        0.2296   0.339409  0.584097   \n",
       "6800       0.320000   0.301594  0.242422        0.2296   0.339330  0.583982   \n",
       "6900       0.319667   0.301409  0.242775        0.2300   0.339666  0.583654   \n",
       "7000       0.320000   0.301646  0.242873        0.2300   0.339853  0.583970   \n",
       "7100       0.320000   0.301922  0.242839        0.2300   0.339805  0.584022   \n",
       "7200       0.320000   0.301922  0.242529        0.2296   0.339550  0.584126   \n",
       "7300       0.319667   0.301733  0.242632        0.2296   0.339669  0.583669   \n",
       "7400       0.320000   0.301901  0.242853        0.2300   0.339828  0.583714   \n",
       "7500       0.319667   0.301807  0.242579        0.2294   0.339639  0.583660   \n",
       "7600       0.319667   0.302310  0.242579        0.2294   0.339639  0.583680   \n",
       "7700       0.319333   0.302188  0.242817        0.2296   0.339933  0.583364   \n",
       "7800       0.319667   0.302432  0.242445        0.2292   0.339585  0.583568   \n",
       "7900       0.319667   0.302315  0.242543        0.2292   0.339772  0.583644   \n",
       "8000       0.319333   0.302106  0.242630        0.2294   0.339827  0.583187   \n",
       "8100       0.320000   0.302309  0.242886        0.2296   0.340184  0.583154   \n",
       "8200       0.319667   0.301568  0.242636        0.2294   0.339851  0.582069   \n",
       "8300       0.319667   0.301568  0.242856        0.2296   0.340095  0.581846   \n",
       "8400       0.319667   0.301568  0.243091        0.2298   0.340381  0.582896   \n",
       "8500       0.320000   0.301823  0.243287        0.2300   0.340573  0.582580   \n",
       "8600       0.319667   0.301654  0.243040        0.2298   0.340250  0.582660   \n",
       "8700       0.320333   0.302133  0.243014        0.2296   0.340280  0.582294   \n",
       "8800       0.320333   0.302133  0.242540        0.2290   0.339878  0.582009   \n",
       "8900       0.320333   0.302133  0.242540        0.2290   0.339878  0.582914   \n",
       "9000       0.320333   0.302133  0.242428        0.2290   0.339633  0.582807   \n",
       "9100       0.319667   0.301865  0.242428        0.2290   0.339633  0.582718   \n",
       "9200       0.319667   0.301865  0.242666        0.2292   0.339927  0.582815   \n",
       "9300       0.319667   0.301865  0.242613        0.2290   0.339897  0.582525   \n",
       "9400       0.320333   0.302545  0.242686        0.2290   0.340005  0.583567   \n",
       "9500       0.320667   0.302802  0.242998        0.2292   0.340433  0.583723   \n",
       "9600       0.320000   0.302316  0.243054        0.2292   0.340574  0.583274   \n",
       "9700       0.319333   0.301890  0.242841        0.2290   0.340346  0.582965   \n",
       "9800       0.319333   0.301890  0.242603        0.2288   0.340052  0.583021   \n",
       "9900       0.319333   0.301890  0.242109        0.2284   0.339401  0.583037   \n",
       "\n",
       "        ndcg@30   ndcg@50  ndcg@all  \n",
       "epoch                                \n",
       "0      0.057901  0.077670  0.356658  \n",
       "100    0.453171  0.450578  0.644184  \n",
       "200    0.471427  0.469708  0.653400  \n",
       "300    0.478819  0.477913  0.654057  \n",
       "400    0.484880  0.484384  0.655765  \n",
       "500    0.490653  0.488342  0.657767  \n",
       "600    0.495901  0.491953  0.660538  \n",
       "700    0.499316  0.494191  0.662025  \n",
       "800    0.501032  0.494666  0.662810  \n",
       "900    0.502053  0.495871  0.664681  \n",
       "1000   0.503598  0.497208  0.666418  \n",
       "1100   0.503981  0.497776  0.667856  \n",
       "1200   0.504379  0.497535  0.668338  \n",
       "1300   0.505808  0.497302  0.669356  \n",
       "1400   0.506690  0.497535  0.670993  \n",
       "1500   0.506469  0.497149  0.671139  \n",
       "1600   0.506940  0.497711  0.672521  \n",
       "1700   0.508701  0.498852  0.674905  \n",
       "1800   0.509421  0.499591  0.675744  \n",
       "1900   0.508768  0.499256  0.675513  \n",
       "2000   0.508213  0.498765  0.675849  \n",
       "2100   0.509068  0.498997  0.676259  \n",
       "2200   0.509428  0.498886  0.676408  \n",
       "2300   0.509560  0.498322  0.676253  \n",
       "2400   0.509652  0.498485  0.676392  \n",
       "2500   0.510182  0.498445  0.676831  \n",
       "2600   0.510259  0.498901  0.677122  \n",
       "2700   0.510510  0.498490  0.677992  \n",
       "2800   0.509741  0.498162  0.677573  \n",
       "2900   0.509904  0.498319  0.677569  \n",
       "3000   0.509651  0.497607  0.677582  \n",
       "3100   0.509474  0.497741  0.677522  \n",
       "3200   0.509291  0.498353  0.677099  \n",
       "3300   0.509189  0.498043  0.678314  \n",
       "3400   0.509333  0.498281  0.677859  \n",
       "3500   0.509109  0.498769  0.678337  \n",
       "3600   0.508832  0.498475  0.678384  \n",
       "3700   0.509056  0.498466  0.677834  \n",
       "3800   0.508577  0.498326  0.678282  \n",
       "3900   0.508845  0.498566  0.678573  \n",
       "4000   0.508605  0.498749  0.678452  \n",
       "4100   0.509190  0.498641  0.678498  \n",
       "4200   0.509438  0.498655  0.678856  \n",
       "4300   0.509324  0.499217  0.677889  \n",
       "4400   0.510345  0.499623  0.679093  \n",
       "4500   0.510394  0.499719  0.679997  \n",
       "4600   0.509774  0.499204  0.679624  \n",
       "4700   0.509552  0.499333  0.678949  \n",
       "4800   0.509231  0.499345  0.679194  \n",
       "4900   0.509361  0.499226  0.679261  \n",
       "5000   0.509185  0.499022  0.679054  \n",
       "5100   0.509206  0.499124  0.679638  \n",
       "5200   0.509793  0.499582  0.679836  \n",
       "5300   0.509877  0.499565  0.679440  \n",
       "5400   0.509631  0.499993  0.679628  \n",
       "5500   0.509705  0.499890  0.679465  \n",
       "5600   0.510753  0.501033  0.680250  \n",
       "5700   0.510048  0.500095  0.680030  \n",
       "5800   0.510145  0.500302  0.680476  \n",
       "5900   0.509817  0.500299  0.680250  \n",
       "6000   0.509663  0.500108  0.679948  \n",
       "6100   0.509683  0.500028  0.680306  \n",
       "6200   0.509499  0.500021  0.679419  \n",
       "6300   0.509411  0.499935  0.679826  \n",
       "6400   0.509679  0.500143  0.679471  \n",
       "6500   0.510114  0.500289  0.680274  \n",
       "6600   0.510367  0.500423  0.680960  \n",
       "6700   0.509954  0.499965  0.680122  \n",
       "6800   0.509902  0.499977  0.679943  \n",
       "6900   0.509829  0.500162  0.679664  \n",
       "7000   0.510180  0.500416  0.680198  \n",
       "7100   0.509917  0.500353  0.680550  \n",
       "7200   0.509978  0.500337  0.680145  \n",
       "7300   0.509991  0.500573  0.680218  \n",
       "7400   0.510279  0.500670  0.679896  \n",
       "7500   0.510179  0.500588  0.680696  \n",
       "7600   0.510443  0.500668  0.680729  \n",
       "7700   0.510366  0.500746  0.680512  \n",
       "7800   0.510428  0.500669  0.679769  \n",
       "7900   0.510046  0.500472  0.679691  \n",
       "8000   0.509530  0.500283  0.680022  \n",
       "8100   0.509716  0.500394  0.680242  \n",
       "8200   0.509441  0.500209  0.679630  \n",
       "8300   0.509487  0.500299  0.679329  \n",
       "8400   0.510310  0.501542  0.680886  \n",
       "8500   0.510581  0.501687  0.680582  \n",
       "8600   0.510538  0.501596  0.679758  \n",
       "8700   0.510697  0.501590  0.680241  \n",
       "8800   0.510653  0.501392  0.680697  \n",
       "8900   0.510766  0.501480  0.681297  \n",
       "9000   0.510721  0.501401  0.681715  \n",
       "9100   0.510506  0.501344  0.680439  \n",
       "9200   0.510575  0.501560  0.681390  \n",
       "9300   0.510547  0.501499  0.681713  \n",
       "9400   0.511044  0.501740  0.682034  \n",
       "9500   0.511331  0.502012  0.681866  \n",
       "9600   0.510920  0.501945  0.680617  \n",
       "9700   0.510279  0.501562  0.680500  \n",
       "9800   0.510215  0.501439  0.680159  \n",
       "9900   0.510223  0.501191  0.681515  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "results_df = pd.DataFrame(results).set_index('epoch')\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e60936db",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['model'] = 'our-lasso'\n",
    "final_results.append(results_df[select_columns].iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2115b07e",
   "metadata": {},
   "source": [
    "## Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5658c77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LR(\n",
       "  (emb): Embedding(100, 13976)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select doc_id and k\n",
    "doc_id = 40\n",
    "topk = 30\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "31562cf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth\n",
      "\u001b[48;5;3mlibido\u001b[0m \u001b[48;5;3mbash\u001b[0m \u001b[48;5;3mneither\u001b[0m \u001b[48;5;3mmale\u001b[0m \u001b[48;5;3mtouch\u001b[0m \u001b[48;5;3mstay\u001b[0m \u001b[48;5;3mfather\u001b[0m \u001b[48;5;3mkid\u001b[0m \u001b[48;5;3mlet\u001b[0m flavour robber mount unto 1933 cecil 1932 pepper fujimori someway ahm conceit carpet kali goddess rear kit squint lui jose arab \n",
      "\n",
      "prediction\n",
      "\u001b[48;5;3mlibido\u001b[0m \u001b[48;5;3mbash\u001b[0m \u001b[48;5;3mmale\u001b[0m \u001b[48;5;3mneither\u001b[0m \u001b[48;5;3mstay\u001b[0m \u001b[48;5;3mtouch\u001b[0m \u001b[48;5;3mfather\u001b[0m \u001b[48;5;3mkid\u001b[0m \u001b[48;5;3mlet\u001b[0m your felt not talk might dad nothing win husband do mood her miss drink clinton moment comfort god wanna sexual affair "
     ]
    }
   ],
   "source": [
    "import colored\n",
    "from colored import stylize\n",
    "\n",
    "word_list = vocab.itos\n",
    "\n",
    "gt = [word_list[word_idx] for word_idx in np.argsort(weight_ans[doc_id])[::-1][:topk]]\n",
    "pred = [word_list[word_idx] for word_idx in np.argsort(model.emb.cpu().weight.data[doc_id].numpy())[::-1][:topk]]\n",
    "\n",
    "print('ground truth')\n",
    "for word in gt:\n",
    "    if word in pred:\n",
    "        print(stylize(word, colored.bg(\"yellow\")), end=' ')\n",
    "    else:\n",
    "        print(word, end=' ')\n",
    "\n",
    "print()\n",
    "print('\\nprediction')\n",
    "for word in pred:\n",
    "    if word in gt:\n",
    "        print(stylize(word, colored.bg(\"yellow\")), end=' ')\n",
    "    else:\n",
    "        print(word, end=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55fa0add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "i think james cameron might be becoming my favorite director because this is my second review of his movies . anyway , everyone remembers the rms titanic . it was big , fast , and \" unsinkable \" . . . until april 1912 . it was all over the news and one of the biggest tragedies ever . well james cameron decided to make a movie out of it but star two fictional characters to be in the spotlight instead of the ship . well , onto the main review but \u001b[48;5;3mlet\u001b[0m me remind you that this is all opinion and zero fact and the only fact that will be present is an event from the film . so our two main characters are jack ( leonardo dicaprio ) and rose ( kate winslet ) . they're not annoying too much but watch this and you'll find out why they could become annoying ( http : //tinyurl . com/ojhoyn ) . the main villain i guess is bad luck , fate , hand of god ( no blasphemy intended ) , or just plain caledon hockley ( billy zane ) . combine all of the above and what do you get ? ! oh yes ! we get a love story on a sinking boat . the supporting characters are the following : my personal favorite , mr . andrews ( victor garber ) ( idk he was so nice ) , lovejoy ( david warner ) , murdoch ( ewan stewart ) , lightoller ( jonathan phillips ) , captain smith ( bernard hill ) , molly brown ( kathy bates ) , and many more . we also got the present day treasure hunter , brock lovett ( bill paxton ) . they add something to the story , something good . the action in here is awesome , especially in the second half , the drama as also good . in the end you can have your eyes dropping rainstorms or silent tears . the story is simple and it works . a treasure hunter seeks the heart of the ocean and instead finds a drawing of a woman wearing the said diamond . she calls and tells her tale on the rms titanic . two lovers separated by social class and ultimately , the fate of the ship . everything about the story works and there are very few flaws . i give titanic , an 86% awesome "
     ]
    }
   ],
   "source": [
    "# raw document\n",
    "print()\n",
    "ps = PorterStemmer()\n",
    "    \n",
    "for word in vocab.raw_documents[doc_id].split():\n",
    "    word_stem = ps.stem(word).lower()\n",
    "\n",
    "    if word_stem in gt:\n",
    "        if word_stem in pred:\n",
    "            print(stylize(word, colored.bg(\"yellow\")), end=' ')\n",
    "        else:\n",
    "            print(stylize(word, colored.bg(\"light_gray\")), end=' ')\n",
    "    else:\n",
    "        print(word, end=' ')\n",
    "# print(dataset.documents[doc_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8fc21f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document ndcg:\n",
      "ground truth length: 9\n",
      "NDCG top50 0.9994384705953052\n",
      "NDCG top100 0.9994384705953052\n",
      "NDCG top200 0.9994384705953052\n",
      "NDCG ALL 0.9994384705953052\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "   \n",
    "scores = np.array(model.emb.weight.data)[doc_id].reshape(1, -1)\n",
    "true_relevance = train_loader.dataset.weight_ans[doc_id].reshape(1, -1)\n",
    "\n",
    "results['ndcg@50'] = (ndcg_score(true_relevance, scores, k=50))\n",
    "results['ndcg@100'] = (ndcg_score(true_relevance, scores, k=100))\n",
    "results['ndcg@200'] = (ndcg_score(true_relevance, scores, k=200))\n",
    "results['ndcg@all'] = (ndcg_score(true_relevance, scores, k=None))\n",
    "\n",
    "print('This document ndcg:')\n",
    "print('ground truth length:', np.sum(weight_ans[doc_id] > 0))\n",
    "print('NDCG top50', results['ndcg@50'])\n",
    "print('NDCG top100', results['ndcg@100'])\n",
    "print('NDCG top200', results['ndcg@200'])\n",
    "print('NDCG ALL', results['ndcg@all'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae49eac",
   "metadata": {},
   "source": [
    "## Final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "48834bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_notebook = in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bdb5ecc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to directory ./records/dataset-IMDB-n_document-100-wdist-IDF-filtertopk-100\n"
     ]
    }
   ],
   "source": [
    "final_results_df = pd.DataFrame(final_results).reset_index(drop=True)\n",
    "\n",
    "experiment_dir = './records/dataset-{}-n_document-{}-wdist-{}-filtertopk-{}'.format(\n",
    "                                        config['dataset'],\n",
    "                                        config['n_document'],\n",
    "                                        config[\"document_vector_agg_weight\"],\n",
    "                                        config[\"topk_word_freq_threshold\"])\n",
    "\n",
    "print('Saving to directory', experiment_dir)\n",
    "os.makedirs(experiment_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b704cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results_df.to_csv(os.path.join(experiment_dir, 'result.csv'), index=False)\n",
    "\n",
    "import json\n",
    "with open(os.path.join(experiment_dir, 'config.json'), 'w') as json_file:\n",
    "    json.dump(config, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45e6cc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "for feat in final_results_df.set_index('model').columns:\n",
    "    plt.bar(final_results_df['model'],\n",
    "            final_results_df[feat], \n",
    "            width=0.5, \n",
    "            bottom=None, \n",
    "            align='center', \n",
    "            color=['lightsteelblue', \n",
    "                   'cornflowerblue', \n",
    "                   'royalblue', \n",
    "                   'navy'])\n",
    "    plt.title(feat)\n",
    "    plt.savefig(os.path.join(experiment_dir, '{}.png'.format(feat)))\n",
    "    plt.clf()\n",
    "    if is_notebook:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c679b721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      F1@10     F1@30     F1@50                 model  percision@10  \\\n",
      "0  0.039426  0.073213  0.091834                  topk           NaN   \n",
      "1  0.140458  0.167621  0.168138  sk-linear-regression         0.341   \n",
      "2  0.262543  0.247872  0.221158              sk-lasso         0.530   \n",
      "3  0.277352  0.273977  0.242109             our-lasso         0.575   \n",
      "\n",
      "   recall@10  percision@30  recall@30  percision@50  recall@50   ndcg@10  \\\n",
      "0        NaN           NaN        NaN           NaN        NaN       NaN   \n",
      "1   0.100978      0.213000   0.171015        0.1674   0.219625  0.363496   \n",
      "2   0.200465      0.279667   0.280765        0.2040   0.319288  0.540316   \n",
      "3   0.209560      0.319333   0.301890        0.2284   0.339401  0.583037   \n",
      "\n",
      "    ndcg@30   ndcg@50  ndcg@all  \n",
      "0       NaN       NaN       NaN  \n",
      "1  0.327512  0.334415  0.563008  \n",
      "2  0.468284  0.465313  0.621234  \n",
      "3  0.510223  0.501191  0.681515  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1@10</th>\n",
       "      <th>F1@30</th>\n",
       "      <th>F1@50</th>\n",
       "      <th>model</th>\n",
       "      <th>percision@10</th>\n",
       "      <th>recall@10</th>\n",
       "      <th>percision@30</th>\n",
       "      <th>recall@30</th>\n",
       "      <th>percision@50</th>\n",
       "      <th>recall@50</th>\n",
       "      <th>ndcg@10</th>\n",
       "      <th>ndcg@30</th>\n",
       "      <th>ndcg@50</th>\n",
       "      <th>ndcg@all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.039426</td>\n",
       "      <td>0.073213</td>\n",
       "      <td>0.091834</td>\n",
       "      <td>topk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.140458</td>\n",
       "      <td>0.167621</td>\n",
       "      <td>0.168138</td>\n",
       "      <td>sk-linear-regression</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.100978</td>\n",
       "      <td>0.213000</td>\n",
       "      <td>0.171015</td>\n",
       "      <td>0.1674</td>\n",
       "      <td>0.219625</td>\n",
       "      <td>0.363496</td>\n",
       "      <td>0.327512</td>\n",
       "      <td>0.334415</td>\n",
       "      <td>0.563008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.262543</td>\n",
       "      <td>0.247872</td>\n",
       "      <td>0.221158</td>\n",
       "      <td>sk-lasso</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.200465</td>\n",
       "      <td>0.279667</td>\n",
       "      <td>0.280765</td>\n",
       "      <td>0.2040</td>\n",
       "      <td>0.319288</td>\n",
       "      <td>0.540316</td>\n",
       "      <td>0.468284</td>\n",
       "      <td>0.465313</td>\n",
       "      <td>0.621234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.277352</td>\n",
       "      <td>0.273977</td>\n",
       "      <td>0.242109</td>\n",
       "      <td>our-lasso</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.209560</td>\n",
       "      <td>0.319333</td>\n",
       "      <td>0.301890</td>\n",
       "      <td>0.2284</td>\n",
       "      <td>0.339401</td>\n",
       "      <td>0.583037</td>\n",
       "      <td>0.510223</td>\n",
       "      <td>0.501191</td>\n",
       "      <td>0.681515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      F1@10     F1@30     F1@50                 model  percision@10  \\\n",
       "0  0.039426  0.073213  0.091834                  topk           NaN   \n",
       "1  0.140458  0.167621  0.168138  sk-linear-regression         0.341   \n",
       "2  0.262543  0.247872  0.221158              sk-lasso         0.530   \n",
       "3  0.277352  0.273977  0.242109             our-lasso         0.575   \n",
       "\n",
       "   recall@10  percision@30  recall@30  percision@50  recall@50   ndcg@10  \\\n",
       "0        NaN           NaN        NaN           NaN        NaN       NaN   \n",
       "1   0.100978      0.213000   0.171015        0.1674   0.219625  0.363496   \n",
       "2   0.200465      0.279667   0.280765        0.2040   0.319288  0.540316   \n",
       "3   0.209560      0.319333   0.301890        0.2284   0.339401  0.583037   \n",
       "\n",
       "    ndcg@30   ndcg@50  ndcg@all  \n",
       "0       NaN       NaN       NaN  \n",
       "1  0.327512  0.334415  0.563008  \n",
       "2  0.468284  0.465313  0.621234  \n",
       "3  0.510223  0.501191  0.681515  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(final_results_df)\n",
    "final_results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
