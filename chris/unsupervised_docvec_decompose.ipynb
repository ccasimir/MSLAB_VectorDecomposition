{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a67d8234",
   "metadata": {},
   "source": [
    "### raw data\n",
    "* word embedding: glove\n",
    "* doc text: ./data/IMDB.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792dad29",
   "metadata": {},
   "source": [
    "### dataset\n",
    "1. IMDB\n",
    "2. CNNNews\n",
    "3. [PubMed](https://github.com/LIAAD/KeywordExtractor-Datasets/blob/master/datasets/PubMed.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b007799a",
   "metadata": {},
   "source": [
    "### preprocess\n",
    "1. filter too frequent and less frequent words\n",
    "2. stemming\n",
    "3. document vector aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6a82ca",
   "metadata": {},
   "source": [
    "### model\n",
    "1. TopK\n",
    "2. Sklearn\n",
    "3. Our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b9967b",
   "metadata": {},
   "source": [
    "### evaluation\n",
    "1. F1\n",
    "2. NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e9beff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/chrisliu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np \n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from itertools import cycle\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Used to get the data\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2341254",
   "metadata": {},
   "source": [
    "## Preprocess config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dc01df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "config[\"dataset\"] = \"CNN\" # \"IMDB\" \"CNN\", \"PubMed\"\n",
    "config[\"n_document\"] = 500\n",
    "config[\"normalize_word_embedding\"] = True\n",
    "config[\"min_word_freq_threshold\"] = 20\n",
    "config[\"topk_word_freq_threshold\"] = 100\n",
    "config[\"document_vector_agg_weight\"] = 'pmi' # ['mean', 'IDF', 'uniform', 'gaussian', 'exponential', 'pmi']\n",
    "config[\"select_topk_TFIDF\"] = None\n",
    "config[\"embedding_file\"] = \"../data/glove.6B.100d.txt\"\n",
    "config[\"topk\"] = [10, 30, 50]\n",
    "\n",
    "sk_lasso_epoch = 10000\n",
    "our_lasso_epoch = 50000\n",
    "is_notebook = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05c1627d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a7d569cd2044199d9125cd58da795d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:400000\n"
     ]
    }
   ],
   "source": [
    "# load word embedding\n",
    "embedding_file = config[\"embedding_file\"]\n",
    "word2embedding = dict()\n",
    "word_dim = int(re.findall(r\".(\\d+)d\",embedding_file)[0])\n",
    "\n",
    "with open(embedding_file,\"r\") as f:\n",
    "    for line in tqdm(f):\n",
    "        line = line.strip().split()\n",
    "        word = line[0]\n",
    "        embedding = list(map(float,line[1:]))\n",
    "        word2embedding[word] = np.array(embedding)\n",
    "\n",
    "print(\"Number of words:%d\" % len(word2embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47bfc175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b3f72892b24b389c9104fcefad8bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def normalize_wordemb(word2embedding):\n",
    "    word_emb = []\n",
    "    word_list = []\n",
    "    for word, emb in word2embedding.items():\n",
    "        word_list.append(word)\n",
    "        word_emb.append(emb)\n",
    "\n",
    "    word_emb = np.array(word_emb)\n",
    "\n",
    "    for i in range(len(word_emb)):\n",
    "        norm = np.linalg.norm(word_emb[i])\n",
    "        word_emb[i] = word_emb[i] / norm\n",
    "\n",
    "    for word, emb in tqdm(zip(word_list, word_emb)):\n",
    "        word2embedding[word] = emb\n",
    "    return word2embedding\n",
    "\n",
    "if config[\"normalize_word_embedding\"]:\n",
    "    normalize_wordemb(word2embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1233053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, word2embedding, min_word_freq_threshold=0, topk_word_freq_threshold=0):\n",
    "        # The low frequency words will be assigned as <UNK> token\n",
    "        self.itos = {0: \"<UNK>\"}\n",
    "        self.stoi = {\"<UNK>\": 0}\n",
    "        \n",
    "        self.word2embedding = word2embedding\n",
    "        self.min_word_freq_threshold = min_word_freq_threshold\n",
    "        self.topk_word_freq_threshold = topk_word_freq_threshold\n",
    "        \n",
    "        self.word_freq_in_corpus = defaultdict(int)\n",
    "        self.IDF = {}\n",
    "        self.ps = PorterStemmer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "#     @staticmethod\n",
    "    def tokenizer_eng(self, text):\n",
    "        text = re.sub(r'[^A-Za-z0-9 ]+', '', text)\n",
    "        text = text.strip().split()\n",
    "        \n",
    "        return [self.ps.stem(w) for w in text if w.lower() not in self.stop_words]\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        self.doc_freq = defaultdict(int) # # of document a word appear\n",
    "        self.document_num = len(sentence_list)\n",
    "        self.word_vectors = [[0]*word_dim] # unknown word emb\n",
    "        \n",
    "        for sentence in tqdm(sentence_list, desc=\"Preprocessing documents\"):\n",
    "            # for doc_freq\n",
    "            document_words = set()\n",
    "            \n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                # pass unknown word\n",
    "                if word not in self.word2embedding:\n",
    "                    continue\n",
    "                    \n",
    "                # calculate word freq\n",
    "                self.word_freq_in_corpus[word] += 1\n",
    "                document_words.add(word)\n",
    "                \n",
    "            for word in document_words:\n",
    "                self.doc_freq[word] += 1\n",
    "        \n",
    "        # calculate IDF\n",
    "        print('doc num', self.document_num)\n",
    "        for word, freq in self.doc_freq.items():\n",
    "            self.IDF[word] = math.log(self.document_num / (freq+1))\n",
    "        \n",
    "        # delete less freq words:\n",
    "        delete_words = []\n",
    "        for word, v in self.word_freq_in_corpus.items():\n",
    "            if v < self.min_word_freq_threshold:\n",
    "                delete_words.append(word)     \n",
    "        for word in delete_words:\n",
    "            del self.IDF[word]    \n",
    "            del self.word_freq_in_corpus[word]    \n",
    "        \n",
    "        # delete too freq words\n",
    "        print('eliminate freq words')\n",
    "        IDF = [(word, freq) for word, freq in self.IDF.items()]\n",
    "        IDF.sort(key=lambda x: x[1])\n",
    "\n",
    "        for i in range(self.topk_word_freq_threshold):\n",
    "            print(word)\n",
    "            word = IDF[i][0]\n",
    "            del self.IDF[word]\n",
    "            del self.word_freq_in_corpus[word]\n",
    "        \n",
    "        # construct word_vectors\n",
    "        idx = 1\n",
    "        for word in self.word_freq_in_corpus:\n",
    "            self.word_vectors.append(self.word2embedding[word])\n",
    "            self.stoi[word] = idx\n",
    "            self.itos[idx] = word\n",
    "            idx += 1\n",
    "            \n",
    "    def init_word_weight(self,sentence_list, agg):\n",
    "        if agg == 'mean':\n",
    "            self.word_weight = {word: 1 for word in self.IDF.keys()}\n",
    "        elif agg == 'IDF':\n",
    "            self.word_weight = self.IDF\n",
    "        elif agg == 'uniform':\n",
    "            self.word_weight = {word: np.random.uniform(low=0.0, high=1.0) for word in self.IDF.keys()}\n",
    "        elif agg == 'gaussian':\n",
    "            mu, sigma = 10, 1 # mean and standard deviation\n",
    "            self.word_weight = {word: np.random.normal(mu, sigma) for word in self.IDF.keys()}\n",
    "        elif agg == 'exponential':\n",
    "            self.word_weight = {word: np.random.exponential(scale=1.0) for word in self.IDF.keys()}\n",
    "        elif agg == 'pmi':\n",
    "            trigram_measures = BigramAssocMeasures()\n",
    "            self.word_weight = defaultdict(int)\n",
    "            corpus = []\n",
    "\n",
    "            for text in tqdm(sentence_list):\n",
    "                corpus.extend(text.split())\n",
    "\n",
    "            finder = BigramCollocationFinder.from_words(corpus)\n",
    "            for pmi_score in finder.score_ngrams(trigram_measures.pmi):\n",
    "                pair, score = pmi_score\n",
    "                self.word_weight[pair[0]] += score\n",
    "                self.word_weight[pair[1]] += score\n",
    "                \n",
    "    def calculate_document_vector(self, sentence_list, agg, n_document, select_topk_TFIDF=None):\n",
    "        document_vectors = []\n",
    "        document_answers = []\n",
    "        document_answers_w = []\n",
    "        \n",
    "        self.init_word_weight(sentence_list, agg)\n",
    "        \n",
    "        for sentence in tqdm(sentence_list[:min(n_document, len(sentence_list))], desc=\"calculate document vectors\"):\n",
    "            document_vector = np.zeros(len(self.word_vectors[0]))\n",
    "            select_words = []\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                # pass unknown word\n",
    "                if word not in self.stoi:\n",
    "                    continue\n",
    "                else:\n",
    "                    select_words.append(word)\n",
    "\n",
    "            # select topk TDIDF\n",
    "            if select_topk_TFIDF is not None:\n",
    "                doc_TFIDF = defaultdict(float)\n",
    "                for word in select_words:    \n",
    "                    doc_TFIDF[word] += self.IDF[word]\n",
    "\n",
    "                doc_TFIDF_l = [(word, TFIDF) for word, TFIDF in doc_TFIDF.items()]\n",
    "                doc_TFIDF_l.sort(key=lambda x:x[1], reverse=True)\n",
    "                \n",
    "                select_topk_words = set(list(map(lambda x:x[0], doc_TFIDF_l[:select_topk_TFIDF])))\n",
    "                select_words = [word for word in select_words if word in select_topk_words]\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            total_weight = 0\n",
    "            # aggregate to doc vectors\n",
    "            for word in select_words:\n",
    "                document_vector += np.array(self.word2embedding[word]) * self.word_weight[word]\n",
    "                total_weight += self.word_weight[word]\n",
    "                \n",
    "            if len(select_words) == 0:\n",
    "                print('error', sentence)\n",
    "                continue\n",
    "            else:\n",
    "                document_vector /= total_weight\n",
    "            \n",
    "            document_vectors.append(document_vector)\n",
    "            document_answers.append(select_words)\n",
    "            document_answers_w.append(total_weight)\n",
    "        \n",
    "        # get answers\n",
    "        document_answers_idx = []    \n",
    "        for ans in document_answers:\n",
    "            ans_idx = []\n",
    "            for token in ans:\n",
    "                if token in self.stoi:\n",
    "                    ans_idx.append(self.stoi[token])                    \n",
    "            document_answers_idx.append(ans_idx)\n",
    "\n",
    "        return document_vectors, document_answers_idx, document_answers_w\n",
    "        \n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c5706a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBowDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 raw_data_file_path,\n",
    "                 word2embedding,\n",
    "                 skip_header = False,\n",
    "                 n_document = None, # read first n document\n",
    "                 min_word_freq_threshold = 20, # eliminate less freq words\n",
    "                 topk_word_freq_threshold = 5, # eliminate smallest k IDF words\n",
    "                 select_topk_TFIDF = None, # select topk tf-idf as ground-truth\n",
    "                 document_vector_agg_weight = 'mean',\n",
    "                 ):\n",
    "\n",
    "        assert document_vector_agg_weight in ['mean', 'IDF', 'uniform', 'gaussian', 'exponential', 'pmi']\n",
    "        \n",
    "        # raw documents\n",
    "        self.documents = []\n",
    "        \n",
    "        with open(raw_data_file_path,'r',encoding='utf-8') as f:\n",
    "            if skip_header:\n",
    "                f.readline()\n",
    "            for line in tqdm(f, desc=\"Loading documents\"):\n",
    "                # read firt n document\n",
    "                # if n_document is not None and len(self.documents) >= n_document:\n",
    "                #     break    \n",
    "                self.documents.append(line.strip(\"\\n\"))\n",
    "\n",
    "        # build vocabulary\n",
    "        self.vocab = Vocabulary(word2embedding, min_word_freq_threshold, topk_word_freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.documents)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "        # calculate document vectors\n",
    "        self.document_vectors, self.document_answers, self.document_answers_w = self.vocab.calculate_document_vector(self.documents, \\\n",
    "                                                                                           document_vector_agg_weight, n_document, select_topk_TFIDF)\n",
    "                \n",
    "        # train-test split\n",
    "        # training\n",
    "        self.train_split_ratio = 0.8\n",
    "        self.train_length = int(len(self.document_answers) * self.train_split_ratio)\n",
    "        self.train_vectors = self.document_vectors[:self.train_length]\n",
    "        self.train_words = self.document_answers[:self.train_length]\n",
    "        self.document_ids = list(range(self.train_length))\n",
    "        self.generator = cycle(self.context_target_generator())\n",
    "        self.dataset_size = sum([len(s) for s in self.train_words])\n",
    "        \n",
    "        # testing\n",
    "        self.test_vectors = self.document_vectors[self.train_length:]\n",
    "        self.test_words = self.document_answers[self.train_length:]\n",
    "\n",
    "    def context_target_generator(self):\n",
    "        np.random.shuffle(self.document_ids) # inplace shuffle\n",
    "\n",
    "        # randomly select a document and create its training example\n",
    "        for document_id in self.document_ids: \n",
    "            word_list = set(self.train_words[document_id])\n",
    "            negative_sample_space = list(set(range(self.vocab_size)) - word_list)\n",
    "            negative_samples = np.random.choice(negative_sample_space,size=len(word_list),replace = False)\n",
    "            for word_id, negative_wordID in zip(word_list, negative_samples):\n",
    "                yield [document_id, word_id, negative_wordID]\n",
    "                \n",
    "    def __getitem__(self, idx):\n",
    "        doc_id, word_id, negative_wordID = next(self.generator)\n",
    "        doc_id = torch.FloatTensor(self.document_vectors[doc_id])\n",
    "        word_id = torch.FloatTensor(self.vocab.word_vectors[word_id])\n",
    "        negative_word = torch.FloatTensor(self.vocab.word_vectors[negative_wordID])\n",
    "\n",
    "        return doc_id, word_id, negative_word\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1a7186c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b1eeb1c8cf84c4ab1302fd0c06f418b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading documents: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca8423b48fe40248817160d5cb43e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing documents:   0%|          | 0/19026 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc num 19026\n",
      "eliminate freq words\n",
      "paroxysm\n",
      "subject\n",
      "line\n",
      "organ\n",
      "write\n",
      "univers\n",
      "one\n",
      "would\n",
      "use\n",
      "like\n",
      "get\n",
      "know\n",
      "dont\n",
      "think\n",
      "time\n",
      "make\n",
      "also\n",
      "say\n",
      "go\n",
      "im\n",
      "could\n",
      "want\n",
      "new\n",
      "work\n",
      "good\n",
      "well\n",
      "way\n",
      "need\n",
      "look\n",
      "even\n",
      "anyon\n",
      "thing\n",
      "see\n",
      "tri\n",
      "thank\n",
      "much\n",
      "year\n",
      "world\n",
      "system\n",
      "right\n",
      "problem\n",
      "may\n",
      "take\n",
      "mani\n",
      "two\n",
      "first\n",
      "seem\n",
      "question\n",
      "pleas\n",
      "1\n",
      "state\n",
      "us\n",
      "come\n",
      "2\n",
      "post\n",
      "help\n",
      "call\n",
      "usa\n",
      "point\n",
      "sinc\n",
      "find\n",
      "read\n",
      "still\n",
      "back\n",
      "mean\n",
      "ive\n",
      "give\n",
      "email\n",
      "sure\n",
      "differ\n",
      "might\n",
      "run\n",
      "cant\n",
      "reason\n",
      "last\n",
      "day\n",
      "interest\n",
      "case\n",
      "let\n",
      "person\n",
      "said\n",
      "never\n",
      "start\n",
      "doesnt\n",
      "tell\n",
      "better\n",
      "ask\n",
      "got\n",
      "without\n",
      "follow\n",
      "part\n",
      "lot\n",
      "3\n",
      "number\n",
      "put\n",
      "fact\n",
      "gener\n",
      "inform\n",
      "actual\n",
      "that\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe56c246b5c4de0a3c3466feb27b51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19026 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327da2fbc6aa41fa868b31c7c58e72d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "calculate document vectors:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load and build torch dataset\n",
    "if config[\"dataset\"] == 'IMDB':\n",
    "    data_file_path = '../data/IMDB.txt'\n",
    "elif config[\"dataset\"] == 'CNN':\n",
    "    data_file_path = '../data/CNN.txt'\n",
    "elif config[\"dataset\"] == 'PubMed':\n",
    "    data_file_path = '../data/PubMed.txt'\n",
    "\n",
    "print(\"Building dataset....\")\n",
    "dataset = CBowDataset(\n",
    "                    raw_data_file_path=data_file_path,\n",
    "                    word2embedding=word2embedding,\n",
    "                    skip_header=False,\n",
    "                    n_document = config[\"n_document\"],\n",
    "                    min_word_freq_threshold = config[\"min_word_freq_threshold\"],\n",
    "                    topk_word_freq_threshold = config[\"topk_word_freq_threshold\"],\n",
    "                    document_vector_agg_weight = config[\"document_vector_agg_weight\"],\n",
    "                    select_topk_TFIDF = config[\"select_topk_TFIDF\"]\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7e6d657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish building dataset!\n",
      "Number of documents:19026\n",
      "Number of words:7602\n",
      "Average length of document: 73.02\n"
     ]
    }
   ],
   "source": [
    "print(\"Finish building dataset!\")\n",
    "print(f\"Number of documents:{len(dataset.documents)}\")\n",
    "print(f\"Number of words:{dataset.vocab_size}\")\n",
    "\n",
    "l = list(map(len, dataset.document_answers))\n",
    "print(\"Average length of document:\", np.mean(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba4758da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# check test doc vectors' correctness\n",
    "word_vectors = np.array(dataset.vocab.word_vectors)\n",
    "word_vectors.shape\n",
    "\n",
    "pred = np.zeros(100)\n",
    "cnt = 0\n",
    "for word_idx in dataset.test_words[0]:\n",
    "    pred += word_vectors[word_idx] * dataset.vocab.word_weight[dataset.vocab.itos[word_idx]]\n",
    "    cnt += dataset.vocab.word_weight[dataset.vocab.itos[word_idx]]\n",
    "print(dataset.test_vectors[0] - pred/cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe65f37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 7602)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39862678b67449d1a32521c09a6f0422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## create weight_ans\n",
    "document_answers = dataset.document_answers\n",
    "\n",
    "onehot_ans = np.zeros((len(document_answers), word_vectors.shape[0]))\n",
    "weight_ans = np.zeros((len(document_answers), word_vectors.shape[0]))\n",
    "print(weight_ans.shape)\n",
    "\n",
    "for i in tqdm(range(len(document_answers))):\n",
    "    for word_idx in document_answers[i]:\n",
    "        weight_ans[i, word_idx] += dataset.vocab.word_weight[dataset.vocab.itos[word_idx]]\n",
    "        onehot_ans[i, word_idx] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1eef4431",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vectors = np.array(dataset.document_vectors)\n",
    "document_answers_w = np.array(dataset.document_answers_w).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbca2e49",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a52979e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model',\n",
       " 'F1@10',\n",
       " 'F1@30',\n",
       " 'F1@50',\n",
       " 'ndcg@10',\n",
       " 'ndcg@30',\n",
       " 'ndcg@50',\n",
       " 'ndcg@all']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results = []\n",
    "select_columns = ['model']\n",
    "for topk in config[\"topk\"]:\n",
    "    select_columns.append('F1@{}'.format(topk))\n",
    "for topk in config[\"topk\"]:\n",
    "    select_columns.append('ndcg@{}'.format(topk))\n",
    "select_columns.append('ndcg@all')\n",
    "select_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87d5379",
   "metadata": {},
   "source": [
    "## setting training size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f9ddbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size_ratio = 1\n",
    "train_size = int(len(dataset.document_answers) * train_size_ratio)\n",
    "train_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d31a21",
   "metadata": {},
   "source": [
    "## Top K freq word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c664900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "251dbcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ans = dataset.document_answers[:train_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1efec49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('x', 6539),\n",
       " ('god', 5208),\n",
       " ('file', 4918),\n",
       " ('0', 4520),\n",
       " ('window', 4444),\n",
       " ('program', 4201),\n",
       " ('drive', 3633),\n",
       " ('4', 3528),\n",
       " ('game', 3474),\n",
       " ('govern', 3268)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = [(word, freq) for word, freq in dataset.vocab.word_freq_in_corpus.items()]\n",
    "word_freq.sort(key=lambda x:x[1], reverse=True)\n",
    "word_freq[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a04793e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a92c289545497982a5d943a6b5ef2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 word\n",
      "percision 0.07180000000000002\n",
      "recall 0.016975938403991114\n",
      "F1 0.02745952111167467\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29a4822347034a81a3498e2f409b2582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 30 word\n",
      "percision 0.0772\n",
      "recall 0.050337316963164104\n",
      "F1 0.06093966788839775\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93a956f049d426ba692519a1c4a3a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 50 word\n",
      "percision 0.07504000000000002\n",
      "recall 0.07963191243187574\n",
      "F1 0.07726779367934514\n"
     ]
    }
   ],
   "source": [
    "def topk_word_evaluation(k=50):\n",
    "    topk_word = [word for (word, freq) in word_freq[:k]]\n",
    "\n",
    "    pr, re = [], []\n",
    "    for ans in tqdm(test_ans):\n",
    "        ans = set(ans)\n",
    "        ans = [dataset.vocab.itos[a] for a in ans]\n",
    "\n",
    "        hit = []\n",
    "        for word in ans:\n",
    "            if word in topk_word:\n",
    "                hit.append(word)\n",
    "\n",
    "        precision = len(hit) / k\n",
    "        recall = len(hit) / len(ans)\n",
    "        pr.append(precision)\n",
    "        re.append(recall)\n",
    "\n",
    "    pr = np.mean(pr)\n",
    "    re = np.mean(re)\n",
    "    f1 = 2 * pr * re / (pr + re) if (pr + re) != 0 else 0\n",
    "    print('top {} word'.format(k))\n",
    "    print('percision', np.mean(pr))\n",
    "    print('recall', np.mean(re))\n",
    "    print('F1', f1)\n",
    "    return f1\n",
    "\n",
    "\n",
    "for topk in config['topk']:\n",
    "    topk_results[\"F1@{}\".format(topk)] = topk_word_evaluation(k=topk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26308789",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1848c900d2d4ba38abfff5410484410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 NDCG:0.02806885474781206\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "effd1ee3d2244fbf9af47e096bd9f728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 30 NDCG:0.03771949992174143\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b678f0a751fb4ce1bdcb99414a9b1697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 50 NDCG:0.04573042479786266\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f4472ba6e849e3b8fc062c7d6d0081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top None NDCG:0.3141552158576379\n"
     ]
    }
   ],
   "source": [
    "def topk_word_evaluation_NDCG(k=50):\n",
    "    freq_word =[word for (word, freq) in word_freq]\n",
    "    freq_word_idx = [dataset.vocab.stoi[word] for word in freq_word if word in dataset.vocab.stoi]\n",
    "    \n",
    "    scores = np.zeros(len(dataset.vocab.word_vectors))\n",
    "    for rank, idx in enumerate(freq_word_idx):\n",
    "        scores[idx] = len(dataset.vocab.word_vectors) - rank\n",
    "    \n",
    "    NDCGs = []\n",
    "    \n",
    "    for ans in tqdm(test_ans):\n",
    "        weight_ans = np.zeros(len(dataset.vocab.word_vectors))\n",
    "        \n",
    "        for word_idx in ans:\n",
    "            if word_idx == 0:\n",
    "                continue\n",
    "            word = dataset.vocab.itos[word_idx]\n",
    "            weight_ans[word_idx] += dataset.vocab.IDF[word]\n",
    "\n",
    "        NDCG_score = ndcg_score(weight_ans.reshape(1,-1), scores.reshape(1,-1), k=k)\n",
    "        NDCGs.append(NDCG_score)\n",
    "\n",
    "    print('top {} NDCG:{}'.format(k, np.mean(NDCGs)))\n",
    "    \n",
    "    return np.mean(NDCGs)\n",
    "\n",
    "\n",
    "for topk in config['topk']:\n",
    "    topk_results[\"ndcg@{}\".format(topk)] = topk_word_evaluation_NDCG(k=topk)\n",
    "    \n",
    "topk_results[\"ndcg@all\"] = topk_word_evaluation_NDCG(k=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a1cbb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_results[\"model\"] = \"topk\"\n",
    "final_results.append(pd.Series(topk_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731c7c3c",
   "metadata": {},
   "source": [
    "## Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a091701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "091f8b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 100)\n",
      "(500, 7602)\n",
      "(7602, 100)\n"
     ]
    }
   ],
   "source": [
    "print(document_vectors.shape)\n",
    "print(weight_ans.shape)\n",
    "print(word_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75933b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sklearn(pred, ans):\n",
    "    results = {}\n",
    "        \n",
    "    one_hot_ans = np.arange(ans.shape[0])[ans > 0]\n",
    "    \n",
    "    for topk in config[\"topk\"]:\n",
    "        one_hot_pred = np.argsort(pred)[-topk:]\n",
    "        hit = np.intersect1d(one_hot_pred, one_hot_ans)\n",
    "        percision = len(hit) / topk\n",
    "        recall = len(hit) / len(one_hot_ans)\n",
    "        f1 = 2 * percision * recall / (percision + recall) if (percision + recall) > 0 else 0\n",
    "        \n",
    "        results['F1@{}'.format(topk)] = f1\n",
    "        \n",
    "    ans = ans.reshape(1, -1)\n",
    "    pred = pred.reshape(1, -1)\n",
    "    for topk in config[\"topk\"]:\n",
    "        results['ndcg@{}'.format(topk)] = ndcg_score(ans, pred, k=topk)\n",
    "\n",
    "    results['ndcg@all'] = (ndcg_score(ans, pred, k=None))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbdc9ed",
   "metadata": {},
   "source": [
    "### linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4e09dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc3b4bec8c214c349b75afcf6f1110b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for doc_id, doc_emb in enumerate(tqdm(document_vectors[:train_size])):\n",
    "    x = word_vectors.T\n",
    "    y = doc_emb\n",
    "    \n",
    "    ans = weight_ans[doc_id]\n",
    "    model = LinearRegression(fit_intercept=False).fit(x, y)\n",
    "    r2 = model.score(x, y)\n",
    "\n",
    "    res = evaluate_sklearn(model.coef_, ans)\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18befbbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F1@10                   0.141648\n",
       "F1@30                   0.154411\n",
       "F1@50                   0.152557\n",
       "ndcg@10                 0.547117\n",
       "ndcg@30                 0.535206\n",
       "ndcg@50                 0.552309\n",
       "ndcg@all                0.682321\n",
       "model       sk-linear-regression\n",
       "dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(results).mean()\n",
    "results['model'] = 'sk-linear-regression'\n",
    "final_results.append(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42cf137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e5b99a67414ebfb6e3776f0c08e036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.1250362580715887e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.220080452281948e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.5561831257524545e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.3852447794681098e-18, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1926223897340549e-18, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0482662914784724e-14, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.8648277366750676e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0023449084628488e-16, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.487878388885868e-16, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.618701254788249e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.5370330836160804e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.008661028647275e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.5778671692021646e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.4911496476764938e-14, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.940721888749053e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1167282376600696e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.5804011705155006e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.577867169202165e-18, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.3338368646821807e-10, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.237402509576498e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.9410748969848086e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4.591596200476111e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 9.433708154926723e-14, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.382710778154774e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.310056643865636e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0191500421363742e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.0126448291014611e-16, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.164042358815848e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7564075194265172e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.197546177488575e-12, tolerance: 0.0\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.734723475976807e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6.830473686658678e-18, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.2959746043559335e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.342584184858884e-07, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.5178830414797062e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.7509865085640897e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.393552799879629e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.74049749507499e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.323432666903827e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.955509944730267e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8.456776945386935e-18, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1275702593849246e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.426078865054194e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.3362566261831965e-12, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.589415207398531e-18, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.3068166260807885e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.2526065174565133e-18, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.422375672105458e-14, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.6266562669640123e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1535911115245767e-16, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1.1926223897340549e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.122502256758253e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7.502679033599691e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.5561831257524545e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2.7755575615628914e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3.5778671692021646e-17, tolerance: 0.0\n",
      "  positive)\n",
      "/home/chrisliu/virtual_env/py37/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5.431852884152377e-17, tolerance: 0.0\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for doc_id, doc_emb in enumerate(tqdm(document_vectors[:train_size])):\n",
    "    x = word_vectors.T\n",
    "    y = doc_emb\n",
    "    \n",
    "    ans = weight_ans[doc_id]\n",
    "    model = Lasso(positive=True, fit_intercept=False, alpha=0.0001, max_iter=sk_lasso_epoch, tol=0).fit(x, y)\n",
    "    r2 = model.score(x, y)\n",
    "\n",
    "    res = evaluate_sklearn(model.coef_, ans)\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07772202",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(results).mean()\n",
    "results['model'] = 'sk-lasso'\n",
    "final_results.append(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4fcb3a",
   "metadata": {},
   "source": [
    "## Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f53f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854f9586",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Dataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 doc_vectors,\n",
    "                 doc_w_sum,\n",
    "                 weight_ans\n",
    "                 ):\n",
    "        self.doc_vectors = torch.FloatTensor(doc_vectors)\n",
    "        self.doc_w_sum = torch.FloatTensor(doc_w_sum)\n",
    "        self.weight_ans = weight_ans\n",
    "        assert len(doc_vectors) == len(doc_w_sum)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "                \n",
    "        return self.doc_vectors[idx], self.doc_w_sum[idx], idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doc_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa07e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR(nn.Module):\n",
    "    \"\"\"\n",
    "    Input shape: (N, 3, 64, 64)\n",
    "    Output shape: (N, )\n",
    "    \"\"\"\n",
    "    def __init__(self, num_doc, num_words):\n",
    "        super(LR, self).__init__()\n",
    "        weight = torch.zeros(num_doc, num_words).to(device)\n",
    "        self.emb = torch.nn.Embedding.from_pretrained(weight, freeze=False)\n",
    "        \n",
    "    def forward(self, doc_ids, word_vectors):\n",
    "        return self.emb(doc_ids) @ word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c135bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_NDCG(model, train_loader):\n",
    "    results = {}\n",
    "    model.eval()\n",
    "    \n",
    "    scores = np.array(model.emb.cpu().weight.data)\n",
    "    model.emb.to(device)\n",
    "    true_relevance = train_loader.dataset.weight_ans\n",
    "\n",
    "    # F1\n",
    "    F1s = []\n",
    "    for i in range(true_relevance.shape[0]):\n",
    "        one_hot_ans = np.arange(true_relevance.shape[1])[true_relevance[i] > 0]\n",
    "        pred = scores[i]\n",
    "        \n",
    "        F1 = []\n",
    "        for topk in config[\"topk\"]:\n",
    "            one_hot_pred = np.argsort(pred)[-topk:]\n",
    "            \n",
    "            hit = np.intersect1d(one_hot_pred, one_hot_ans)\n",
    "            percision = len(hit) / topk\n",
    "            recall = len(hit) / len(one_hot_ans)\n",
    "            \n",
    "            ans = 2 * percision * recall / (percision + recall) if (percision + recall) > 0 else 0\n",
    "            F1.append(ans)\n",
    "        F1s.append(F1)\n",
    "        \n",
    "    F1s = np.mean(F1s, axis=0)\n",
    "    \n",
    "    for i, topk in enumerate(config[\"topk\"]):\n",
    "        results['F1@{}'.format(topk)] = F1s[i]\n",
    "\n",
    "    # NDCG\n",
    "    for topk in config[\"topk\"]:\n",
    "        results['ndcg@{}'.format(topk)] = ndcg_score(true_relevance, scores, k=topk)\n",
    "    results['ndcg@all'] = ndcg_score(true_relevance, scores, k=None)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac91f68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "print('document num', train_size)\n",
    "\n",
    "train_dataset = Custom_Dataset(document_vectors[:train_size], document_answers_w[:train_size], weight_ans[:train_size])\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e533d31",
   "metadata": {},
   "source": [
    "## start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3f76e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting\n",
    "lr = 0.5\n",
    "momentum = 0.99\n",
    "weight_decay = 0\n",
    "nesterov = False # True\n",
    "\n",
    "n_epoch = our_lasso_epoch\n",
    "\n",
    "w_sum_reg = 1e-3\n",
    "w_sum_reg_mul = 0.9\n",
    "w_clip_value = 0\n",
    "\n",
    "L1 = 1e-6\n",
    "\n",
    "verbose = False\n",
    "valid_epoch = 100\n",
    "\n",
    "model = LR(num_doc=train_size, num_words=word_vectors.shape[0]).to(device)\n",
    "model.train()\n",
    "\n",
    "word_vectors_tensor = torch.FloatTensor(word_vectors).to(device)\n",
    "    \n",
    "opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay, nesterov=nesterov)\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "results = []\n",
    "step = 0\n",
    "for epoch in tqdm(range(n_epoch)):    \n",
    "    loss_mse_his = []\n",
    "    loss_w_reg_his = []\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:\n",
    "        doc_embs, doc_w_sum, doc_ids = data\n",
    "        \n",
    "        doc_embs = doc_embs.to(device)\n",
    "        doc_w_sum = doc_w_sum.to(device)\n",
    "        doc_ids = doc_ids.to(device)\n",
    "        \n",
    "        w_reg = (torch.ones(doc_embs.size(0), 1) * w_sum_reg_mul).to(device)\n",
    "        \n",
    "        # MSE loss\n",
    "        pred_doc_embs = model(doc_ids, word_vectors_tensor)     \n",
    "        loss_mse = criterion(pred_doc_embs, doc_embs)\n",
    "\n",
    "        pred_w_sum = torch.sum(model.emb(doc_ids), axis=1).view(-1, 1)\n",
    "        loss_w_reg = criterion(pred_w_sum, w_reg)\n",
    "        \n",
    "        loss_l1 = torch.sum(torch.abs(model.emb(doc_ids)))\n",
    "        loss = loss_mse + loss_w_reg * w_sum_reg + loss_l1 * L1\n",
    "        \n",
    "        # Model backwarding\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        loss_mse_his.append(loss_mse.item())\n",
    "        loss_w_reg_his.append(loss_w_reg.item())\n",
    "\n",
    "        for p in model.parameters():\n",
    "            p.data.clamp_(w_clip_value, float('inf'))\n",
    "\n",
    "        \n",
    "    if epoch % valid_epoch == 0:\n",
    "        res = {}\n",
    "        res['epoch'] = epoch\n",
    "        res['loss_mse'] = np.mean(loss_mse_his)\n",
    "        res['loss_w_reg'] = np.mean(loss_w_reg_his)\n",
    "        \n",
    "        res_ndcg = evaluate_NDCG(model, train_loader)\n",
    "        res.update(res_ndcg)\n",
    "        results.append(res)\n",
    "        \n",
    "        if verbose:\n",
    "            print()\n",
    "            for k, v in res.items():\n",
    "                print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49290d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "results_df = pd.DataFrame(results).set_index('epoch')\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60936db",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['model'] = 'our-lasso'\n",
    "final_results.append(results_df[select_columns].iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2115b07e",
   "metadata": {},
   "source": [
    "## Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5658c77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select doc_id and k\n",
    "doc_id = 90\n",
    "topk = 30\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31562cf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import colored\n",
    "from colored import stylize\n",
    "\n",
    "word_list = dataset.vocab.itos\n",
    "\n",
    "gt = [word_list[word_idx] for word_idx in np.argsort(weight_ans[doc_id])[::-1][:topk]]\n",
    "pred = [word_list[word_idx] for word_idx in np.argsort(model.emb.cpu().weight.data[doc_id].numpy())[::-1][:topk]]\n",
    "\n",
    "print('ground truth')\n",
    "for word in gt:\n",
    "    if word in pred:\n",
    "        print(stylize(word, colored.bg(\"yellow\")), end=' ')\n",
    "    else:\n",
    "        print(word, end=' ')\n",
    "\n",
    "print()\n",
    "print('\\nprediction')\n",
    "for word in pred:\n",
    "    if word in gt:\n",
    "        print(stylize(word, colored.bg(\"yellow\")), end=' ')\n",
    "    else:\n",
    "        print(word, end=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fa0add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw document\n",
    "print()\n",
    "ps = PorterStemmer()\n",
    "    \n",
    "for word in dataset.documents[doc_id].split():\n",
    "    word_stem = ps.stem(word)\n",
    "    if word_stem in gt:\n",
    "        if word_stem in pred:\n",
    "            print(stylize(word, colored.bg(\"yellow\")), end=' ')\n",
    "        else:\n",
    "            print(stylize(word, colored.bg(\"light_gray\")), end=' ')\n",
    "    else:\n",
    "        print(word, end=' ')\n",
    "# print(dataset.documents[doc_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc21f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "   \n",
    "scores = np.array(model.emb.weight.data)[doc_id].reshape(1, -1)\n",
    "true_relevance = train_loader.dataset.weight_ans[doc_id].reshape(1, -1)\n",
    "\n",
    "results['ndcg@50'] = (ndcg_score(true_relevance, scores, k=50))\n",
    "results['ndcg@100'] = (ndcg_score(true_relevance, scores, k=100))\n",
    "results['ndcg@200'] = (ndcg_score(true_relevance, scores, k=200))\n",
    "results['ndcg@all'] = (ndcg_score(true_relevance, scores, k=None))\n",
    "\n",
    "print('This document ndcg:')\n",
    "print('ground truth length:', np.sum(weight_ans[doc_id] > 0))\n",
    "print('NDCG top50', results['ndcg@50'])\n",
    "print('NDCG top100', results['ndcg@100'])\n",
    "print('NDCG top200', results['ndcg@200'])\n",
    "print('NDCG ALL', results['ndcg@all'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae49eac",
   "metadata": {},
   "source": [
    "## Final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb5ecc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_results_df = pd.DataFrame(final_results).reset_index(drop=True)\n",
    "\n",
    "experiment_dir = './records/dataset-{}-n_document-{}-wdist-{}-filtertopk-{}'.format(\n",
    "                                        config['dataset'],\n",
    "                                        config['n_document'],\n",
    "                                        config[\"document_vector_agg_weight\"],\n",
    "                                        config[\"topk_word_freq_threshold\"])\n",
    "\n",
    "print('Saving to directory', experiment_dir)\n",
    "os.makedirs(experiment_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b704cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results_df.to_csv(os.path.join(experiment_dir, 'result.csv'), index=False)\n",
    "\n",
    "import json\n",
    "with open(os.path.join(experiment_dir, 'config.json'), 'w') as json_file:\n",
    "    json.dump(config, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e6cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat in final_results_df.set_index('model').columns:\n",
    "    plt.bar(final_results_df['model'],\n",
    "            final_results_df[feat], \n",
    "            width=0.5, \n",
    "            bottom=None, \n",
    "            align='center', \n",
    "            color=['lightsteelblue', \n",
    "                   'cornflowerblue', \n",
    "                   'royalblue', \n",
    "                   'navy'])\n",
    "    plt.title(feat)\n",
    "    plt.savefig(os.path.join(experiment_dir, '{}.png'.format(feat)))\n",
    "    plt.clf()\n",
    "    if is_notebook:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c679b721",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52570ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
