{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a67d8234",
   "metadata": {},
   "source": [
    "### raw data\n",
    "* word embedding: glove\n",
    "* doc text: ./data/IMDB.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b007799a",
   "metadata": {},
   "source": [
    "### preprocess\n",
    "1. filter too frequent and less frequent words\n",
    "2. stemming\n",
    "3. document vector aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6a82ca",
   "metadata": {},
   "source": [
    "### model\n",
    "1. TopK\n",
    "2. Sklearn\n",
    "3. Our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b9967b",
   "metadata": {},
   "source": [
    "### evaluation\n",
    "1. F1\n",
    "2. NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e9beff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np \n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from itertools import cycle\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.metrics import ndcg_score\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2341254",
   "metadata": {},
   "source": [
    "## Preprocess config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dc01df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "config[\"n_document\"] = 10000\n",
    "config[\"min_word_freq_threshold\"] = 20\n",
    "config[\"topk_word_freq_threshold\"] = 500\n",
    "config[\"document_vector_agg\"] = 'TF-IDF'\n",
    "config[\"select_topk_TFIDF\"] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05c1627d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66c6ff10de44c3ba46fd76f5d87df25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:400000\n"
     ]
    }
   ],
   "source": [
    "# load word embedding\n",
    "embedding_file = \"../data/glove.6B.100d.txt\"\n",
    "\n",
    "word2embedding = dict()\n",
    "word_dim = int(re.findall(r\".(\\d+)d\",embedding_file)[0])\n",
    "\n",
    "with open(embedding_file,\"r\") as f:\n",
    "    for line in tqdm(f):\n",
    "        line = line.strip().split()\n",
    "        word = line[0]\n",
    "        embedding = list(map(float,line[1:]))\n",
    "        word2embedding[word] = embedding\n",
    "\n",
    "print(\"Number of words:%d\" % len(word2embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1233053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, word2embedding, min_word_freq_threshold=0, topk_word_freq_threshold=0):\n",
    "        # The low frequency words will be assigned as <UNK> token\n",
    "        self.itos = {0: \"<UNK>\"}\n",
    "        self.stoi = {\"<UNK>\": 0}\n",
    "        \n",
    "        self.word2embedding = word2embedding\n",
    "        self.min_word_freq_threshold = min_word_freq_threshold\n",
    "        self.topk_word_freq_threshold = topk_word_freq_threshold\n",
    "        \n",
    "        self.word_freq_in_corpus = defaultdict(int)\n",
    "        self.IDF = {}\n",
    "        self.ps = PorterStemmer()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "#     @staticmethod\n",
    "    def tokenizer_eng(self, text):\n",
    "        text = re.sub(r'[^A-Za-z0-9 ]+', '', text)\n",
    "        text = text.strip().split()\n",
    "        \n",
    "        return [self.ps.stem(w) for w in text]\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        \n",
    "        self.doc_freq = defaultdict(int) # # of document a word appear\n",
    "        self.document_num = len(sentence_list)\n",
    "        self.word_vectors = [[0]*word_dim] # unknown word emb\n",
    "        \n",
    "        for sentence in tqdm(sentence_list, desc=\"Preprocessing documents\"):\n",
    "            # for doc_freq\n",
    "            document_words = set()\n",
    "            \n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                # pass unknown word\n",
    "                if word not in self.word2embedding:\n",
    "                    continue\n",
    "                    \n",
    "                # calculate word freq\n",
    "                self.word_freq_in_corpus[word] += 1\n",
    "                document_words.add(word)\n",
    "                \n",
    "            for word in document_words:\n",
    "                self.doc_freq[word] += 1\n",
    "        \n",
    "        # calculate IDF\n",
    "        print('doc num', self.document_num)\n",
    "        for word, freq in self.doc_freq.items():\n",
    "            self.IDF[word] = math.log(self.document_num / (freq+1))\n",
    "        \n",
    "        # delete less freq words:\n",
    "        delete_words = []\n",
    "        for word, v in self.word_freq_in_corpus.items():\n",
    "            if v < self.min_word_freq_threshold:\n",
    "                delete_words.append(word)     \n",
    "        for word in delete_words:\n",
    "            del self.IDF[word]    \n",
    "            del self.word_freq_in_corpus[word]    \n",
    "        \n",
    "        # delete too freq words\n",
    "        print('eliminate freq words')\n",
    "        IDF = [(word, freq) for word, freq in self.IDF.items()]\n",
    "        IDF.sort(key=lambda x: x[1])\n",
    "\n",
    "        for i in range(self.topk_word_freq_threshold):\n",
    "            print(word)\n",
    "            word = IDF[i][0]\n",
    "            del self.IDF[word]\n",
    "            del self.word_freq_in_corpus[word]\n",
    "        \n",
    "        # construct word_vectors\n",
    "        idx = 1\n",
    "        for word in self.word_freq_in_corpus:\n",
    "            self.word_vectors.append(self.word2embedding[word])\n",
    "            self.stoi[word] = idx\n",
    "            self.itos[idx] = word\n",
    "            idx += 1\n",
    "            \n",
    "    def calculate_document_vector(self, sentence_list, agg, select_topk_TFIDF=None):\n",
    "        document_vectors = []\n",
    "        document_answers = []\n",
    "        document_answers_w = []\n",
    "        \n",
    "        for sentence in tqdm(sentence_list, desc=\"calculate document vectors\"):\n",
    "            document_vector = np.zeros(len(self.word_vectors[0]))\n",
    "            select_words = []\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                # pass unknown word\n",
    "                if word not in self.stoi:\n",
    "                    continue\n",
    "                else:\n",
    "                    select_words.append(word)\n",
    "\n",
    "            # select topk TDIDF\n",
    "            if select_topk_TFIDF is not None:\n",
    "                doc_TFIDF = defaultdict(float)\n",
    "                for word in select_words:    \n",
    "                    doc_TFIDF[word] += self.IDF[word]\n",
    "\n",
    "                doc_TFIDF_l = [(word, TFIDF) for word, TFIDF in doc_TFIDF.items()]\n",
    "                doc_TFIDF_l.sort(key=lambda x:x[1], reverse=True)\n",
    "                \n",
    "                select_topk_words = set(list(map(lambda x:x[0], doc_TFIDF_l[:select_topk_TFIDF])))\n",
    "                select_words = [word for word in select_words if word in select_topk_words]\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            total_weight = 0\n",
    "            # aggregate to doc vectors\n",
    "            for word in select_words:\n",
    "                if agg == 'mean':\n",
    "                    document_vector += self.word2embedding[word]\n",
    "                    total_weight += 1\n",
    "                elif agg == 'TF-IDF':\n",
    "                    document_vector += np.array(self.word2embedding[word]) * self.IDF[word]\n",
    "                    total_weight += self.IDF[word]\n",
    "\n",
    "            if len(select_words) == 0:\n",
    "                print('error', sentence)\n",
    "                continue\n",
    "            else:\n",
    "                document_vector /= len(select_words)\n",
    "                total_weight /= len(select_words)\n",
    "            \n",
    "            document_vectors.append(document_vector)\n",
    "            document_answers.append(select_words)\n",
    "            document_answers_w.append(total_weight)\n",
    "        \n",
    "        # get answers\n",
    "        document_answers_idx = []    \n",
    "        for ans in document_answers:\n",
    "            ans_idx = []\n",
    "            for token in ans:\n",
    "                if token in self.stoi:\n",
    "                    ans_idx.append(self.stoi[token])                    \n",
    "            document_answers_idx.append(ans_idx)\n",
    "            \n",
    "        return document_vectors, document_answers_idx, document_answers_w\n",
    "        \n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c5706a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBowDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 raw_data_file_path,\n",
    "                 word2embedding,\n",
    "                 skip_header = False,\n",
    "                 n_document = None, # read first n document\n",
    "                 min_word_freq_threshold = 20, # eliminate less freq words\n",
    "                 topk_word_freq_threshold = 5, # eliminate smallest k IDF words\n",
    "                 select_topk_TFIDF = None, # select topk tf-idf as ground-truth\n",
    "                 document_vector_agg = 'mean',\n",
    "                 ):\n",
    "\n",
    "        assert document_vector_agg in ['mean', 'TF-IDF']\n",
    "        \n",
    "        # raw documents\n",
    "        self.documents = []\n",
    "        \n",
    "        with open(raw_data_file_path,'r',encoding='utf-8') as f:\n",
    "            if skip_header:\n",
    "                f.readline()\n",
    "            for line in tqdm(f, desc=\"Loading documents\"):\n",
    "                # read firt n document\n",
    "                if n_document is not None and len(self.documents) >= n_document:\n",
    "                    break    \n",
    "                self.documents.append(line.strip(\"\\n\"))\n",
    "\n",
    "        # build vocabulary\n",
    "        self.vocab = Vocabulary(word2embedding, min_word_freq_threshold, topk_word_freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.documents)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "        # calculate document vectors\n",
    "        self.document_vectors, self.document_answers, self.document_answers_w = self.vocab.calculate_document_vector(self.documents, \\\n",
    "                                                                                           document_vector_agg, select_topk_TFIDF)\n",
    "        # train-test split\n",
    "        # training\n",
    "        self.train_split_ratio = 0.8\n",
    "        self.train_length = int(len(self.document_answers) * self.train_split_ratio)\n",
    "        self.train_vectors = self.document_vectors[:self.train_length]\n",
    "        self.train_words = self.document_answers[:self.train_length]\n",
    "        self.document_ids = list(range(self.train_length))\n",
    "        self.generator = cycle(self.context_target_generator())\n",
    "        self.dataset_size = sum([len(s) for s in self.train_words])\n",
    "        \n",
    "        # testing\n",
    "        self.test_vectors = self.document_vectors[self.train_length:]\n",
    "        self.test_words = self.document_answers[self.train_length:]\n",
    "\n",
    "    def context_target_generator(self):\n",
    "        np.random.shuffle(self.document_ids) # inplace shuffle\n",
    "\n",
    "        # randomly select a document and create its training example\n",
    "        for document_id in self.document_ids: \n",
    "            word_list = set(self.train_words[document_id])\n",
    "            negative_sample_space = list(set(range(self.vocab_size)) - word_list)\n",
    "            negative_samples = np.random.choice(negative_sample_space,size=len(word_list),replace = False)\n",
    "            for word_id, negative_wordID in zip(word_list, negative_samples):\n",
    "                yield [document_id, word_id, negative_wordID]\n",
    "                \n",
    "    def __getitem__(self, idx):\n",
    "        doc_id, word_id, negative_wordID = next(self.generator)\n",
    "        doc_id = torch.FloatTensor(self.document_vectors[doc_id])\n",
    "        word_id = torch.FloatTensor(self.vocab.word_vectors[word_id])\n",
    "        negative_word = torch.FloatTensor(self.vocab.word_vectors[negative_wordID])\n",
    "\n",
    "        return doc_id, word_id, negative_word\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1a7186c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585b56e6195a41da8db8e749e169e2e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading documents: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a34df36059a4fd1aa78e34c1b4ebae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing documents:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc num 10000\n",
      "eliminate freq words\n",
      "boel\n",
      "the\n",
      "and\n",
      "a\n",
      "of\n",
      "to\n",
      "is\n",
      "it\n",
      "thi\n",
      "in\n",
      "that\n",
      "i\n",
      "for\n",
      "with\n",
      "but\n",
      "as\n",
      "on\n",
      "wa\n",
      "be\n",
      "film\n",
      "one\n",
      "not\n",
      "are\n",
      "have\n",
      "all\n",
      "an\n",
      "you\n",
      "at\n",
      "by\n",
      "from\n",
      "who\n",
      "like\n",
      "hi\n",
      "ha\n",
      "so\n",
      "he\n",
      "time\n",
      "about\n",
      "out\n",
      "there\n",
      "if\n",
      "veri\n",
      "see\n",
      "good\n",
      "what\n",
      "more\n",
      "they\n",
      "when\n",
      "just\n",
      "some\n",
      "or\n",
      "make\n",
      "watch\n",
      "great\n",
      "get\n",
      "well\n",
      "my\n",
      "other\n",
      "up\n",
      "can\n",
      "love\n",
      "also\n",
      "which\n",
      "would\n",
      "their\n",
      "will\n",
      "even\n",
      "most\n",
      "her\n",
      "me\n",
      "had\n",
      "much\n",
      "than\n",
      "first\n",
      "do\n",
      "way\n",
      "into\n",
      "play\n",
      "end\n",
      "were\n",
      "no\n",
      "best\n",
      "scene\n",
      "think\n",
      "been\n",
      "how\n",
      "go\n",
      "look\n",
      "show\n",
      "made\n",
      "she\n",
      "after\n",
      "we\n",
      "year\n",
      "mani\n",
      "work\n",
      "know\n",
      "too\n",
      "seen\n",
      "act\n",
      "him\n",
      "them\n",
      "come\n",
      "thing\n",
      "perform\n",
      "two\n",
      "life\n",
      "still\n",
      "never\n",
      "take\n",
      "dont\n",
      "could\n",
      "give\n",
      "say\n",
      "then\n",
      "actor\n",
      "ani\n",
      "doe\n",
      "your\n",
      "where\n",
      "seem\n",
      "find\n",
      "enjoy\n",
      "want\n",
      "ever\n",
      "while\n",
      "man\n",
      "did\n",
      "over\n",
      "cast\n",
      "feel\n",
      "here\n",
      "such\n",
      "back\n",
      "these\n",
      "part\n",
      "those\n",
      "lot\n",
      "live\n",
      "tri\n",
      "role\n",
      "plot\n",
      "wonder\n",
      "interest\n",
      "use\n",
      "though\n",
      "better\n",
      "through\n",
      "now\n",
      "real\n",
      "off\n",
      "new\n",
      "befor\n",
      "world\n",
      "should\n",
      "set\n",
      "both\n",
      "quit\n",
      "again\n",
      "alway\n",
      "day\n",
      "director\n",
      "star\n",
      "young\n",
      "actual\n",
      "few\n",
      "own\n",
      "old\n",
      "same\n",
      "doesnt\n",
      "music\n",
      "direct\n",
      "may\n",
      "excel\n",
      "right\n",
      "fact\n",
      "bit\n",
      "start\n",
      "im\n",
      "turn\n",
      "whi\n",
      "between\n",
      "us\n",
      "saw\n",
      "without\n",
      "thought\n",
      "person\n",
      "long\n",
      "bad\n",
      "point\n",
      "down\n",
      "fan\n",
      "big\n",
      "recommend\n",
      "differ\n",
      "didnt\n",
      "around\n",
      "final\n",
      "must\n",
      "got\n",
      "last\n",
      "effect\n",
      "entertain\n",
      "place\n",
      "done\n",
      "cant\n",
      "need\n",
      "ive\n",
      "tell\n",
      "happen\n",
      "origin\n",
      "almost\n",
      "friend\n",
      "sinc\n",
      "put\n",
      "begin\n",
      "fun\n",
      "help\n",
      "although\n",
      "girl\n",
      "move\n",
      "each\n",
      "action\n",
      "true\n",
      "yet\n",
      "job\n",
      "enough\n",
      "keep\n",
      "sure\n",
      "line\n",
      "cours\n",
      "moment\n",
      "expect\n",
      "screen\n",
      "perfect\n",
      "lead\n",
      "onc\n",
      "classic\n",
      "view\n",
      "away\n",
      "far\n",
      "anyon\n",
      "woman\n",
      "worth\n",
      "kind\n",
      "whole\n",
      "found\n",
      "guy\n",
      "dvd\n",
      "isnt\n",
      "rather\n",
      "name\n",
      "seri\n",
      "later\n",
      "noth\n",
      "am\n",
      "hard\n",
      "follow\n",
      "our\n",
      "laugh\n",
      "mean\n",
      "nice\n",
      "reason\n",
      "goe\n",
      "might\n",
      "appear\n",
      "hope\n",
      "let\n",
      "shot\n",
      "includ\n",
      "run\n",
      "portray\n",
      "least\n",
      "himself\n",
      "tv\n",
      "complet\n",
      "read\n",
      "kill\n",
      "call\n",
      "john\n",
      "american\n",
      "understand\n",
      "viewer\n",
      "human\n",
      "miss\n",
      "face\n",
      "night\n",
      "favorit\n",
      "dure\n",
      "mind\n",
      "script\n",
      "chang\n",
      "power\n",
      "special\n",
      "sens\n",
      "eye\n",
      "second\n",
      "kid\n",
      "bring\n",
      "along\n",
      "three\n",
      "wife\n",
      "10\n",
      "usual\n",
      "open\n",
      "hollywood\n",
      "touch\n",
      "said\n",
      "main\n",
      "book\n",
      "left\n",
      "until\n",
      "meet\n",
      "creat\n",
      "age\n",
      "father\n",
      "support\n",
      "home\n",
      "death\n",
      "idea\n",
      "short\n",
      "came\n",
      "hand\n",
      "often\n",
      "boy\n",
      "top\n",
      "today\n",
      "high\n",
      "small\n",
      "product\n",
      "care\n",
      "horror\n",
      "problem\n",
      "gener\n",
      "heart\n",
      "brilliant\n",
      "full\n",
      "fall\n",
      "men\n",
      "version\n",
      "style\n",
      "murder\n",
      "less\n",
      "fine\n",
      "natur\n",
      "hous\n",
      "sound\n",
      "next\n",
      "base\n",
      "rest\n",
      "impress\n",
      "rate\n",
      "overal\n",
      "absolut\n",
      "given\n",
      "els\n",
      "drama\n",
      "talk\n",
      "sever\n",
      "die\n",
      "close\n",
      "war\n",
      "throughout\n",
      "head\n",
      "present\n",
      "humor\n",
      "wasnt\n",
      "except\n",
      "talent\n",
      "word\n",
      "song\n",
      "case\n",
      "review\n",
      "black\n",
      "cinema\n",
      "fight\n",
      "relationship\n",
      "imagin\n",
      "light\n",
      "dark\n",
      "success\n",
      "write\n",
      "late\n",
      "actress\n",
      "against\n",
      "deal\n",
      "side\n",
      "comment\n",
      "itself\n",
      "anim\n",
      "lost\n",
      "past\n",
      "develop\n",
      "learn\n",
      "art\n",
      "strong\n",
      "hour\n",
      "total\n",
      "stand\n",
      "instead\n",
      "written\n",
      "sort\n",
      "2\n",
      "mother\n",
      "abl\n",
      "theme\n",
      "either\n",
      "wish\n",
      "brother\n",
      "score\n",
      "matter\n",
      "wrong\n",
      "ye\n",
      "mr\n",
      "hit\n",
      "women\n",
      "thank\n",
      "recent\n",
      "soon\n",
      "ask\n",
      "felt\n",
      "camera\n",
      "mention\n",
      "wont\n",
      "import\n",
      "twist\n",
      "under\n",
      "event\n",
      "went\n",
      "citi\n",
      "children\n",
      "happi\n",
      "number\n",
      "type\n",
      "money\n",
      "behind\n",
      "attempt\n",
      "ago\n",
      "wait\n",
      "youll\n",
      "son\n",
      "id\n",
      "white\n",
      "school\n",
      "charm\n",
      "writer\n",
      "video\n",
      "stop\n",
      "question\n",
      "add\n",
      "element\n",
      "disappoint\n",
      "pace\n",
      "dead\n",
      "hold\n",
      "robert\n",
      "jame\n",
      "lack\n",
      "critic\n",
      "told\n",
      "dream\n",
      "evil\n",
      "visual\n",
      "heard\n",
      "daughter\n",
      "superb\n",
      "child\n",
      "order\n",
      "credit\n",
      "somewhat\n",
      "return\n",
      "known\n",
      "comic\n",
      "stay\n",
      "provid\n",
      "major\n",
      "strang\n",
      "career\n",
      "similar\n",
      "basic\n",
      "gave\n",
      "michael\n",
      "took\n",
      "note\n",
      "form\n",
      "opinion\n",
      "rare\n",
      "allow\n",
      "offer\n",
      "remind\n",
      "anyway\n",
      "greatest\n",
      "whose\n",
      "oscar\n",
      "level\n",
      "tale\n",
      "ill\n",
      "alon\n",
      "husband\n",
      "guess\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f56709a11d74c47a52bab94bd0aa38a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "calculate document vectors:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error i think it's one of the greatest movies which are ever made ,  and i've seen many .  .  .  the book is better ,  but it's still a very good movie ! \n",
      "error all this talk about this being a bad movie is nonsense .  as a matter of fact this is the best movie i've ever seen .  it's an excellent story and the actors in the movie are some of the best .  i would not give criticism to any of the actors .  that movie is the best and it will always stay that way . \n",
      "error smallville episode justice is the best episode of smallville  !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !  it's my favorite episode of smallville !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   ! \n",
      "error smallville episode justice is the best episode of smallville  !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !  it's my favorite episode of smallville !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   !   ! \n",
      "error this is actually one of my favorite films ,  i would recommend that everyone watches it .  there is some great acting in it and it shows that not all  \" good \"  films are american .  .  .  . \n",
      "error i thought it was an original story ,  very nicely told .  i think all you people are expecting too much .  i mean .  .  . it's just a made for television movie !  what are you expecting ?  some great wonderful dramtic piece ?  i thought it was a really great story for a made for television movie .  .  .  . and that's my opinion . \n",
      "error it's highly stylized ,  but this movie shows that real people appear on these shows and what seems like good fun and a chance to appear on television can have serious consequences .   yes ,  i's mostly comedy ,  but there are some sad moments . \n",
      "Finish building dataset!\n",
      "Number of documents:10000\n",
      "Number of words:4127\n"
     ]
    }
   ],
   "source": [
    "# load and build torch dataset\n",
    "data_file_path = '../data/IMDB.txt'\n",
    "\n",
    "print(\"Building dataset....\")\n",
    "dataset = CBowDataset(\n",
    "                    raw_data_file_path=data_file_path,\n",
    "                    word2embedding=word2embedding,\n",
    "                    skip_header=False,\n",
    "                    n_document = config[\"n_document\"],\n",
    "                    min_word_freq_threshold = config[\"min_word_freq_threshold\"],\n",
    "                    topk_word_freq_threshold = config[\"topk_word_freq_threshold\"],\n",
    "                    document_vector_agg = config[\"document_vector_agg\"],\n",
    "                    select_topk_TFIDF = config[\"select_topk_TFIDF\"]\n",
    "                    )\n",
    "print(\"Finish building dataset!\")\n",
    "print(f\"Number of documents:{len(dataset.documents)}\")\n",
    "print(f\"Number of words:{dataset.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba4758da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# check test doc vectors' correctness\n",
    "word_vectors = np.array(dataset.vocab.word_vectors)\n",
    "word_vectors.shape\n",
    "\n",
    "pred = np.zeros(100)\n",
    "cnt = 0\n",
    "for word_idx in dataset.test_words[0]:\n",
    "    pred += word_vectors[word_idx] * dataset.vocab.IDF[dataset.vocab.itos[word_idx]]\n",
    "    cnt += 1\n",
    "print(dataset.test_vectors[0] - pred/cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe65f37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9993, 4127)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833950b626004351a27c275e83c76875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## create tfidf_ans\n",
    "document_answers = dataset.document_answers\n",
    "\n",
    "onehot_ans = np.zeros((len(document_answers), word_vectors.shape[0]))\n",
    "tfidf_ans = np.zeros((len(document_answers), word_vectors.shape[0]))\n",
    "print(tfidf_ans.shape)\n",
    "\n",
    "for i in tqdm(range(len(document_answers))):\n",
    "    for word_idx in document_answers[i]:\n",
    "        tfidf_ans[i, word_idx] += dataset.vocab.IDF[dataset.vocab.itos[word_idx]]\n",
    "        onehot_ans[i, word_idx] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d31a21",
   "metadata": {},
   "source": [
    "## Top K freq word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f81780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ans = dataset.document_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1efec49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('game', 783),\n",
       " ('season', 671),\n",
       " ('david', 549),\n",
       " ('ladi', 538),\n",
       " ('jack', 536),\n",
       " ('killer', 527),\n",
       " ('sex', 524),\n",
       " ('town', 524),\n",
       " ('king', 512),\n",
       " ('novel', 496)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = [(word, freq) for word, freq in dataset.vocab.word_freq_in_corpus.items()]\n",
    "word_freq.sort(key=lambda x:x[1], reverse=True)\n",
    "word_freq[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a04793e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1724ce6189ff4a9b9371cd690429a8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 50 word\n",
      "percision 0.035060542379665764\n",
      "recall 0.06100105864457941\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c835688b7d54ff9be3e11a8fef5a016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 100 word\n",
      "percision 0.034443110177123995\n",
      "recall 0.11986880156935618\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d712aec95c14396b81710aeeb09f517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 200 word\n",
      "percision 0.03148153707595318\n",
      "recall 0.22046597654109124\n"
     ]
    }
   ],
   "source": [
    "def topk_word_evaluation(k=50):\n",
    "    topk_word = [word for (word, freq) in word_freq[:k]]\n",
    "\n",
    "    pr, re = [], []\n",
    "    for ans in tqdm(test_ans):\n",
    "        ans = set(ans)\n",
    "        ans = [dataset.vocab.itos[a] for a in ans]\n",
    "\n",
    "        hit = []\n",
    "        for word in ans:\n",
    "            if word in topk_word:\n",
    "                hit.append(word)\n",
    "\n",
    "        precision = len(hit) / k\n",
    "        recall = len(hit) / len(ans)\n",
    "        pr.append(precision)\n",
    "        re.append(recall)\n",
    "\n",
    "    print('top {} word'.format(k))\n",
    "    print('percision', np.mean(pr))\n",
    "    print('recall', np.mean(re))\n",
    "\n",
    "topk_word_evaluation(k=50)\n",
    "topk_word_evaluation(k=100)\n",
    "topk_word_evaluation(k=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26308789",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab544cb4ecd94aa8bc9c8aca45193693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 50 NDCG:0.033762099027129774\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1761706dfed340d69f85390f3422f8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 100 NDCG:0.050457240136176806\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0dd095f280e4d24aa5ae59501dbde4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 200 NDCG:0.07732441220294914\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a99265d296403a892112a689bdafcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9993 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top None NDCG:0.2833640263103313\n"
     ]
    }
   ],
   "source": [
    "def topk_word_evaluation_NDCG(k=50):\n",
    "    freq_word =[word for (word, freq) in word_freq]\n",
    "    freq_word_idx = [dataset.vocab.stoi[word] for word in freq_word if word in dataset.vocab.stoi]\n",
    "    \n",
    "    scores = np.zeros(len(dataset.vocab.word_vectors))\n",
    "    for rank, idx in enumerate(freq_word_idx):\n",
    "        scores[idx] = len(dataset.vocab.word_vectors) - rank\n",
    "    \n",
    "    NDCGs = []\n",
    "    \n",
    "    for ans in tqdm(test_ans):\n",
    "        TFIDF_ans = np.zeros(len(dataset.vocab.word_vectors))\n",
    "        \n",
    "        for word_idx in ans:\n",
    "            if word_idx == 0:\n",
    "                continue\n",
    "            word = dataset.vocab.itos[word_idx]\n",
    "            TFIDF_ans[word_idx] += dataset.vocab.IDF[word]\n",
    "\n",
    "        NDCG_score = ndcg_score(TFIDF_ans.reshape(1,-1), scores.reshape(1,-1), k=k)\n",
    "        NDCGs.append(NDCG_score)\n",
    "\n",
    "    print('top {} NDCG:{}'.format(k, np.mean(NDCGs)))\n",
    "\n",
    "topk_word_evaluation_NDCG(k=50)\n",
    "topk_word_evaluation_NDCG(k=100)\n",
    "topk_word_evaluation_NDCG(k=200)\n",
    "topk_word_evaluation_NDCG(k=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea304b5",
   "metadata": {},
   "source": [
    "## Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4fcb3a",
   "metadata": {},
   "source": [
    "## Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "854f9586",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Dataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 doc_vectors,\n",
    "                 doc_w_sum,\n",
    "                 tfidf_ans\n",
    "                 ):\n",
    "        self.doc_vectors = torch.FloatTensor(doc_vectors)\n",
    "        self.doc_w_sum = torch.FloatTensor(doc_w_sum)\n",
    "        self.tfidf_ans = tfidf_ans\n",
    "        assert len(doc_vectors) == len(doc_w_sum)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "                \n",
    "        return self.doc_vectors[idx], self.doc_w_sum[idx], idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doc_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0aa07e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR(nn.Module):\n",
    "    \"\"\"\n",
    "    Input shape: (N, 3, 64, 64)\n",
    "    Output shape: (N, )\n",
    "    \"\"\"\n",
    "    def __init__(self, num_doc, num_words):\n",
    "        super(LR, self).__init__()\n",
    "        weight = torch.zeros(num_doc, num_words)\n",
    "        self.emb = torch.nn.Embedding.from_pretrained(weight, freeze=False)\n",
    "        \n",
    "    def forward(self, doc_ids, word_vectors):\n",
    "        return self.emb(doc_ids) @ word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c135bd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_NDCG(model, train_loader):\n",
    "    results = {}\n",
    "    model.eval()\n",
    "    \n",
    "    scores = np.array(model.emb.weight.data)\n",
    "    true_relevance = train_loader.dataset.tfidf_ans\n",
    "        \n",
    "    results['ndcg@50'] = (ndcg_score(true_relevance, scores, k=50))\n",
    "    results['ndcg@100'] = (ndcg_score(true_relevance, scores, k=100))\n",
    "    results['ndcg@200'] = (ndcg_score(true_relevance, scores, k=200))\n",
    "    results['ndcg@all'] = (ndcg_score(true_relevance, scores, k=None))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ac91f68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document num 999\n"
     ]
    }
   ],
   "source": [
    "batch_size = 200\n",
    "train_size_ratio = 0.1\n",
    "\n",
    "train_size = int(len(dataset.document_vectors) * train_size_ratio)\n",
    "print('document num', train_size)\n",
    "\n",
    "document_vectors = np.array(dataset.document_vectors)\n",
    "document_answers_w = np.array(dataset.document_answers_w).reshape(-1, 1)\n",
    "\n",
    "train_dataset = Custom_Dataset(document_vectors[:train_size], document_answers_w[:train_size], tfidf_ans[:train_size])\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e533d31",
   "metadata": {},
   "source": [
    "## start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d389257f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43cdffebd9cc42e5af632af4d6f3c3fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "loss_mse 1.1490628719329834\n",
      "loss_w_reg 12.81353816986084\n",
      "ndcg@50 0.2393351073073729\n",
      "ndcg@100 0.2648776289053164\n",
      "ndcg@200 0.29479027112870226\n",
      "ndcg@all 0.4482029312540207\n",
      "epoch 100\n",
      "loss_mse 0.006500968988984823\n",
      "loss_w_reg 9.558284950256347\n",
      "ndcg@50 0.569417324314581\n",
      "ndcg@100 0.5995820158353545\n",
      "ndcg@200 0.6308079438538046\n",
      "ndcg@all 0.7198786361148665\n",
      "epoch 200\n",
      "loss_mse 0.0034469071310013534\n",
      "loss_w_reg 7.397932720184326\n",
      "ndcg@50 0.5906546487545926\n",
      "ndcg@100 0.6189840169533798\n",
      "ndcg@200 0.6503873998819233\n",
      "ndcg@all 0.7347971424377536\n",
      "epoch 300\n",
      "loss_mse 0.002494489448145032\n",
      "loss_w_reg 6.34325008392334\n",
      "ndcg@50 0.601336915754799\n",
      "ndcg@100 0.6290661594206691\n",
      "ndcg@200 0.6598518419501244\n",
      "ndcg@all 0.7421291650375673\n",
      "epoch 400\n",
      "loss_mse 0.001994803408160806\n",
      "loss_w_reg 5.646750640869141\n",
      "ndcg@50 0.6091635519733181\n",
      "ndcg@100 0.6365924641499666\n",
      "ndcg@200 0.6673771276506394\n",
      "ndcg@all 0.7476708711353401\n",
      "epoch 500\n",
      "loss_mse 0.0016836134251207112\n",
      "loss_w_reg 5.1406303405761715\n",
      "ndcg@50 0.6148230785715703\n",
      "ndcg@100 0.6422229196404209\n",
      "ndcg@200 0.6727560791878983\n",
      "ndcg@all 0.7517038857689283\n",
      "epoch 600\n",
      "loss_mse 0.0014685020549222827\n",
      "loss_w_reg 4.744681549072266\n",
      "ndcg@50 0.6193347656336949\n",
      "ndcg@100 0.6465818738675999\n",
      "ndcg@200 0.6769011864311398\n",
      "ndcg@all 0.7546530777135008\n",
      "epoch 700\n",
      "loss_mse 0.001309207477606833\n",
      "loss_w_reg 4.4240045070648195\n",
      "ndcg@50 0.6235413833549909\n",
      "ndcg@100 0.6508141391100133\n",
      "ndcg@200 0.6806606767746025\n",
      "ndcg@all 0.7575685326639825\n",
      "epoch 800\n",
      "loss_mse 0.0011855014367029072\n",
      "loss_w_reg 4.156520557403565\n",
      "ndcg@50 0.6272867277497609\n",
      "ndcg@100 0.6541229848657444\n",
      "ndcg@200 0.6837105665514508\n",
      "ndcg@all 0.7599364788125402\n",
      "epoch 900\n",
      "loss_mse 0.0010865732794627548\n",
      "loss_w_reg 3.930256462097168\n",
      "ndcg@50 0.630227840647964\n",
      "ndcg@100 0.6569169896218643\n",
      "ndcg@200 0.6864161887480343\n",
      "ndcg@all 0.7618784363978747\n",
      "epoch 1000\n",
      "loss_mse 0.0010046045994386078\n",
      "loss_w_reg 3.7334775924682617\n",
      "ndcg@50 0.6331138790981264\n",
      "ndcg@100 0.6597061084665025\n",
      "ndcg@200 0.6891007454016814\n",
      "ndcg@all 0.7638918742539766\n",
      "epoch 1100\n",
      "loss_mse 0.0009361417032778263\n",
      "loss_w_reg 3.5619994163513184\n",
      "ndcg@50 0.6354716848979093\n",
      "ndcg@100 0.6621209124720733\n",
      "ndcg@200 0.6912048620408322\n",
      "ndcg@all 0.7655503681822762\n",
      "epoch 1200\n",
      "loss_mse 0.0008774997084401547\n",
      "loss_w_reg 3.41050329208374\n",
      "ndcg@50 0.637956363476616\n",
      "ndcg@100 0.6644366318069566\n",
      "ndcg@200 0.6932959030244532\n",
      "ndcg@all 0.7671691301103984\n",
      "epoch 1300\n",
      "loss_mse 0.0008265567128546536\n",
      "loss_w_reg 3.274407720565796\n",
      "ndcg@50 0.6399924664438543\n",
      "ndcg@100 0.6663488400812702\n",
      "ndcg@200 0.6953016809883906\n",
      "ndcg@all 0.768613104377629\n",
      "epoch 1400\n",
      "loss_mse 0.0007826985674910247\n",
      "loss_w_reg 3.1519290447235107\n",
      "ndcg@50 0.6419702924167563\n",
      "ndcg@100 0.6681815512232114\n",
      "ndcg@200 0.6971422832265047\n",
      "ndcg@all 0.7700705299423014\n",
      "epoch 1500\n",
      "loss_mse 0.0007436685147695244\n",
      "loss_w_reg 3.0409267425537108\n",
      "ndcg@50 0.6441126435330893\n",
      "ndcg@100 0.6701824615548923\n",
      "ndcg@200 0.6989781746148086\n",
      "ndcg@all 0.7715400161451083\n",
      "epoch 1600\n",
      "loss_mse 0.0007085324497893453\n",
      "loss_w_reg 2.9382973670959474\n",
      "ndcg@50 0.646217726657873\n",
      "ndcg@100 0.6720811842011876\n",
      "ndcg@200 0.7009750861536599\n",
      "ndcg@all 0.7730252268171048\n",
      "epoch 1700\n",
      "loss_mse 0.0006774942041374743\n",
      "loss_w_reg 2.8455646514892576\n",
      "ndcg@50 0.6479011866373838\n",
      "ndcg@100 0.6737768284565571\n",
      "ndcg@200 0.7026468631878516\n",
      "ndcg@all 0.774274182123022\n",
      "epoch 1800\n",
      "loss_mse 0.0006489817751571536\n",
      "loss_w_reg 2.758875274658203\n",
      "ndcg@50 0.6495716998732002\n",
      "ndcg@100 0.6754771158627777\n",
      "ndcg@200 0.7042884418047927\n",
      "ndcg@all 0.7755556474336357\n",
      "epoch 1900\n",
      "loss_mse 0.0006229468039236963\n",
      "loss_w_reg 2.67864933013916\n",
      "ndcg@50 0.6511623213710446\n",
      "ndcg@100 0.6766904025864494\n",
      "ndcg@200 0.7054035886799339\n",
      "ndcg@all 0.7765554809450576\n",
      "epoch 2000\n",
      "loss_mse 0.000599169498309493\n",
      "loss_w_reg 2.604190301895142\n",
      "ndcg@50 0.6524735938746109\n",
      "ndcg@100 0.677981928003778\n",
      "ndcg@200 0.7062490281569841\n",
      "ndcg@all 0.777311902814019\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2fd3a884518e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mloss_mse_his\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_mse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtual_env/py37/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtual_env/py37/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtual_env/py37/lib/python3.7/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    115\u001b[0m                   \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                   \u001b[0mdampening\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdampening\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                   nesterov=nesterov)\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;31m# update momentum_buffers in state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/virtual_env/py37/lib/python3.7/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0mmomentum_buffer_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdampening\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# setting\n",
    "lr = 0.02\n",
    "momentum = 0.99\n",
    "weight_decay = 0\n",
    "nesterov = False # True\n",
    "\n",
    "n_epoch = 5000\n",
    "\n",
    "w_sum_reg = 1e-3\n",
    "w_sum_reg_mul = 0.8\n",
    "w_clip_value = 0\n",
    "\n",
    "verbose = True\n",
    "valid_epoch = 100\n",
    "\n",
    "model = LR(num_doc=train_size, num_words=word_vectors.shape[0])\n",
    "model.train()\n",
    "\n",
    "word_vectors_tensor = torch.FloatTensor(word_vectors)\n",
    "\n",
    "opt = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay, nesterov=nesterov)\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "results = []\n",
    "step = 0\n",
    "for epoch in tqdm(range(n_epoch)):    \n",
    "    loss_mse_his = []\n",
    "    loss_w_reg_his = []\n",
    "    \n",
    "    model.train()\n",
    "    for data in train_loader:\n",
    "        doc_embs, doc_w_sum, doc_ids = data\n",
    "        # MSE loss\n",
    "        pred_doc_embs = model(doc_ids, word_vectors_tensor)     \n",
    "        loss_mse = criterion(pred_doc_embs, doc_embs)\n",
    "\n",
    "        pred_w_sum = torch.sum(model.emb(doc_ids), axis=1).view(-1, 1)\n",
    "        loss_w_reg = criterion(pred_w_sum, doc_w_sum * w_sum_reg_mul)\n",
    "        \n",
    "        loss = loss_mse + loss_w_reg * w_sum_reg\n",
    "        \n",
    "        # Model backwarding\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        loss_mse_his.append(loss_mse.item())\n",
    "        loss_w_reg_his.append(loss_w_reg.item())\n",
    "\n",
    "        for p in model.parameters():\n",
    "            p.data.clamp_(w_clip_value, float('inf'))\n",
    "\n",
    "        \n",
    "    if epoch % valid_epoch == 0:\n",
    "        res = {}\n",
    "        res['epoch'] = epoch\n",
    "        res['loss_mse'] = np.mean(loss_mse_his)\n",
    "        res['loss_w_reg'] = np.mean(loss_w_reg_his)\n",
    "        \n",
    "        res_ndcg = evaluate_NDCG(model, train_loader)\n",
    "        res.update(res_ndcg)\n",
    "        results.append(res)\n",
    "        \n",
    "        if verbose:\n",
    "            for k, v in res.items():\n",
    "                print(k, v)\n",
    "\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c90f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.groupby(by=['epoch']).mean().plot()\n",
    "results_df.groupby(by=['epoch']).mean()\n",
    "# results_df.groupby(by=['epoch']).mean().iloc[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2115b07e",
   "metadata": {},
   "source": [
    "## Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5658c77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LR(\n",
       "  (emb): Embedding(999, 4127)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select doc_id and k\n",
    "doc_id = 4\n",
    "topk = 50\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31562cf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth\n",
      "\u001b[48;5;3mwarren\u001b[0m \u001b[48;5;3mbrook\u001b[0m sailor \u001b[48;5;3mmel\u001b[0m flesh \u001b[48;5;3mslapstick\u001b[0m \u001b[48;5;3mlesli\u001b[0m price \u001b[48;5;3mann\u001b[0m rent room speak cut inherit richard warp process interpret montag ebert roger 3rd spectacular grin pg g x mislead humour couldnt mankind rapid complex spiritu notion 4th rid dean dimension krell list storm struck hors discern stone cum soap detect current \n",
      "\n",
      "prediction\n",
      "\u001b[48;5;3mbrook\u001b[0m \u001b[48;5;3mwarren\u001b[0m \u001b[48;5;3mlesli\u001b[0m merril \u001b[48;5;3mmel\u001b[0m worker dana wood \u001b[48;5;3mann\u001b[0m cabin bacon charli jim cliff walk scarlett bargain \u001b[48;5;3mslapstick\u001b[0m oil duck chuck columbia farmer swim stream spike jack hickock meadow cunningham student gershwin matthau craig neeson liam hill harrow deeper christin lower bend robin brent bill prison river darker lane regret "
     ]
    }
   ],
   "source": [
    "import colored\n",
    "from colored import stylize\n",
    "\n",
    "word_list = dataset.vocab.itos\n",
    "\n",
    "gt = [word_list[word_idx] for word_idx in np.argsort(tfidf_ans[doc_id])[::-1][:topk]]\n",
    "pred = [word_list[word_idx] for word_idx in np.argsort(model.emb.weight.data[doc_id].numpy())[::-1][:topk]]\n",
    "\n",
    "print('ground truth')\n",
    "for word in gt:\n",
    "    if word in pred:\n",
    "        print(stylize(word, colored.bg(\"yellow\")), end=' ')\n",
    "    else:\n",
    "        print(word, end=' ')\n",
    "\n",
    "print()\n",
    "print('\\nprediction')\n",
    "for word in pred:\n",
    "    if word in gt:\n",
    "        print(stylize(word, colored.bg(\"yellow\")), end=' ')\n",
    "    else:\n",
    "        print(word, end=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55fa0add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is not the typical mel brooks film .  it was much less slapstick than most of his movies and actually had a plot that was followable .  leslie ann warren made the movie ,  she is such a fantastic ,  under-rated actress .  there were some moments that could have been fleshed out a bit more ,  and some scenes that could probably have been cut to make the room to do so ,  but all in all ,  this is worth the price to rent and see it .  the acting was good overall ,  brooks himself did a good job without his characteristic speaking to directly to the audience .  again ,  warren was the best actor in the movie ,  but  \" fume \"  and  \" sailor \"  both played their parts well . '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw document\n",
    "dataset.documents[doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fc21f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG top50 0.6997726360764731\n",
      "NDCG top100 0.6997726360764731\n",
      "NDCG top200 0.7617430914633224\n",
      "NDCG ALL 0.807197545132113\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "   \n",
    "scores = np.array(model.emb.weight.data)[doc_id].reshape(1, -1)\n",
    "true_relevance = train_loader.dataset.tfidf_ans[doc_id].reshape(1, -1)\n",
    "\n",
    "results['ndcg@50'] = (ndcg_score(true_relevance, scores, k=50))\n",
    "results['ndcg@100'] = (ndcg_score(true_relevance, scores, k=100))\n",
    "results['ndcg@200'] = (ndcg_score(true_relevance, scores, k=200))\n",
    "results['ndcg@all'] = (ndcg_score(true_relevance, scores, k=None))\n",
    "\n",
    "print('NDCG top50', results['ndcg@50'])\n",
    "print('NDCG top100', results['ndcg@100'])\n",
    "print('NDCG top200', results['ndcg@200'])\n",
    "print('NDCG ALL', results['ndcg@all'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
