{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "982acb8f",
   "metadata": {},
   "source": [
    "## Siamese network \n",
    "Steps:\n",
    "1. load word embeding and document embedding\n",
    "2. create pytorch dataset and dataloader\n",
    "3. Try Contrastive loss and triplet loss\n",
    "4. further improve negative sampling (e.g. hard negative or word2vec negative sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e9beff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from itertools import cycle\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05c1627d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:400000\n"
     ]
    }
   ],
   "source": [
    "# load word embedding\n",
    "word2embedding = dict()\n",
    "embedding_file = \"../data/glove.6B.100d.txt\"\n",
    "word_dim = int(re.findall(r\".(\\d+)d\",embedding_file)[0])\n",
    "with open(embedding_file,\"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split()\n",
    "        word = line[0]\n",
    "        embedding = list(map(float,line[1:]))\n",
    "        word2embedding[word] = embedding\n",
    "\n",
    "print(\"Number of words:%d\" % len(word2embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1233053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold, word2embedding):\n",
    "        # The low frequency words will be assigned as <UNK> token\n",
    "        self.itos = {0: \"<UNK>\"}\n",
    "        self.stoi = {\"<UNK>\": 0}\n",
    "        self.freq_threshold = freq_threshold\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer_eng(text):\n",
    "        text = re.sub(r'[^A-Za-z0-9 ]+', '', text)\n",
    "        return text.strip().split()\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        self.frequencies = {}\n",
    "        self.word_vectors = [[0]*word_dim] # init zero padding\n",
    "        idx = 1\n",
    "        \n",
    "        for sentence in tqdm(sentence_list, desc=\"Preprocessing documents\"):\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                if word not in word2embedding:\n",
    "                    continue\n",
    "                if word not in self.frequencies:\n",
    "                    self.frequencies[word] = 1\n",
    "\n",
    "                else:\n",
    "                    self.frequencies[word] += 1\n",
    "\n",
    "                if self.frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    self.word_vectors.append(word2embedding[word])\n",
    "                    idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c5706a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBowDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 data_file_path,\n",
    "                 word2embedding,\n",
    "                 freq_threshold=20,\n",
    "                 skip_header = False,\n",
    "                 max_length = None,\n",
    "                 ):\n",
    "        # read data\n",
    "        self.document_vectors = []\n",
    "        docEmb_file = open(\"../data/docvector.txt\",\"r\")\n",
    "        with open(data_file_path,'r',encoding='utf-8') as f:\n",
    "            if skip_header:\n",
    "                f.readline()\n",
    "            self.documents = []\n",
    "            for line in tqdm(f, desc=\"Loading documents\"):\n",
    "                if max_length is not None and len(self.documents) >= max_length:\n",
    "                    break\n",
    "                self.documents.append(line.strip(\"\\n\"))\n",
    "                doc_vec = docEmb_file.readline().strip().split()\n",
    "                doc_vec = list(map(float, doc_vec))\n",
    "                self.document_vectors.append(doc_vec)\n",
    "        \n",
    "        # build vocabulary\n",
    "        self.vocab = Vocabulary(freq_threshold,word2embedding)\n",
    "        self.vocab.build_vocabulary(self.documents)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.words_tokenized = [self.vocab.numericalize(text) for text in self.documents]\n",
    "        \n",
    "        # train-test split\n",
    "        # training\n",
    "        self.train_length = int(len(self.words_tokenized)*0.8)\n",
    "        self.train_vectors = self.document_vectors[:self.train_length]\n",
    "        self.train_words = self.words_tokenized[:self.train_length]\n",
    "        self.document_ids = list(range(self.train_length))\n",
    "        self.generator = cycle(self.context_target_generator())\n",
    "        self.dataset_size = sum([len(s) for s in self.train_words])\n",
    "        \n",
    "        # testing\n",
    "        self.test_vectors = self.document_vectors[self.train_length:]\n",
    "        self.test_words = self.words_tokenized[self.train_length:]\n",
    "\n",
    "\n",
    "\n",
    "    def context_target_generator(self):\n",
    "        np.random.shuffle(self.document_ids) # inplace shuffle\n",
    "\n",
    "        # randomly select a document and create its training example\n",
    "        for document_id in self.document_ids: \n",
    "            word_list = set(self.train_words[document_id])\n",
    "            negative_sample_space = list(set(range(self.vocab_size)) - word_list)\n",
    "            negative_samples = np.random.choice(negative_sample_space,size=len(word_list),replace = False)\n",
    "            for word_id, negative_wordID in zip(word_list, negative_samples):\n",
    "                yield [document_id, word_id, negative_wordID]\n",
    "                \n",
    "    def __getitem__(self, idx):\n",
    "        doc_id, word_id, negative_wordID = next(self.generator)\n",
    "        doc_id = torch.FloatTensor(self.document_vectors[doc_id])\n",
    "        word_id = torch.FloatTensor(self.vocab.word_vectors[word_id])\n",
    "        negative_word = torch.FloatTensor(self.vocab.word_vectors[negative_wordID])\n",
    "\n",
    "        return doc_id, word_id, negative_word\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1a7186c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading documents: 2519it [00:00, 25182.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading documents: 100000it [00:04, 24255.79it/s]\n",
      "Preprocessing documents: 100%|██████████| 100000/100000 [00:10<00:00, 9279.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish building dataset!\n",
      "Number of documents:100000\n",
      "Number of words:27961\n"
     ]
    }
   ],
   "source": [
    "# load and build torch dataset\n",
    "data_file_path = '../data/IMDB.txt'\n",
    "# checkpoint_path = \"doc2vecC_lr0.001.pt\"\n",
    "print(\"Building dataset....\")\n",
    "dataset = CBowDataset(\n",
    "                    data_file_path=data_file_path,\n",
    "                    word2embedding=word2embedding,\n",
    "                    max_length=None,\n",
    "                    freq_threshold=20,\n",
    "                    skip_header=False\n",
    "                    )\n",
    "print(\"Finish building dataset!\")\n",
    "print(f\"Number of documents:{len(dataset.documents)}\")\n",
    "print(f\"Number of words:{dataset.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf6bf5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 doc_vectors,\n",
    "                 ans_words,\n",
    "                 ):\n",
    "        self.doc_vectors = doc_vectors\n",
    "        self.ans_words = ans_words\n",
    "        assert len(doc_vectors) == len(ans_words)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        doc_vec = torch.FloatTensor(self.doc_vectors[idx])\n",
    "        ans_w = torch.tensor(list(set(self.ans_words[idx])))\n",
    "        return doc_vec, ans_w\n",
    "\n",
    "    def collate_fn(self,batch):\n",
    "        # Batch: List of tuples [(batch1), (batch2)]\n",
    "        \n",
    "        doc_vec = torch.cat([item[0].unsqueeze(0) for item in batch], dim=0)\n",
    "        ans_w = [item[1] for item in batch]\n",
    "        ans_w = pad_sequence(ans_w, batch_first=True, padding_value=-1)\n",
    "        \n",
    "        return doc_vec, ans_w \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doc_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "457c5d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletNet(nn.Module):\n",
    "    def __init__(self, hdim):\n",
    "        super(TripletNet, self).__init__()\n",
    "        self.fc = nn.Sequential(nn.Linear(hdim, 512),\n",
    "                        nn.PReLU(),\n",
    "                        nn.Linear(512, 512),\n",
    "                        nn.PReLU(),\n",
    "                        nn.Linear(512, 2)\n",
    "                        )\n",
    "\n",
    "    def forward(self, x1, x2, x3):\n",
    "        output1 = self.fc(x1)\n",
    "        output2 = self.fc(x2)\n",
    "        output3 = self.fc(x3)\n",
    "        return output1, output2, output3\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59ec9a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Triplet loss\n",
    "    Takes embeddings of an anchor sample, a positive sample and a negative sample\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative, size_average=True):\n",
    "        distance_positive = (anchor - positive).pow(2).sum(1)  # .pow(.5)\n",
    "        distance_negative = (anchor - negative).pow(2).sum(1)  # .pow(.5)\n",
    "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean() if size_average else losses.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b60ca7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "margin = 1.\n",
    "BATCH_SIZE = 2048\n",
    "EPOCH = 300\n",
    "\n",
    "device = \"cuda:0\"\n",
    "model = TripletNet(word_dim).to(device)\n",
    "loss_fn = TripletLoss(margin).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22a934f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "                        dataset, \n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        num_workers=4,\n",
    "                        shuffle=True,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2aa0290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docvec = dataset.test_vectors\n",
    "test_ans = dataset.test_words\n",
    "test_dataset = TestDataset(test_docvec,test_ans)\n",
    "test_loader = DataLoader(test_dataset,                         \n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         num_workers=4,\n",
    "                         collate_fn=test_dataset.collate_fn)\n",
    "word_embedding_tensor = torch.FloatTensor(dataset.vocab.word_vectors).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05bb89ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_word_emb, loader,Ks = [50,100,150,200]):\n",
    "    avg_precision, avg_recall = [], []\n",
    "    for batch in test_loader:\n",
    "        batch = [item.to(device) for item in batch]\n",
    "        emb, ans = batch\n",
    "        emb = model.get_embedding(emb)\n",
    "        scores = torch.cdist(emb, test_word_emb)\n",
    "        ans_length = torch.sum((~ans.eq(-1)).float(), dim=-1)\n",
    "        mask = ~ans.eq(-1).unsqueeze(-1)\n",
    "        \n",
    "        # calculate precision and recall\n",
    "        tmp_pr, tmp_re = [],[]\n",
    "        for K in Ks:\n",
    "            top_indices = torch.argsort(scores,dim=1)[:,:K]\n",
    "            hit = top_indices.unsqueeze(-2) == ans.unsqueeze(-1)\n",
    "            hit = torch.sum((hit * mask).flatten(1),dim=-1)\n",
    "            precision = hit / torch.min(ans_length,torch.tensor(K).to(device))\n",
    "            recall = hit / ans_length\n",
    "            tmp_pr.append(precision)\n",
    "            tmp_re.append(recall)\n",
    "        tmp_pr = torch.stack(tmp_pr).T.detach().cpu().numpy().tolist()\n",
    "        tmp_re = torch.stack(tmp_re).T.detach().cpu().numpy().tolist()\n",
    "        avg_precision.extend(tmp_pr)\n",
    "        avg_recall.extend(tmp_re)\n",
    "        \n",
    "    avg_precision = np.mean(avg_precision,axis=0)\n",
    "    avg_recall = np.mean(avg_recall, axis=0)\n",
    "    for idx, kval in enumerate(Ks):\n",
    "        print(f\"[K={kval}] Precision:{avg_precision[idx]:.4f} Recall:{avg_recall[idx]:.4f}\")\n",
    "    return avg_precision, avg_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c124d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9047/9047 [06:30<00:00, 23.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.189930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9047 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K=50] Precision:0.2319 Recall:0.0955\n",
      "[K=100] Precision:0.2493 Recall:0.1869\n",
      "[K=150] Precision:0.2997 Recall:0.2711\n",
      "[K=200] Precision:0.3607 Recall:0.3471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9047/9047 [05:54<00:00, 25.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.183412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9047 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K=50] Precision:0.5651 Recall:0.2413\n",
      "[K=100] Precision:0.4574 Recall:0.3459\n",
      "[K=150] Precision:0.4553 Recall:0.4120\n",
      "[K=200] Precision:0.4752 Recall:0.4571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9047/9047 [05:50<00:00, 25.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.183480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9047 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K=50] Precision:0.4457 Recall:0.1866\n",
      "[K=100] Precision:0.4238 Recall:0.3201\n",
      "[K=150] Precision:0.4264 Recall:0.3857\n",
      "[K=200] Precision:0.4631 Recall:0.4457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9047/9047 [05:43<00:00, 26.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.181225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9047 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K=50] Precision:0.5474 Recall:0.2327\n",
      "[K=100] Precision:0.4530 Recall:0.3427\n",
      "[K=150] Precision:0.4463 Recall:0.4041\n",
      "[K=200] Precision:0.4720 Recall:0.4542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9047/9047 [05:54<00:00, 25.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.180449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9047 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K=50] Precision:0.5637 Recall:0.2388\n",
      "[K=100] Precision:0.4643 Recall:0.3513\n",
      "[K=150] Precision:0.4527 Recall:0.4098\n",
      "[K=200] Precision:0.4772 Recall:0.4591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9047/9047 [05:49<00:00, 25.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.179355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9047 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K=50] Precision:0.5574 Recall:0.2360\n",
      "[K=100] Precision:0.4609 Recall:0.3481\n",
      "[K=150] Precision:0.4540 Recall:0.4107\n",
      "[K=200] Precision:0.4768 Recall:0.4586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9047/9047 [05:50<00:00, 25.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.178145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9047 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K=50] Precision:0.5553 Recall:0.2353\n",
      "[K=100] Precision:0.4622 Recall:0.3494\n",
      "[K=150] Precision:0.4546 Recall:0.4112\n",
      "[K=200] Precision:0.4780 Recall:0.4598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 5801/9047 [03:49<01:43, 31.45it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 9047/9047 [05:45<00:00, 26.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.179775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9047 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K=50] Precision:0.4964 Recall:0.2097\n",
      "[K=100] Precision:0.4410 Recall:0.3324\n",
      "[K=150] Precision:0.4446 Recall:0.4016\n",
      "[K=200] Precision:0.4677 Recall:0.4496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 4448/9047 [03:01<02:24, 31.76it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 9047/9047 [05:52<00:00, 25.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.181444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9047 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K=50] Precision:0.5004 Recall:0.2123\n",
      "[K=100] Precision:0.4438 Recall:0.3340\n",
      "[K=150] Precision:0.4441 Recall:0.4011\n",
      "[K=200] Precision:0.4642 Recall:0.4462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 3163/9047 [02:14<03:33, 27.60it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 9047/9047 [06:35<00:00, 22.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.191374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9047 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K=50] Precision:0.4923 Recall:0.2078\n",
      "[K=100] Precision:0.4399 Recall:0.3320\n",
      "[K=150] Precision:0.4461 Recall:0.4031\n",
      "[K=200] Precision:0.4680 Recall:0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 733/9047 [00:34<19:02,  7.28it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 9047/9047 [05:45<00:00, 26.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.180400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9047 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K=50] Precision:0.4945 Recall:0.2089\n",
      "[K=100] Precision:0.4427 Recall:0.3341\n",
      "[K=150] Precision:0.4469 Recall:0.4039\n",
      "[K=200] Precision:0.4694 Recall:0.4514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 2149/9047 [01:32<03:41, 31.15it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 9047/9047 [05:52<00:00, 25.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.176195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9047 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K=50] Precision:0.5005 Recall:0.2124\n",
      "[K=100] Precision:0.4423 Recall:0.3338\n",
      "[K=150] Precision:0.4441 Recall:0.4014\n",
      "[K=200] Precision:0.4658 Recall:0.4478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 8377/9047 [05:20<00:20, 32.36it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 9047/9047 [05:53<00:00, 25.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.177499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9047 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K=50] Precision:0.4865 Recall:0.2053\n",
      "[K=100] Precision:0.4404 Recall:0.3323\n",
      "[K=150] Precision:0.4457 Recall:0.4028\n",
      "[K=200] Precision:0.4672 Recall:0.4491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 649/9047 [00:30<04:30, 31.09it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 9047/9047 [05:46<00:00, 26.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.174651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9047 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K=50] Precision:0.4610 Recall:0.1942\n",
      "[K=100] Precision:0.4300 Recall:0.3242\n",
      "[K=150] Precision:0.4384 Recall:0.3959\n",
      "[K=200] Precision:0.4638 Recall:0.4459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 4831/9047 [03:13<02:12, 31.90it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 65%|██████▍   | 5850/9047 [04:01<01:37, 32.86it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 9047/9047 [05:50<00:00, 25.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.178876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9047 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K=50] Precision:0.5039 Recall:0.2137\n",
      "[K=100] Precision:0.4412 Recall:0.3330\n",
      "[K=150] Precision:0.4461 Recall:0.4032\n",
      "[K=200] Precision:0.4676 Recall:0.4496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 4877/9047 [03:17<02:10, 32.04it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 28%|██▊       | 2538/9047 [01:46<03:29, 31.14it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 9047/9047 [05:43<00:00, 26.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.174499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9047 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K=50] Precision:0.4882 Recall:0.2060\n",
      "[K=100] Precision:0.4379 Recall:0.3303\n",
      "[K=150] Precision:0.4433 Recall:0.4007\n",
      "[K=200] Precision:0.4646 Recall:0.4467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 1788/9047 [01:17<03:55, 30.77it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 90%|████████▉ | 8125/9047 [05:15<00:28, 31.80it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 9047/9047 [05:54<00:00, 25.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.174004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9047 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K=50] Precision:0.4940 Recall:0.2081\n",
      "[K=100] Precision:0.4464 Recall:0.3366\n",
      "[K=150] Precision:0.4485 Recall:0.4053\n",
      "[K=200] Precision:0.4693 Recall:0.4512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 7332/9047 [04:53<00:54, 31.24it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 53%|█████▎    | 4818/9047 [03:15<02:20, 30.18it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 9047/9047 [05:54<00:00, 25.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.176838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9047 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K=50] Precision:0.4598 Recall:0.1928\n",
      "[K=100] Precision:0.4204 Recall:0.3168\n",
      "[K=150] Precision:0.4358 Recall:0.3939\n",
      "[K=200] Precision:0.4595 Recall:0.4419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4014/9047 [02:44<11:37,  7.22it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 9047/9047 [06:14<00:00, 24.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.172757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9047 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K=50] Precision:0.5016 Recall:0.2117\n",
      "[K=100] Precision:0.4400 Recall:0.3318\n",
      "[K=150] Precision:0.4417 Recall:0.3992\n",
      "[K=200] Precision:0.4629 Recall:0.4451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1518/9047 [01:06<03:52, 32.42it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 9047/9047 [05:47<00:00, 26.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.177057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9047 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K=50] Precision:0.4522 Recall:0.1890\n",
      "[K=100] Precision:0.4299 Recall:0.3236\n",
      "[K=150] Precision:0.4368 Recall:0.3946\n",
      "[K=200] Precision:0.4587 Recall:0.4410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 777/9047 [00:36<04:20, 31.73it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 9047/9047 [05:48<00:00, 25.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.172568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9047 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K=50] Precision:0.5073 Recall:0.2142\n",
      "[K=100] Precision:0.4441 Recall:0.3347\n",
      "[K=150] Precision:0.4473 Recall:0.4042\n",
      "[K=200] Precision:0.4679 Recall:0.4499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9047/9047 [05:52<00:00, 25.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.173663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9047 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K=50] Precision:0.5093 Recall:0.2150\n",
      "[K=100] Precision:0.4440 Recall:0.3346\n",
      "[K=150] Precision:0.4478 Recall:0.4047\n",
      "[K=200] Precision:0.4674 Recall:0.4495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9047/9047 [05:47<00:00, 26.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.183238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9047 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K=50] Precision:0.4967 Recall:0.2094\n",
      "[K=100] Precision:0.4427 Recall:0.3337\n",
      "[K=150] Precision:0.4470 Recall:0.4040\n",
      "[K=200] Precision:0.4675 Recall:0.4495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9047/9047 [05:52<00:00, 25.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.175671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9047 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K=50] Precision:0.5021 Recall:0.2117\n",
      "[K=100] Precision:0.4461 Recall:0.3365\n",
      "[K=150] Precision:0.4479 Recall:0.4048\n",
      "[K=200] Precision:0.4699 Recall:0.4519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9047/9047 [05:52<00:00, 25.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.172573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9047 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K=50] Precision:0.4983 Recall:0.2099\n",
      "[K=100] Precision:0.4381 Recall:0.3303\n",
      "[K=150] Precision:0.4428 Recall:0.4001\n",
      "[K=200] Precision:0.4645 Recall:0.4466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9047/9047 [05:51<00:00, 25.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.185931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9047 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K=50] Precision:0.3978 Recall:0.1644\n",
      "[K=100] Precision:0.4009 Recall:0.3017\n",
      "[K=150] Precision:0.4268 Recall:0.3856\n",
      "[K=200] Precision:0.4454 Recall:0.4282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9047/9047 [05:52<00:00, 25.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.178193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9047 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K=50] Precision:0.5029 Recall:0.2125\n",
      "[K=100] Precision:0.4418 Recall:0.3331\n",
      "[K=150] Precision:0.4452 Recall:0.4024\n",
      "[K=200] Precision:0.4675 Recall:0.4496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9047/9047 [05:54<00:00, 25.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.175348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9047 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K=50] Precision:0.5006 Recall:0.2106\n",
      "[K=100] Precision:0.4384 Recall:0.3300\n",
      "[K=150] Precision:0.4453 Recall:0.4023\n",
      "[K=200] Precision:0.4677 Recall:0.4497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9047/9047 [05:58<00:00, 25.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.172986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/9047 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K=50] Precision:0.3665 Recall:0.1531\n",
      "[K=100] Precision:0.3538 Recall:0.2677\n",
      "[K=150] Precision:0.3849 Recall:0.3490\n",
      "[K=200] Precision:0.4163 Recall:0.4009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 8637/9047 [05:37<00:16, 25.56it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a332d75b24da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpos_w\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneg_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mavg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gnn/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gnn/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    avg_loss = []\n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch = [item.to(device) for item in batch]\n",
    "        doc_id,pos_w,neg_w = batch\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(*model(doc_id,pos_w,neg_w))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss.append(loss.item())\n",
    "    avg_loss = np.mean(avg_loss)\n",
    "    print(f\"Loss:{avg_loss:4f}\")\n",
    "    \n",
    "    # evaluate\n",
    "    model.eval()\n",
    "    test_word_emb = model.get_embedding(word_embedding_tensor)\n",
    "    res = evaluate(test_word_emb,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c89be81",
   "metadata": {},
   "outputs": [],
   "source": [
    "validTrain_docvec = dataset.train_vectors\n",
    "validTrain_ans = dataset.train_words\n",
    "validTrain_dataset = TestDataset(validTrain_docvec,validTrain_ans)\n",
    "validTrain_loader = DataLoader(validTrain_dataset,                         \n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         num_workers=4,\n",
    "                         collate_fn=validTrain_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc430927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[K=50] Precision:0.4818 Recall:0.2027\n",
      "[K=100] Precision:0.4392 Recall:0.3311\n",
      "[K=150] Precision:0.4447 Recall:0.4017\n",
      "[K=200] Precision:0.4674 Recall:0.4494\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "model.eval()\n",
    "test_word_emb = model.get_embedding(word_embedding_tensor)\n",
    "res = evaluate(test_word_emb,validTrain_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d39ff4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 23 06:11:38 2021\n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.45.01    Driver Version: 455.45.01    CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "\u001b[31m|   0  GeForce RTX 3090    On   | 00000000:1D:00.0 Off |                  N/A |\u001b[0m\n",
      "\u001b[31m| 76%   62C    P2   333W / 350W |  21435MiB / 24268MiB |    100%      Default |\u001b[0m\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "\u001b[31m|   1  GeForce RTX 3090    On   | 00000000:B4:00.0 Off |                  N/A |\u001b[0m\n",
      "\u001b[31m| 58%   60C    P2   331W / 350W |  14178MiB / 24268MiB |    100%      Default |\u001b[0m\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------+\n",
      "|  GPU   PID     USER    GPU MEM  %CPU  %MEM      TIME  COMMAND               |\n",
      "|    0  2196     root       4MiB   0.0   0.0   12 days  /usr/lib/xorg/Xorg v  |\n",
      "|    0 315760 cealia3+    1875MiB   0.1   1.0    5 days  /dhome/cealia312/min  |\n",
      "|    0 456580 cealia3+    2081MiB   0.7   1.0    4 days  /dhome/cealia312/min  |\n",
      "|    0 456989 cealia3+    2191MiB   2.2   1.0    4 days  /dhome/cealia312/min  |\n",
      "|    0 457679 cealia3+       0MiB   0.5   1.0    4 days  /dhome/cealia312/min  |\n",
      "|    0 457910 cealia3+       0MiB   1.8   1.0    4 days  /dhome/cealia312/min  |\n",
      "|    0 459254 cealia3+       0MiB   0.3   3.6    4 days  /dhome/cealia312/min  |\n",
      "|    0 459464 cealia3+       0MiB   0.3   3.6    4 days  /dhome/cealia312/min  |\n",
      "|    0 459689 cealia3+       0MiB   0.5   1.2    4 days  /dhome/cealia312/min  |\n",
      "|    0 628178 steven9+    4761MiB   4.5   0.0    2 days  /dhome/steven95421/m  |\n",
      "|    0 855361  roytsai   10519MiB  57.0   1.2  12:27:09  /dhome/roytsai/gnn/b  |\n",
      "|    1  2196     root       4MiB   0.0   0.0   12 days  /usr/lib/xorg/Xorg v  |\n",
      "|    1 315760 cealia3+       0MiB   0.1   1.0    5 days  /dhome/cealia312/min  |\n",
      "|    1 456580 cealia3+       0MiB   0.7   1.0    4 days  /dhome/cealia312/min  |\n",
      "|    1 456989 cealia3+       0MiB   2.2   1.0    4 days  /dhome/cealia312/min  |\n",
      "|    1 457679 cealia3+    2089MiB   0.5   1.0    4 days  /dhome/cealia312/min  |\n",
      "|    1 457910 cealia3+    2247MiB   1.8   1.0    4 days  /dhome/cealia312/min  |\n",
      "|    1 459254 cealia3+    1495MiB   0.3   3.6    4 days  /dhome/cealia312/min  |\n",
      "|    1 459464 cealia3+    1495MiB   0.3   3.6    4 days  /dhome/cealia312/min  |\n",
      "|    1 459689 cealia3+    2083MiB   0.5   1.2    4 days  /dhome/cealia312/min  |\n",
      "|    1 628178 steven9+    4761MiB   4.5   0.0    2 days  /dhome/steven95421/m  |\n",
      "|    1 855361  roytsai       0MiB  57.0   1.2  12:27:09  /dhome/roytsai/gnn/b  |\n",
      "+-----------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nvidia-htop.py --color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b1fe30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
