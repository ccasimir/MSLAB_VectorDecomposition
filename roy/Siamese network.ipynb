{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "982acb8f",
   "metadata": {},
   "source": [
    "## Siamese network \n",
    "Steps:\n",
    "1. load word embeding and document embedding\n",
    "2. create pytorch dataset and dataloader\n",
    "3. Try Contrastive loss and triplet loss\n",
    "4. further improve negative sampling (e.g. hard negative or word2vec negative sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1e9beff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from itertools import cycle\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "05c1627d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:400000\n"
     ]
    }
   ],
   "source": [
    "# load word embedding\n",
    "word2embedding = dict()\n",
    "embedding_file = \"../../word/glove.6B.100d.txt\"\n",
    "word_dim = int(re.findall(r\".(\\d+)d\",embedding_file)[0])\n",
    "with open(embedding_file,\"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split()\n",
    "        word = line[0]\n",
    "        embedding = list(map(float,line[1:]))\n",
    "        word2embedding[word] = embedding\n",
    "        word_embedding_matrix.append(embedding)\n",
    "        stoi[word] = index\n",
    "        itos[index] = word\n",
    "        index += 1\n",
    "\n",
    "print(\"Number of words:%d\" % len(word2embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4a937983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a1233053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold, word2embedding):\n",
    "        # The low frequency words will be assigned as <UNK> token\n",
    "        self.itos = {0: \"<UNK>\"}\n",
    "        self.stoi = {\"<UNK>\": 0}\n",
    "        self.freq_threshold = freq_threshold\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer_eng(text):\n",
    "        text = re.sub(r'[^A-Za-z0-9 ]+', '', text)\n",
    "        return text.strip().split()\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        self.frequencies = {}\n",
    "        self.word_vectors = [[0]*word_dim] # init zero padding\n",
    "        idx = 1\n",
    "        \n",
    "        for sentence in tqdm(sentence_list, desc=\"Preprocessing documents\"):\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                if word not in word2embedding:\n",
    "                    continue\n",
    "                if word not in self.frequencies:\n",
    "                    self.frequencies[word] = 1\n",
    "\n",
    "                else:\n",
    "                    self.frequencies[word] += 1\n",
    "\n",
    "                if self.frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    self.word_vectors.append(word2embedding[word])\n",
    "                    idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "\n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1c5706a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBowDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 data_file_path,\n",
    "                 word2embedding,\n",
    "                 freq_threshold=20,\n",
    "                 skip_header = False,\n",
    "                 max_length = None,\n",
    "                 ):\n",
    "        # read data\n",
    "        self.document_vectors = []\n",
    "        docEmb_file = open(\"../data/docvector.txt\",\"r\")\n",
    "        with open(data_file_path,'r',encoding='utf-8') as f:\n",
    "            if skip_header:\n",
    "                f.readline()\n",
    "            self.documents = []\n",
    "            for line in tqdm(f, desc=\"Loading documents\"):\n",
    "                if max_length is not None and len(self.documents) >= max_length:\n",
    "                    break\n",
    "                self.documents.append(line.strip(\"\\n\"))\n",
    "                doc_vec = docEmb_file.readline().strip().split()\n",
    "                doc_vec = list(map(float, doc_vec))\n",
    "                self.document_vectors.append(doc_vec)\n",
    "        \n",
    "        #build vocabulary\n",
    "        self.vocab = Vocabulary(freq_threshold,word2embedding)\n",
    "        self.vocab.build_vocabulary(self.documents)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.words_tokenized = [self.vocab.numericalize(text) for text in self.documents]\n",
    "        self.document_ids = list(range(len(self.words_tokenized)))\n",
    "        self.generator = cycle(self.context_target_generator())\n",
    "\n",
    "\n",
    "    def context_target_generator(self):\n",
    "        np.random.shuffle(self.document_ids) # inplace shuffle\n",
    "\n",
    "        # randomly select a document and create its training example\n",
    "        for document_id in self.document_ids: \n",
    "            word_list = set(self.words_tokenized[document_id])\n",
    "            negative_sample_space = list(set(range(self.vocab_size)) - word_list)\n",
    "            negative_samples = np.random.choice(negative_sample_space,size=len(word_list),replace = False)\n",
    "            for word_id, negative_wordID in zip(word_list, negative_samples):\n",
    "                yield [document_id, word_id, negative_wordID]\n",
    "                \n",
    "    def __getitem__(self, idx):\n",
    "        doc_id, word_id, negative_wordID = next(self.generator)\n",
    "        doc_id = torch.tensor(doc_id)\n",
    "        word_id = torch.tensor(word_id)\n",
    "        negative_word = torch.tensor(negative_wordID)\n",
    "\n",
    "        return doc_id, word_id, negative_word\n",
    "\n",
    "    def __len__(self):\n",
    "        return 2**20 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a1a7186c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset....\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46efb4de23b14e6c90c2c5f7c35ef397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading documents: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2c77398086466a9399dae2039507e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing documents:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish building dataset!\n",
      "Number of documents:100000\n",
      "Number of words:27961\n"
     ]
    }
   ],
   "source": [
    "# load and build torch dataset\n",
    "data_file_path = '../data/IMDB.txt'\n",
    "# checkpoint_path = \"doc2vecC_lr0.001.pt\"\n",
    "print(\"Building dataset....\")\n",
    "dataset = CBowDataset(\n",
    "                    data_file_path=data_file_path,\n",
    "                    word2embedding=word2embedding,\n",
    "                    max_length=None,\n",
    "                    freq_threshold=20,\n",
    "                    skip_header=False\n",
    "                    )\n",
    "print(\"Finish building dataset!\")\n",
    "print(f\"Number of documents:{len(dataset.documents)}\")\n",
    "print(f\"Number of words:{dataset.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2bb93f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "40242209",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\n",
    "                        dataset, \n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        num_workers=4,\n",
    "                        shuffle=True,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3708d3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([70663, 70663, 70663, 70663, 70663, 70663, 70663, 70663, 70663, 70663,\n",
      "        70663, 70663, 70663, 70663, 70663, 70663]) tensor([   0,    1,    3,    4,    5,    6,    8,    9,   10,   11,   12,  526,\n",
      "         527, 7183,   18,   19]) tensor([ 9370, 15830, 10243,  1979,  8193, 21789, 13163, 26057, 12130,  1440,\n",
      "        17503, 21588, 12861, 11286,  2515, 11441])\n"
     ]
    }
   ],
   "source": [
    "for doc_id,pos_w,neg_w in data_loader:\n",
    "    print(doc_id,pos_w,neg_w)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea024d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
